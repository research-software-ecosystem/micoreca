[
    {
        "create_time": "2025-10-31",
        "creators": [
            "VGP",
            " Galaxy"
        ],
        "description": "Evaluation of Pacbio Hifi Reads and genome profiling. Create Meryl Database used for the estimation of assembly parameters and quality control with Merqury. Part of the VGP pipeline.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in name",
        "id": "632",
        "keep": "Reject",
        "latest_version": 13,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/632?version=13",
        "name": "kmer-profiling-hifi-VGP1/main",
        "number_of_steps": 17,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "",
            "compose_text_param",
            "meryl_count_kmers",
            "imagemagick_image_montage",
            "genomescope",
            "rdeval",
            "meryl_groups_kmers",
            "meryl_histogram_kmers",
            "rdeval_report",
            "param_value_from_file"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-31",
        "versions": 13
    },
    {
        "create_time": "2025-10-26",
        "creators": [],
        "description": "# 3D genome builder (3DGB)\r\n\r\n3D genome builder (3DGB) is a workflow to build 3D models of genomes from HiC raw data and to integrate omics data on the produced models for further visual exploration.\r\n3DGB bundles [HiC-Pro](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0831-x), [PASTIS](https://academic.oup.com/bioinformatics/article/30/12/i26/385087) and custom Python scripts into a unified Snakemake workflow with limited inputs (see *Preparing Required Files*). 3DGB produces annotated 3D models of genome in PDB and G3D formats.\r\n\r\n## Download this repository\r\n\r\n```bash\r\ngit clone https://github.com/data-fun/3d-genome-builder.git\r\ncd 3d-genome-builder\r\n```\r\n\r\n## Install dependencies\r\n\r\n### Singularity\r\n\r\nDownload the latest version [here](https://github.com/apptainer/singularity/releases)\r\n\r\nInstall Singularity:\r\n\r\n```bash\r\nsudo apt install -y ./singularity-container_3.8.7_amd64.deb\r\n```\r\n\r\nVerify version:\r\n\r\n```\r\n$ singularity --version\r\nsingularity version 3.8.7\r\n```\r\n\r\n### Conda environment\r\n\r\nInstall [conda](https://docs.conda.io/en/latest/miniconda.html).\r\n\r\nInstall mamba:\r\n\r\n```bash\r\nconda install mamba -n base -c conda-forge\r\n```\r\n\r\nCreate conda environment and install dependendies:\r\n\r\n```bash\r\nmamba env create -f binder/environment.yml\r\n```\r\n\r\nLoad conda environment:\r\n\r\n```bash\r\nconda activate 3DGB\r\n```\r\n\r\n### Download  HiC-Pro Singularity image\r\n\r\n\r\n```bash\r\nwget --ciphers=DEFAULT:@SECLEVEL=1 https://zerkalo.curie.fr/partage/HiC-Pro/hicpro_3.1.0_ubuntu.img -P images\r\n```\r\n\r\nIf this command fails, try with an [alternate download link](https://zenodo.org/record/8376626):\r\n\r\n```bash\r\nwget https://zenodo.org/record/8376626/files/hicpro_3.1.0_ubuntu.img -P images\r\n```\r\n\r\nCheck the integrity of the image:\r\n\r\n```bash\r\n$ md5sum images/hicpro_3.1.0_ubuntu.img\r\nd480e636397c14e187608e50309eb9af  images/hicpro_3.1.0_ubuntu.img\r\n```\r\n\r\nVerify HiC-Pro version with:\r\n\r\n```bash\r\n$ singularity exec images/hicpro_3.1.0_ubuntu.img HiC-Pro --version\r\n[...]\r\nHiC-Pro version 3.1.0\r\n```\r\n\r\nand bowtie2 version:\r\n\r\n```bash\r\n$ singularity exec images/hicpro_3.1.0_ubuntu.img bowtie2 --version  2>/dev/null | head -n 1\r\n/usr/local/conda/envs/hicpro/bin/bowtie2-align-s version 2.4.4\r\n```\r\n\r\n\r\n## Prepare required files\r\n\r\n### Create the config file\r\n\r\nCreate and edit a configuration file in [yaml](https://en.wikipedia.org/wiki/YAML) format. See for instance the template `config_template.yml`\r\n\r\n### Add the reference genome\r\n\r\nThe reference genome fasta file must be located in `WORKING_DIR/genome.fasta` where `WORKING_DIR` is the name of the working directory as specified in your config file.\r\n\r\n### Add FASTQ files (optional)\r\n\r\nIf you already have fastq files stored locally or some fastq files are not available on GEO or SRA, you can use these files providing they are in the proper directory structure:\r\n\r\n<img align=\"right\" width=\"200px\" \r\n    src=\"assets/SCERE_chromosome_13.gif\"\r\n    alt=\"3D structure of the chromosome 13 of S. cerevisiae at 5 kb resolution\">\r\n\r\n```\r\nWORKING_DIR/\r\n\u251c\u2500\u2500 fastq_files\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ID1\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ID1_R1.fastq.gz\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ID1_R2.fastq.gz\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ID2\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ID2_R1.fastq.gz\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ID2_R2.fastq.gz\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ID3\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ID3_R1.fastq.gz\r\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ID3_R2.fastq.gz\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ID4\r\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ID4_R1.fastq.gz\r\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ID4_R2.fastq.gz\r\n\u2514\u2500\u2500 genome.fasta\r\n```\r\n\r\n- `WORKING_DIR` is the name of the working directory as specified in your config file.\r\n- Paired-end fastq files are in the directory `WORKING_DIR/fastq_files/IDx` with `IDx` the identifier of the paired fastq files. Fastq identifiers are reported in the config file. Please note fastq files have to follow the pattern `<sample ID>_R<1 or 2>.fastq.gz`.\r\n\r\n> **Note**\r\n>\r\n> Please strictly follow this file organization as it is required by the 3DGB workflow.\r\n\r\n## Build model\r\n\r\nRun 3DGB:\r\n\r\n```bash\r\nsnakemake --profile smk_profile -j 4 --configfile YOUR-CONFIG.yml\r\n```\r\n\r\n> **Note**\r\n> - Adapt `YOUR-CONFIG.yml` to the exact name of the config file you created.\r\n> - Option `-j 4` tells Snakemake to use up to 4 cores. If you are more cores available, you can increase this value (*e.g.* `-j 16`).\r\n\r\nOr with debugging options:\r\n\r\n```bash\r\nsnakemake --profile smk_profile_debug -j 4 --configfile YOUR-CONFIG.yml --verbose\r\n```\r\n\r\nDepending on the number and size of fastq files, the 3D construction will take a couple of hours to run.\r\n\r\nFor troubleshooting, have a look to log files in `WORKING_DIR/logs`, where `WORKING_DIR` is the name of the working directory as specified in your config file.\r\n\r\n## Map quantitative values on the 3D model\r\n\r\nTo map quantitative values on the model run:\r\n\r\n```bash\r\npython ./scripts/map_parameter.py --pdb path/to/structure.pdb --bedgraph path/to/annotation.bedgraph --output path/to/output.pdb\r\n```\r\n\r\nQuantitative values should be formatted in a 4-column bedgraph file (chromosome/start/stop/value):\r\n\r\n```\r\nchr1\t0\t50000\t116.959\r\nchr1\t50000\t100000\t48.4495\r\nchr1\t100000\t150000\t22.8726\r\nchr1\t150000\t200000\t84.3106\r\nchr1\t200000\t250000\t113.109\r\n```\r\n\r\nEach bead of the model will be assigned a quantitative value. The resolution in the bedgraph file should match the resolution used to build the model.\r\n\r\n\r\n## Get results\r\n\r\nUpon completion, the `WORKING_DIR` should look like this:\r\n\r\n```\r\nWORKING_DIR/\r\n\u251c\u2500\u2500 contact_maps\r\n\u251c\u2500\u2500 dense_matrix\r\n\u251c\u2500\u2500 fastq_files\r\n\u251c\u2500\u2500 HiC-Pro\r\n\u251c\u2500\u2500 logs\r\n\u251c\u2500\u2500 pastis\r\n\u251c\u2500\u2500 sequence\r\n\u2514\u2500\u2500 structure\r\n```\r\n\r\nThe following paths contain the most interesting results:\r\n\r\n- `WORKING_DIR/contact_maps/*.png` : contact maps.\r\n- `WORKING_DIR/HiC-Pro/output/hic_results/pic/*/*.pdf` : graphical summaries of read alignments produced by Hi-C Pro.\r\n- `WORKING_DIR/pastis/structure_RESOLUTION.pdb` : raw 3D models (in PDB format) produced by Pastis.\r\n- `WORKING_DIR/structure/RESOLUTION/structure_cleaned.*` : final (annotated) 3D models in PDB and G3D formats.\r\n\r\n> **Note**\r\n> - `WORKING_DIR` is the name of the working directory as specified in your config file.\r\n> - `RESOLUTION` is the resolution of the Hi-C data specified in the config file.\r\n\r\n## Examples\r\n\r\n- [Wild type model for *Neurospora crassa*](examples/n_crassa.md)\r\n- [Models built for the 3DGB paper](examples/paper/paper.md)\r\n\r\n## Visualize 3D model structures\r\n\r\nTo visualize 3D model structures (.pdb and .g3d files), follow this quick [tutorial](visualization/visualization.md).\r\n\r\n\r\n## Build DAG graph\r\n\r\nFor visualization purpose, you can build the graph of all computational steps involved in the 3D construction of the genome.\r\n\r\n```bash\r\nsnakemake --profile smk_profile --configfile YOUR-CONFIG.yml --rulegraph  | dot -Tpdf > rules.pdf\r\n```\r\n\r\nwhere `YOUR-CONFIG.yml` should be replaced by the name of the config file you created.\r\n\r\nWith wildcards:\r\n\r\n```bash\r\nsnakemake --profile smk_profile --configfile YOUR-CONFIG.yml --dag  | dot -Tpdf > dag.pdf\r\n```\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Functional genomics",
            "Genomics"
        ],
        "filtered_on": "profil.* in description",
        "id": "2010",
        "keep": "Keep",
        "latest_version": 1,
        "license": "BSD-3-Clause",
        "link": "https:/workflowhub.eu/workflows/2010?version=1",
        "name": "3D genome builder (3DGB)",
        "number_of_steps": 0,
        "projects": [
            "datafun"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-10-26",
        "versions": 1
    },
    {
        "create_time": "2025-10-24",
        "creators": [],
        "description": "This workflow encodes the top-ranking predicted pathways from the previous workflow into plasmids intended to be expressed in the specified organism. BASIC is used as assembly method. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid.* in description",
        "id": "2009",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/2009?version=1",
        "name": "Genetic Design (BASIC Assembly)",
        "number_of_steps": 3,
        "projects": [
            "galaxy-SynBioCAD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "brs"
        ],
        "tools": [
            "rpbasicdesign",
            "selenzy-wrapper",
            "dnabot"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-30",
        "versions": 1
    },
    {
        "create_time": "2025-10-24",
        "creators": [],
        "description": "This workflow encodes the top-ranking predicted pathways from the previous workflow into plasmids intended to be expressed in the specified organism. Assembly methods are Gibson, Golden or Ligation Chain Reaction.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Synthetic biology"
        ],
        "filtered_on": "plasmid.* in description",
        "id": "2007",
        "keep": "Reject",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/2007?version=1",
        "name": "Genetic Design (Gibson, Golden Gate, LCR)",
        "number_of_steps": 6,
        "projects": [
            "galaxy-SynBioCAD"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "dnaweaver",
            "LCRGenie",
            "PartsGenie",
            "sbml2sbol",
            "optdoe",
            "selenzy-wrapper"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-30",
        "versions": 1
    },
    {
        "create_time": "2025-10-22",
        "creators": [
            "Evangelos Karatzas",
            "Martin Beracochea"
        ],
        "description": "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new/nf-core/proteinfamilies)\r\n[![GitHub Actions CI Status](https://github.com/nf-core/proteinfamilies/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/proteinfamilies/actions/workflows/nf-test.yml)\r\n[![GitHub Actions Linting Status](https://github.com/nf-core/proteinfamilies/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/proteinfamilies/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/proteinfamilies/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.14881993-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.14881993)\r\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\r\n\r\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A525.04.0-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\r\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.4.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.4.1)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/proteinfamilies)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23proteinfamilies-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/proteinfamilies)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\r\n\r\n## Introduction\r\n\r\n**nf-core/proteinfamilies** is a bioinformatics pipeline that generates protein families from amino acid sequences and/or updates existing families with new sequences.\r\nIt takes a protein fasta file as input, clusters the sequences and then generates protein family Hidden Markov Models (HMMs) along with their multiple sequence alignments (MSAs).\r\nOptionally, paths to existing family HMMs and MSAs can be given (must have matching base filenames one-to-one) in order to update with new sequences in case of matching hits.\r\n\r\n### Check quality and pre-process\r\n\r\nGenerate input amino acid sequence statistics with ([`SeqFu`](https://github.com/telatin/seqfu2/)) and pre-process them (i.e., gap removal, convert to upper case, validate, filter by length, replace special characters such as `/`, and remove duplicate sequences) with ([`SeqKit`](https://github.com/shenwei356/seqkit/))\r\n\r\n### Create families\r\n\r\n1. Cluster sequences ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/))\r\n2. Perform multiple sequence alignment (MSA) ([`FAMSA`](https://github.com/refresh-bio/FAMSA/) or [`mafft`](https://github.com/GSLBiotech/mafft/))\r\n3. Optionally, clip gap parts of the MSA ([`ClipKIT`](https://github.com/JLSteenwyk/ClipKIT/))\r\n4. Generate family HMMs and fish additional sequences into the family ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\r\n5. Optionally, remove redundant and/or merge similar families by comparing family representative sequences against family models with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\r\n6. Optionally, from the remaining families, remove in-family redundant sequences by strictly clustering with ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/)) and keep cluster representatives\r\n7. Optionally, if in-family redundancy was not removed, reformat the `.sto` full MSAs to `.fas` with ([`HH-suite3`](https://github.com/soedinglab/hh-suite))\r\n8. Present statistics for remaining/updated family size distributions and representative sequence lengths ([`MultiQC`](http://multiqc.info/))\r\n\r\n### Update families\r\n\r\n1. Find which families to update by comparing the input sequences against existing family models with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\r\n2. For non hit sequences continue with the above: A. Create families. For hit sequences and families continue to: 3\r\n3. Extract family sequences ([`SeqKit`](https://github.com/shenwei356/seqkit/)) and concatenate with filtered hit sequences of each family\r\n4. Optionally, remove in-family redundant sequences by strictly clustering with ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/)) and keeping cluster representatives\r\n5. Perform multiple sequence alignment (MSA) ([`FAMSA`](https://github.com/refresh-bio/FAMSA/) or [`mafft`](https://github.com/GSLBiotech/mafft/))\r\n6. Optionally, clip gap parts of the MSA ([`ClipKIT`](https://github.com/JLSteenwyk/ClipKIT/))\r\n7. Update family HMM with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\r\n\r\n## Usage\r\n\r\n> [!NOTE]\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\r\n\r\nFirst, prepare a samplesheet with your input data that looks as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,fasta,existing_hmms_to_update,existing_msas_to_update\r\nCONTROL_REP1,input/mgnifams_input_small.faa,,\r\n```\r\n\r\nEach row contains a fasta file with amino acid sequences (can be zipped or unzipped).\r\nOptionally, a row may contain tarball archives (tar.gz) of existing families' HMM and MSA folders, in order to be updated.\r\nIn this case, the HMM and MSA files must be matching in numbers and in base filenames (not the extension).\r\nHit families/sequences will be updated, while no hit sequences will create new families.\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\nnextflow run nf-core/proteinfamilies \\\r\n   -profile <docker/singularity/.../institute> \\\r\n   --input samplesheet.csv \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\n> [!WARNING]\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\r\n\r\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/proteinfamilies/usage) and the [parameter documentation](https://nf-co.re/proteinfamilies/parameters).\r\n\r\n## Pipeline output\r\n\r\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/proteinfamilies/results) tab on the nf-core website pipeline page.\r\nFor more details about the output files and reports, please refer to the\r\n[output documentation](https://nf-co.re/proteinfamilies/output).\r\n\r\n## Credits\r\n\r\nnf-core/proteinfamilies was originally written by Evangelos Karatzas.\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- [Martin Beracochea](https://github.com/mberacochea)\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#proteinfamilies` channel](https://nfcore.slack.com/channels/proteinfamilies) (you can join with [this invite](https://nf-co.re/join/slack)).\r\n\r\n## Citations\r\n\r\nIf you use nf-core/proteinfamilies for your analysis, please cite the article as follows:\r\n\r\n> **nf-core/proteinfamilies: A scalable pipeline for the generation of protein families.**\r\n>\r\n> Evangelos Karatzas, Martin Beracochea, Fotis A. Baltoumas, Eleni Aplakidou, Lorna Richardson, James A. Fellows Yates, Daniel Lundin, nf-core community, Aydin Bulu\u00e7, Nikos C. Kyrpides, Ilias Georgakopoulos-Soares, Georgios A. Pavlopoulos & Robert D. Finn\r\n>\r\n> _biorxiv._ 2025 Aug. doi: [10.1101/2025.08.12.670010](https://dx.doi.org/10.1101/2025.08.12.670010).\r\n\r\nYou can cite the nf-core/proteinfamilies zenodo record for a specific version using the following doi: [10.5281/zenodo.14881993](https://doi.org/10.5281/zenodo.14881993).\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": "10.48546/workflowhub.workflow.1954.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1954",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1954?version=2",
        "name": "nf-core/proteinfamilies",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "proteomics",
            "protein-families"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-10-22",
        "versions": 2
    },
    {
        "create_time": "2025-10-19",
        "creators": [
            "Andreas Wilm"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-bacass_logo_dark.png\">\n    <img alt=\"nf-core/bacass\" src=\"docs/images/nf-core-bacass_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/bacass/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/bacass/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/bacass/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/bacass/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/bacass/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/bacass)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23bacass-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/bacass)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/bacass** is a bioinformatics best-practice analysis pipeline for simple bacterial assembly and annotation. The pipeline is able to assemble short reads, long reads, or a mixture of short and long reads (hybrid assembly).\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/bacass/results).\n\n## Pipeline summary\n\n### Short Read Assembly\n\nThis pipeline is primarily for bacterial assembly of next-generation sequencing reads. It can be used to quality trim your reads using [FastP](https://github.com/OpenGene/fastp) and performs basic sequencing QC using [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/). Afterwards, the pipeline performs read assembly using [Unicycler](https://github.com/rrwick/Unicycler). Contamination of the assembly is checked using [Kraken2](https://ccb.jhu.edu/software/kraken2/) and [Kmerfinder](https://bitbucket.org/genomicepidemiology/kmerfinder/src/master/) to verify sample purity.\n\n### Long Read Assembly\n\nFor users that only have Nanopore data, the pipeline quality trims these using [PoreChop](https://github.com/rrwick/Porechop) or filter long reads by quality using [Filtlong](https://github.com/rrwick/Filtlong) and assesses basic sequencing QC utilizing [NanoPlot](https://github.com/wdecoster/NanoPlot) and [PycoQC](https://github.com/a-slide/pycoQC). Contamination of the assembly is checked using [Kraken2](https://ccb.jhu.edu/software/kraken2/) and [Kmerfinder](https://bitbucket.org/genomicepidemiology/kmerfinder/src/master/) to verify sample purity.\n\nThe pipeline can then perform long read assembly utilizing [Unicycler](https://github.com/rrwick/Unicycler), [Miniasm](https://github.com/lh3/miniasm) in combination with [Racon](https://github.com/isovic/racon), [Canu](https://github.com/marbl/canu) or [Flye](https://github.com/fenderglass/Flye) by using the [Dragonflye](https://github.com/rpetit3/dragonflye)(\\*) pipeline. Long reads assembly can be polished using [Medaka](https://github.com/nanoporetech/medaka) or [NanoPolish](https://github.com/jts/nanopolish) with Fast5 files.\n\n> [!NOTE]\n> Dragonflye is a comprehensive pipeline designed for genome assembly of Oxford Nanopore Reads. It facilitates the utilization of Flye (default), Miniasm, and Raven assemblers, along with Racon (default) and Medaka polishers. For more information, visit the [Dragonflye GitHub](https://github.com/rpetit3/dragonflye) repository.\n\n### Hybrid Assembly\n\nFor users specifying both short read and long read (NanoPore) data, the pipeline can perform a hybrid assembly approach utilizing [Unicycler](https://github.com/rrwick/Unicycler) (short read assembly followed by gap closing with long reads) or [Dragonflye](https://github.com/rpetit3/dragonflye) (long read assembly followed by polishing with short reads), taking the full set of information from short reads and long reads into account.\n\n### Assembly QC and annotation\n\nIn all cases, the assembly is assessed using [QUAST](http://bioinf.spbau.ru/quast) and [BUSCO](https://busco.ezlab.org/). The resulting bacterial assembly is furthermore annotated using [Prokka](https://github.com/tseemann/prokka), [Bakta](https://github.com/oschwengers/bakta) or [DFAST](https://github.com/nigyta/dfast_core).\n\nIf Kmerfinder is invoked, the pipeline will group samples according to the [Kmerfinder](https://bitbucket.org/genomicepidemiology/kmerfinder/src/master/)-estimated reference genomes. Afterwards, two QUAST steps will be carried out: an initial ('general') [QUAST](http://bioinf.spbau.ru/quast) of all samples without reference genomes, and subsequently, a 'by reference genome' [QUAST](http://bioinf.spbau.ru/quast) to aggregate samples with their reference genomes.\n\n> [!NOTE]\n> This scenario is supported when [Kmerfinder](https://bitbucket.org/genomicepidemiology/kmerfinder/src/master/) analysis is performed only.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.tsv`:\n\n```tsv\nID      R1                            R2                            LongFastQ                    Fast5    GenomeSize\nshortreads      ./data/S1_R1.fastq.gz       ./data/S1_R2.fastq.gz       NA                            NA      NA\nlongreads       NA                          NA                          ./data/S1_long_fastq.gz      ./data/FAST5  2.8m\nshortNlong      ./data/S1_R1.fastq.gz       ./data/S1_R2.fastq.gz       ./data/S1_long_fastq.gz      ./data/FAST5  2.8m\n\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\nShort read assembly with Unicycler, `--kraken2db` can be any [compressed database (`.tar.gz`/`.tgz`)](https://benlangmead.github.io/aws-indexes/k2):\n\n```console\nnextflow run nf-core/bacass -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input samplesheet.tsv --assembly_type 'short' --kraken2db \"https://genome-idx.s3.amazonaws.com/kraken/k2_standard_8gb_20210517.tar.gz\"\n```\n\nLong read assembly with Miniasm:\n\n```console\nnextflow run nf-core/bacass -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input samplesheet.tsv --assembly_type 'long' --assembler 'miniasm' --kraken2db \"https://genome-idx.s3.amazonaws.com/kraken/k2_standard_8gb_20210517.tar.gz\"\n```\n\n```bash\nnextflow run nf-core/bacass \\\n  -profile <docker/singularity/.../institute> \\\n  --input samplesheet.tsv \\\n  --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/bacass/usage) and the [parameter documentation](https://nf-co.re/bacass/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/bacass/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/bacass/output).\n\n## Credits\n\nnf-core/bacass was initiated by [Andreas Wilm](https://github.com/andreas-wilm), originally written by [Alex Peltzer](https://github.com/apeltzer) (DSL1), rewritten by [Daniel Straub](https://github.com/d4straub) (DSL2) and maintained by [Daniel Valle-Millares](https://github.com/Daniel-VM).\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#bacass` channel](https://nfcore.slack.com/channels/bacass) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/bacass for your analysis, please cite it using the following doi: [10.5281/zenodo.2669428](https://doi.org/10.5281/zenodo.2669428)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "966",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/966?version=10",
        "name": "nf-core/bacass",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "bacterial-genomes",
            "denovo",
            "denovo-assembly",
            "genome-assembly",
            "hybrid-assembly",
            "nanopore",
            "nanopore-sequencing"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-10-19",
        "versions": 10
    },
    {
        "create_time": "2025-10-17",
        "creators": [
            "daniel rickert"
        ],
        "description": "# pb_variants \r\nA snakemake 9 based Pipeline for hifi snp, sv, cnv calling, phasing and more\r\n\r\nOnly PacBio data for now\r\n\r\n__!!THIS PIPLINE IS IN-DEVELOPMENT AND EXPERIMENTAL, USE AT YOUR OWN RISK!!__\r\n\r\n## what this tool aims to deliver:\r\n    - newest and best tools suited for HiFi data (only for now)\r\n    - singletons and trio analysis (trio is coming sometime...)\r\n    - human-first (hg38 for now), others should be possible (untested...)\r\n\r\n## included tools:\r\n- deepvariant or bcftools for snp calling\r\n- snps get used for phasing with whatshap and longphase\r\n- paraphase \r\n- trgt\r\n- hificnv \r\n- pb-cpg-tools (uses whatshap phased .bam file)\r\n- mitorsaw (just hg38)\r\n- sniffles for sv calls that get phased with longphase\r\n- sawfish for svs and cnvs (results phased by sawfish)\r\n- mosdepth, multiqc\r\n- pbmm2 for mapping\r\n- kraken2 for contamination detection (downsamples massively, needs kraken2 database)\r\n- demultiplexing of input as option, will not split the files per barcode.\r\n- for now one .bam per sample\r\n- NanoCaller for phased snp/indel calls\r\n- Variants get annotated if configured through config.yaml:\r\n    - SVs with sansa and AnnotSV\r\n    - SNPs with vep and snpsift\r\n    - both only tested with hg38 for now\r\n- if enabled, SNPs and SVs get overlaped (compared)\r\n    - svs: truvari overlaps sniffles as the \"truth\" to sawfish data for each sample\r\n    - snps: rtg-tools overlaps deepvariant or bcftools snps as the \"truth\" to nanocaller snps for each sample\r\n- if enabled, paraphase results get visualized with paraviewer\r\n\r\n## how to run\r\n- make sure you have a conda environment active with snakemake9+ (called smk9 in the runPipeline_local.sh)\r\n    - this can also be achieved by running the included setupPipeline_hpc.sh\r\n        - that script uses conda to create the env smk9 - with snakemake 9 installed already (check file smk9.yaml)\r\n- cp/mv/ln your unmapped .bam file into the root folder of this directory (pb_variants/your_bam_file_here.bam)\r\n- edit samplesheet.csv with your filename \r\n    - one sample per line, do not delete the header line (add to line2: your_bam_file_here.bam)\r\n- edit config.yaml to your liking/ folder structure (enable only things you want / need. keep in mind some analysis are hg38 only)\r\n- make sure you are in an interactive terminal session inside a screen / tmux or similar\r\n- bash runPipeline_local.sh for local installment on single-server setups, \r\n- bash runPipeline.sh on HPC \r\n- non-hpc users need to edit the config.yaml and enable deepvariant and disable deepvariant_hpc in the config.yaml:\r\nuse_deepvariant_hpc: True <- only set this to True on HPC HILBERT\r\n\r\n\r\n\r\n# DAG\r\nBefore each start, a DAG is created visualising the planned tasks for each sample.\r\nAn example:\r\n\r\n![alt text](dag.png)\r\n\r\nAll options enabled:\r\n\r\n![alt text](full.png)\r\n\r\nMinimalistic execution (and bcftools instead of Deepvariant) with only mandatory tasks:\r\n\r\n![alt text](minimalistic.png)\r\n\r\n\r\n## output files\r\n- the first step of the pipeline is to strip the kinetics data out of the .bam input file, but keep the methylation data. This makes all following processes much faster without any real data loss. \r\n- for each input sample:\r\n    - mosdepth and kraken (optional) report that get summarized with multiqc\r\n    - mapped .bam file haplotaged with whatshap and longphase\r\n    - the with whatshap phased bam is used for methylation track generation with cpg_tools\r\n    - bed/bw file for methylation tracks for IGV, should be used together with the whatshap phased output .bam file\r\n    - .vcf(.gz) file for:\r\n        - snps / indels from deepvariant or bcftools, phased with whatshap and longphase\r\n        - trgt\r\n        - paraphase\r\n        - mitorsaw\r\n        - hificnv\r\n        - sawfish sv / cnv \r\n        - svs from sniffles phased with longphase\r\n        - snps / indels from nanocaller\r\n    - phased snps and svs get annotated with snpsift and sansa and annotsv and vep   \r\n- snakemake report, rulegraph, copy of samplesheet and config.yaml with timestamp\r\n\r\n\r\n# general workflow in short:\r\n\r\n## prep the files:\r\n\r\n1. - move all .bam files into the folder pb_variants. (up to 100 at once makes sense, more overloads the hpc for sure)\r\n\t- with methylation, no kinetics needed\r\n\r\n2. - edit the config.yaml according to your needs. Mostly True/False. If unsure, keep as is but CHANGE THE OUTPUT DIRECTORY\r\n\r\n3. - edit the samplesheet.csv: keep the first line and then list all .bam files that you want to analyze in the run. all files listed in that file NEED TO BE IN THE FOLDER \r\n\r\n## now the actual pipeline execution:\r\n\r\n1. - start a screen session on the hpc\r\n\r\n2. - start an interactive job inside the screen session with at least 2 days runtime\r\n\r\n3. - cd into the folder pb_variants\r\n\r\n4. - activate the correct conda env with: \"conda activate smk9\"\r\n\r\n5. - run the pipeline with:\"bash runPipeline.sh\"\r\n\r\n\r\n## supervising the run:\r\n\r\n- check the screen regulary if you want -red colour is bad\r\n\r\n- qstat -u your_hpc_username if you want\r\n\r\n- check outputfolder if files are appearing\r\n\r\n- check clusterlogs_your_hpc_username for errors\r\n\r\n- check outputfolder/logs for errors that occured\r\n\r\n- if a multiqc_report.html is in the outputfolder then the pipeline is done\r\n\r\n- if the interactive job is done, but the pipeline is not done yet, retry\r\n\r\n- if you want to understand more, email me\r\n\r\n\r\n## transfering results:\r\n\r\n- you can transfer the complete outputfolder if you want. all results are stored in subfolders that should explain themselves.\r\n\r\n\r\n## roadmap:\r\n  trio calling : deeptrio, glnexus  -> only if there are requests for this. \r\n  assembly : hifiasm or similar -> only if there are requests for this. \r\n  str profiling: strkit, currently testing\r\n\r\n## why this work is being done:\r\n- nf-core/pacvar: https://nf-co.re/pacvar/1.0.1/\r\n    - does not run without sudo for us\r\n    - seems not mature enough (imho)\r\n    - not newest tools included\r\n    - not all wanted tools included\r\n\r\n- pacbios wdl-based workflow: https://github.com/PacificBiosciences/HiFi-somatic-WDL\r\n    - doesnt run on our hardware\r\n\r\n- other, locally developed snakemake-based workflows: (eg: https://github.com/core-unit-bioinformatics/workflow-smk-longread-variant-calling)\r\n    - not all wanted tools included\r\n\r\n- Radboud's Valentine workflow:\r\n    - not available to us\r\n    - not all wanted tools included\r\n\r\n- smrtlinks internal pipeline:\r\n    - singularity not working, limited tool options\r\n    - not all wanted tools included\r\n",
        "doi": "10.48546/workflowhub.workflow.1965.2",
        "edam_operation": [],
        "edam_topic": [
            "Genomics",
            "Human genetics"
        ],
        "filtered_on": "profil.* in description",
        "id": "1965",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1965?version=2",
        "name": "Snakemake workflow for PacBio WGS short and long variant calling, phasing and much more",
        "number_of_steps": 0,
        "projects": [
            "WGGC"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "snps",
            "snakemake",
            "dna-methylation",
            "structural variants"
        ],
        "tools": [
            "Snakemake",
            "DeepVariant",
            "Sniffles",
            "BCFtools",
            "SAMtools",
            "FastQC",
            "Variant Effect Predictor (VEP)",
            "SnpSift",
            "AnnotSV",
            "pbmm2"
        ],
        "type": "Snakemake",
        "update_time": "2025-10-28",
        "versions": 2
    },
    {
        "create_time": "2025-10-17",
        "creators": [
            "Diane Duroux"
        ],
        "description": "\r\n# \ud83d\udcc4 Generalizable machine learning models for rapid antimicrobial resistance prediction in unseen healthcare settings\r\n\r\nThis repository contains the code used for the experiments in the paper:\r\n\r\n**_Generalizable machine learning models for rapid antimicrobial resistance prediction in unseen healthcare settings_**  \r\nby *Diane Duroux, Paul P. Meyer, Giovanni Vison\u00e0, and Niko Beerenwinkel*.\r\n\r\n## \u2699\ufe0f Install the dependencies\r\nYou can set up the project with either pip or uv.\r\n\r\n### Option A - pip:\r\nInstall the necessary dependencies listed in the requirements.txt file\r\n\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n\r\n### Option B - uv:\r\nWe provide pyproject.toml and uv.lock for macOS, Windows, and Linux.\r\n\r\nNote: On a Linux or non-apple silicon  please use the pyproject.toml file for Mac and rewrite the uv.lock after installation. \r\n\r\n```bash\r\n# 0) Install uv (one-time)\r\n# mac/linux:\r\ncurl -LsSf https://astral.sh/uv/install.sh | sh\r\n# windows (PowerShell):\r\niwr https://astral.sh/uv/install.ps1 -UseBasicParsing | iex\r\n \r\n# 1) Ensure the pinned Python is available (adjust if your pyproject pins a version)\r\nuv python install 3.11\r\n \r\n# 2) Create the exact environment from the lockfile\r\nuv sync --frozen\r\n \r\n# 3) Run your code within the env\r\nuv run python -V\r\nuv run python your_script.py\r\n```\r\n\r\n## \ud83d\udcbb AMR Classifier Training with ResMLP and inference\r\n\r\nThe following command trains a ResMLP model for AMR classification using the preprocessed DRIAMS data.\r\n\r\n### \ud83d\udce6 Output\r\n\r\nIn `output/<experiment_group>/<experiment_name>_results/`, the script generates:\r\n\r\n- `test_set_seed0.csv`  \r\n  \u27a4 Contains predictions: `species`, `sample_id`, `drug`, `response`, and `Prediction`.\r\n\r\n### \ud83d\udee0 Required Arguments\r\n\r\n| Argument                | Description                                                                                     |\r\n|-------------------------|-------------------------------------------------------------------------------------------------|\r\n| `--driams_long_table`   | Path to the metadata file for the current dataset.                                              |\r\n| `--spectra_matrix`      | Path to the input mass spectra (either raw or MAE-encoded).                                     |\r\n| `--sample_embedding_dim`| Dimension of the spectra input (6000 for raw, or same as <encoding_dim> for MAE).               |\r\n| `--drugs_df`            | Path to the antimicrobial compound encoding file.                                               |\r\n| `--fingerprint_class`   | Type of encoding: `'morgan_1024'`, `'molformer_github'`, or `'selfies_flattened_one_hot'`.      |\r\n| `--fingerprint_size`    | Size of the encoding: 1024 (Morgan), 768 (Molformer), or 24160 (SELFIES).                       |\r\n| `--split_type`          | Set to `specific` if splits are pre-defined, else random.                                       |\r\n| `--split_ids`           | Path to the `data_splits.csv` file.                                                             |\r\n| `--experiment_group`    | Name of the output folder.                                                                      |\r\n| `--experiment_name`     | Name of the output subfolder.                                                                   |\r\n| `--seed`                | Random seed for reproducibility.                                                                |\r\n| `--n_epochs`            | Number of epochs for classifier training.                                                       |\r\n| `--learning_rate`       | Learning rate for the optimizer.                                                                |\r\n| `--patience`            | Number of epochs to wait before early stopping.                                                 |\r\n| `--batch_size`          | Batch size for classifier training.                                                             |\r\n\r\n### \ud83d\ude80 Example: ResMLP Training on DRIAMS B2018 with Raw Spectra + Morgan Fingerprints\r\n\r\n```bash\r\nulimit -Sn 10000  # Optional: increase file descriptor limit if needed\r\n\r\npython3 code/ResAMR_classifier.py \\\r\n    --driams_long_table ProcessedData/B2018/combined_long_table.csv \\\r\n    --spectra_matrix ProcessedData/B2018/rawSpectra_data.npy \\\r\n    --sample_embedding_dim 6000 \\\r\n    --drugs_df OriginalData/drug_fingerprints_Mol_selfies.csv \\\r\n    --fingerprint_class morgan_1024 \\\r\n    --fingerprint_size 1024 \\\r\n    --split_type specific \\\r\n    --split_ids ProcessedData/B2018/data_splits.csv \\\r\n    --experiment_group rawMS_MorganFing \\\r\n    --experiment_name ResMLP \\\r\n    --seed 0 \\\r\n    --n_epochs 2 \\\r\n    --learning_rate 0.0003 \\\r\n    --patience 10 \\\r\n    --batch_size 128\r\n```\r\n\r\n---\r\n\r\n## \ud83d\udcb0 Funding\r\n\r\nThis research was primarily supported by the ETH AI Center.\r\n",
        "doi": "10.48546/workflowhub.workflow.1999.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "antimicrobial.* in name",
        "id": "1999",
        "keep": "Keep",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1999?version=1",
        "name": "Generalizable machine learning models for rapid antimicrobial resistance prediction in unseen healthcare settings",
        "number_of_steps": 0,
        "projects": [
            "AMRMALDI"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2025-10-17",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Maren B\u00f6se",
            "Savas Ceylan",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis workflow generates **evolutionary ShakeMaps** by combining multiple parametric data sources:  \r\nevent alerts, automatic and manual peak motions, and crowdsourced felt reports.\r\n\r\nIt integrates **pyFinDer**, **FinDer**, **ShakeMap**, **RRSM**, and **EMSC** to continuously update ground motion maps as new information arrives.\r\n\r\nIn its full configuration, WF7602 is also capable of:\r\n- Sending **event alerts** for each ShakeMap update.\r\n\r\n**Note:** In this distribution, the alerting feature is **disabled**.\r\n\r\nThe main CWL definition is provided in `WF7602.cwl`.  \r\nWorkflow metadata follows the [workflow-ro-crate-1.0](https://w3id.org/workflowhub/workflow-ro-crate/1.0) profile.\r\n\r\n## Workflow Structure\r\nThe workflow consists of the following datasets (DT) and software services (SS):\r\n\r\n1. **Data Ingestion**\r\n   - **DT7602:** Event alerts from seismic services.\r\n   - **DT7603:** Automated peak motions from RRSM.\r\n   - **DT7604:** Not Implemented: Felt reports from EMSC.\r\n   - **DT7605:** Manual peak motions from ESM.\r\n\r\n2. **Processing & ShakeMap Generation**\r\n   - **SS7602:** Parametric web services and Python wrapper for FinDer (SS7603) \r\n   - **SS7603:** FinDer \u2013 finite fault estimation.\r\n   - **SS7601:** Swiss, Italian and European ShakeMap \u2013 produces ground motion maps.\r\n\r\n3. **Output Storage**\r\n   - **DT7606:** Evolutionary shake maps calculated at every update.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1998",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1998?version=1",
        "name": "[DTC-E6] WF7602: ShakeMaps from Parametric Data",
        "number_of_steps": 8,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophyics"
        ],
        "tools": [
            "Finite-source characterization using template matching (FinDer).",
            "Generate ground motion and shaking intensity maps.",
            "Automated peak motions from RRSM.",
            "EMSC felt reports - planned but not implemented",
            "ML-based algorithm to estimate peak motions at stations.",
            "Integrate manual peak motions into the workflow.",
            "Prepare evolutionary shake maps for output using finite fault characterization.",
            "Collect event alerts."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-14",
        "creators": [
            "Maren B\u00f6se",
            "Savas Ceylan",
            "Johannes Kemper"
        ],
        "description": "## Overview\r\nThis workflow produces **synthetic shaking simulations** for historical earthquake events.  \r\nIt integrates **SeisComP**, **FinDer**, and the **Swiss, Italian, and European ShakeMap** implementation to process continuous seismic data, estimate rupture parameters, and generate **evolutionary shake maps**.\r\n\r\nIn its full configuration, WF7601 is also capable of:\r\n- Sending **event alerts** when specific trigger conditions are met.\r\n- Pushing generated ShakeMaps to an external **web portal** for dissemination.\r\n\r\n**Note:** In this distribution, both the alerting and ShakeMap publishing features are **disabled**.\r\n\r\nThe main CWL definition is provided in `WF7601.cwl`.  \r\nWorkflow metadata follows the [workflow-ro-crate-1.0](https://w3id.org/workflowhub/workflow-ro-crate/1.0) profile.\r\n\r\n## Workflow Structure\r\nThe workflow consists of the following datasets (DT) and software services (SS):\r\n\r\n1. **Data Ingestion & Preprocessing**\r\n   - **DT7601:** Continuous seismic data (waveforms) from seismic stations. Replaced by a *playback* module to mimic data stream using past earthquakes.\r\n   - **SS7602:** SeisComP \u2013 for seismic data acquisition and processing.\r\n\r\n2. **Finite Fault Estimation**\r\n   - **SS7603:** FinDer \u2013 finite fault earthquake early warning algorithm.\r\n\r\n3. **ShakeMap Generation**\r\n   - **SS7601:** Swiss, Italian and European ShakeMap \u2013 generates synthetic ground motion maps.\r\n\r\n4. **Output Storage**\r\n   - **DT7606:** Evolutionary shake maps calculated at multiple update intervals between 0-60 s.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1997",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1997?version=1",
        "name": "[DTC-E6] WF7601: Synthetic Shaking Workflow",
        "number_of_steps": 6,
        "projects": [
            "WP7 - Earthquakes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dt-geo",
            "digital twin",
            "geophysics"
        ],
        "tools": [
            "Generate ground motion and shaking intensity maps.",
            "Finite-source characterization using template matching (FinDer).",
            "ML-based algorithm to estimate peak motions at stations.",
            "Seedlink streaming to processing infrastructure.",
            "Receive continuous seismic waveform data.",
            "Prepare evolutionary shake maps for output using finite fault characterization."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-14",
        "versions": 1
    },
    {
        "create_time": "2025-10-10",
        "creators": [
            "Daniel Straub",
            "Alexander Peltzer"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-ampliseq_logo_dark.png\">\n    <img alt=\"nf-core/ampliseq\" src=\"docs/images/nf-core-ampliseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/ampliseq/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/ampliseq/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/ampliseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/ampliseq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/ampliseq/results)[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.1493841-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.1493841)[![Cite Publication](https://img.shields.io/badge/Cite%20Us!-Cite%20Publication-important?labelColor=000000)](https://doi.org/10.3389/fmicb.2020.550420)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/ampliseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23ampliseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/ampliseq)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)[![Watch on YouTube](http://img.shields.io/badge/youtube-ampliseq-FFFF00?labelColor=000000&logo=youtube)](https://youtu.be/a0VOEeAvETs)\n\n## Introduction\n\n**nfcore/ampliseq** is a bioinformatics analysis pipeline used for amplicon sequencing, supporting denoising of any amplicon and supports a variety of taxonomic databases for taxonomic assignment including 16S, ITS, CO1 and 18S. Phylogenetic placement is also possible. Multiple region analysis such as 5R is implemented. Supported is paired-end Illumina or single-end Illumina, PacBio and IonTorrent data. Default is the analysis of 16S rRNA gene amplicons sequenced paired-end with Illumina.\n\nA video about relevance, usage and output of the pipeline (version 2.1.0; 26th Oct. 2021) can also be found in [YouTube](https://youtu.be/a0VOEeAvETs) and [billibilli](https://www.bilibili.com/video/BV1B44y1e7MM), the slides are deposited at [figshare](https://doi.org/10.6084/m9.figshare.16871008.v1).\n\n<p align=\"center\">\n    <img src=\"docs/images/ampliseq_workflow.png\" alt=\"nf-core/ampliseq workflow overview\" width=\"60%\">\n</p>\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/ampliseq/results).\n\n## Pipeline summary\n\nBy default, the pipeline currently performs the following:\n\n- Sequencing quality control ([FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n- Trimming of reads ([Cutadapt](https://journal.embnet.org/index.php/embnetjournal/article/view/200))\n- Infer Amplicon Sequence Variants (ASVs) ([DADA2](https://doi.org/10.1038/nmeth.3869))\n- Optional post-clustering with [VSEARCH](https://github.com/torognes/vsearch)\n- Predict whether ASVs are ribosomal RNA sequences ([Barrnap](https://github.com/tseemann/barrnap))\n- Phylogenetic placement ([EPA-NG](https://github.com/Pbdas/epa-ng))\n- Taxonomical classification using DADA2; alternatives are [SINTAX](https://doi.org/10.1101/074161), [Kraken2](https://doi.org/10.1186/s13059-019-1891-0), and [QIIME2](https://www.nature.com/articles/s41587-019-0209-9)\n- Excludes unwanted taxa, produces absolute and relative feature/taxa count tables and plots, plots alpha rarefaction curves, computes alpha and beta diversity indices and plots thereof ([QIIME2](https://www.nature.com/articles/s41587-019-0209-9))\n- Creates phyloseq R objects ([Phyloseq](https://www.bioconductor.org/packages/release/bioc/html/phyloseq.html) and [TreeSE](https://doi.org/10.12688/f1000research.26669.2))\n- Pipeline QC summaries ([MultiQC](https://multiqc.info/))\n- Pipeline summary report ([R Markdown](https://github.com/rstudio/rmarkdown))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, you need to know whether the sequencing files at hand are expected to contain primer sequences (usually yes) and if yes, what primer sequences. In the example below, the paired end sequencing data was produced with 515f (GTGYCAGCMGCCGCGGTAA) and 806r (GGACTACNVGGGTWTCTAAT) primers of the V4 region of the 16S rRNA gene. Please note, that those sequences should not contain any sequencing adapter sequences, only the sequence that matches the biological amplicon.\n\nNext, the data needs to be organized in a folder, here `data`, or detailed in a samplesheet (see [input documentation](https://nf-co.re/ampliseq/usage#input-specifications)).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/ampliseq \\\n   -profile <docker/singularity/.../institute> \\\n   --input \"data\" \\\n   --FW_primer \"GTGYCAGCMGCCGCGGTAA\" \\\n   --RV_primer \"GGACTACNVGGGTWTCTAAT\" \\\n   --outdir <OUTDIR>\n```\n\n> [!NOTE]\n> Adding metadata will considerably increase the output, see [metadata documentation](https://nf-co.re/ampliseq/usage#metadata).\n\n> [!TIP]\n> By default the taxonomic assignment will be performed with DADA2 on SILVA database, but there are various tools and databases readily available, see [taxonomic classification documentation](https://nf-co.re/ampliseq/usage#taxonomic-classification). Differential abundance testing with ([ANCOM](https://www.ncbi.nlm.nih.gov/pubmed/26028277)) or ([ANCOM-BC](https://www.ncbi.nlm.nih.gov/pubmed/32665548)) when opting in.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/ampliseq/usage) and the [parameter documentation](https://nf-co.re/ampliseq/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/ampliseq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/ampliseq/output).\n\n## Credits\n\nnf-core/ampliseq was originally written by Daniel Straub ([@d4straub](https://github.com/d4straub)) and Alexander Peltzer ([@apeltzer](https://github.com/apeltzer)) for use at the [Quantitative Biology Center (QBiC)](https://www.info.qbic.uni-tuebingen.de/) and [Microbial Ecology, Center for Applied Geosciences](http://www.uni-tuebingen.de/de/104325), part of Eberhard Karls Universit\u00e4t T\u00fcbingen (Germany). Daniel Lundin [@erikrikarddaniel](https://github.com/erikrikarddaniel) ([Linnaeus University, Sweden](https://lnu.se/)) joined before pipeline release 2.0.0 and helped to improve the pipeline considerably.\n\nWe thank the following people for their extensive assistance in the development of this pipeline (in alphabetical order):\n\n[Adam Bennett](https://github.com/a4000), [Diego Brambilla](https://github.com/DiegoBrambilla), [Emelie Nilsson](https://github.com/emnilsson), [Jeanette T\u00e5ngrot](https://github.com/jtangrot), [Lokeshwaran Manoharan](https://github.com/lokeshbio), [Marissa Dubbelaar](https://github.com/marissaDubbelaar), [Sabrina Krakau](https://github.com/skrakau), [Sam Minot](https://github.com/sminot), [Till Englert](https://github.com/tillenglert)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#ampliseq` channel](https://nfcore.slack.com/channels/ampliseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use `nf-core/ampliseq` for your analysis, please cite the `ampliseq` article as follows:\n\n> **Interpretations of Environmental Microbial Community Studies Are Biased by the Selected 16S rRNA (Gene) Amplicon Sequencing Pipeline**\n>\n> Daniel Straub, Nia Blackwell, Adrian Langarica-Fuentes, Alexander Peltzer, Sven Nahnsen, Sara Kleindienst\n>\n> _Frontiers in Microbiology_ 2020, 11:2652 [doi: 10.3389/fmicb.2020.550420](https://doi.org/10.3389/fmicb.2020.550420).\n\nYou can cite the `nf-core/ampliseq` zenodo record for a specific version using the following [doi: 10.5281/zenodo.1493841](https://zenodo.org/badge/latestdoi/150448201)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "964",
        "keep": "To Curate",
        "latest_version": 28,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/964?version=28",
        "name": "nf-core/ampliseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "16s",
            "18s",
            "its",
            "metabarcoding",
            "metagenomics",
            "amplicon-sequencing",
            "edna",
            "illumina",
            "iontorrent",
            "metataxonomics",
            "microbiome",
            "pacbio",
            "qiime2",
            "rrna",
            "taxonomic-classification",
            "taxonomic-profiling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-10-10",
        "versions": 28
    },
    {
        "create_time": "2025-10-07",
        "creators": [
            "Alem Gusinac",
            "Thomas Ederveen",
            "Jos Boekhorst",
            "Annemarie Boleij"
        ],
        "description": "[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.10.0-23aa62.svg?labelColor=000000)](https://www.nextflow.io/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![nf-test](https://img.shields.io/badge/tested_with-nf--test-337ab7.svg)](https://code.askimed.com/nf-test)\r\n\r\n## Introduction: **metaBIOMx**\r\n\r\nThe metagenomics microbiomics pipeline is a best-practice suite for the decontamination and annotation of sequencing data obtained via short-read shotgun sequencing. The pipeline contains [NF-core modules](https://github.com/nf-core/modules) and other local modules that are in the similar format. It can be runned via both docker and singularity containers.\r\n\r\n## Pipeline summary\r\nThe pipeline is able to perform different taxonomic annotation on either (single/paired) reads or contigs. The different subworkflows can be defined via `--bypass_<method>` flags, a full overview is shown by running `--help`. By default the pipeline will check if the right databases are present in the right formats, when the path is provided. If this is not the case, compatible databases will be automatically downloaded.\r\n\r\nFor both subworkflows the pipeline will perform read trimming via [Trimmomatic](https://github.com/timflutre/trimmomatic) and/or [AdapterRemoval](https://github.com/MikkelSchubert/adapterremoval), followed by human removal via [Kneaddata](https://huttenhower.sph.harvard.edu/kneaddata/). Before and after each step the quality control will be assessed via [fastqc](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and a [multiqc](https://github.com/MultiQC/MultiQC) report is created as output. Then taxonomy annotation is done as follows:\r\n\r\n**Read annotation**\r\n- paired reads are interleaved using [BBTools](https://archive.jgi.doe.gov/data-and-tools/software-tools/bbtools/).\r\n- [MetaPhlAn3](https://huttenhower.sph.harvard.edu/metaphlan/) and [HUMAnN3](https://huttenhower.sph.harvard.edu/humann/) are used for taxonomy and functional profiling.\r\n- taxonomy profiles are merged into a single BIOM file using [biom-format](https://github.com/biocore/biom-format).\r\n\r\n**Contig annotation**\r\n- read assembly is performed via [SPAdes](http://cab.spbu.ru/software/spades/).\r\n- Quality assesment of contigs is done via [Busco](https://busco.ezlab.org/).\r\n- taxonomy profiles are created using [CAT](https://github.com/dutilh/CAT).\r\n- Read abundance estimation is performed on the contigs using [Bowtie2]() and [BCFtools](http://samtools.github.io/bcftools/bcftools.html).\r\n- Contigs are selected if a read can be aligned against a contig and a BIOM file is generated using [biom-format](https://github.com/biocore/biom-format).\r\n\r\n## Installation\r\n> [!NOTE]\r\n> Make sure you have installed the latest [nextflow](https://www.nextflow.io/docs/latest/install.html#install-nextflow) version! \r\n\r\nClone the repository in a directory of your choice:\r\n```bash\r\ngit clone https://github.com/CMG-GUTS/metabiomx.git\r\n```\r\n\r\nThe pipeline is containerised, meaning it can be runned via docker or singularity images. No further actions need to be performed when using the docker profile, except a docker registery needs to be set on your local system, see [docker](https://docs.docker.com/engine/install/). In case singularity is used, images are automatically cached within the project directory.\r\n\r\n## Usage\r\nSince the latest version, metaBIOMx works with both a samplesheet (CSV) format or a path to the input files. Preferably, samplesheets should be provided.\r\n```bash\r\nnextflow run main.nf --input <samplesheet.csv> -work-dir work -profile singularity\r\nnextflow run main.nf --input <'*_{1,R1,2,R2}.{fq,fq.gz,fastq,fastq.gz}'> -work-dir work -profile singularity\r\n```\r\n\r\n### \ud83d\udccb Sample Metadata File Specification\r\n\r\nmetaBIOMx expects your sample input data to follow a **simple, but strict** structure to ensure compatibility and allow upfront validation. The input should be provided as a **CSV** file where **each entry = one sample** with specified sequencing file paths. Additional properties not mentioned here will be ignored by the validation step.\r\n\r\n### **Properties and Validation Rules**\r\n\r\n#### \ud83d\udd39 Required properties\r\n\r\n| Property     | Type   | Rules / Description                                                                                   |\r\n|--------------|--------|----------------------------------------------------------------------------------------------------|\r\n| `sample_id`     | string | Unique sample ID with no spaces (`^\\S+$`). Serves as an identifier.                                  |\r\n| `forward_read` | string | File path to forward sequencing read. Must be non-empty string matching FASTQ gzipped pattern. File must exist. |\r\n\r\n#### \ud83d\udd39 Optional property\r\n\r\n| Property       | Type   | Rules / Description                                                                                   |\r\n|----------------|--------|----------------------------------------------------------------------------------------------------|\r\n| `reverse_read` | string | File path to reverse sequencing read. Same constraints as `forward_read`. Required if specified.   |\r\n\r\n#### \ud83d\udd39 Pattern\u2011based columns \r\nYou can define extra variables using special prefixes:\r\n- **`CONTRAST_...`** \u2192 grouping/category labels used in differential comparisons  \r\n  Example: `CONTRAST_Treatment` with values `Drug` / `Placebo`\r\nThese prefixes are used to generate an automated `OmicFlow` report with alpha, beta diversity and compositional plots. For more information see [OmicFlow](https://github.com/agusinac/OmicFlow).\r\n\r\n### Example cases\r\n#### \ud83d\udd39 Read annotation\r\n```bash\r\nnextflow run main.nf \\\r\n    --input <samplesheet.csv> \\\r\n    # (optional) --bypass_trim \\\r\n    # (optional) --bypass_decon \\\r\n    --bypass_contig_annotation \\\r\n    -work-dir work \\\r\n    -profile singularity\r\n```\r\n\r\n#### \ud83d\udd39 Contig annotation\r\n```bash\r\nnextflow run main.nf \\\r\n    --input <samplesheet.csv> \\\r\n    # (optional) --bypass_trim \\\r\n    # (optional) --bypass_decon \\\r\n    --bypass_read_annotation \\\r\n    -work-dir work \\\r\n    -profile singularity\r\n```\r\n\r\nIn case you only have assemblies and wish to perform contig annotation:\r\n```bash\r\nnextflow run main.nf \\\r\n    --input <samplesheet.csv> \\\r\n    --bypass_assembly \\\r\n    --bypass_read_annotation \\\r\n    -work-dir work \\\r\n    -profile singularity\r\n```\r\n\r\n## Automatic database setup\r\nThe pipeline requires a set of databases which are used by the different tools within this workflow. The user is required to specify the location in where the databases will be downloaded. It is also possible to download the databases manually. The `configure` subworkflow will evaluate the database format and presence of the compatible files automatically.\r\n```bash\r\nnextflow run main.nf \\\r\n    --bowtie_db path/to/db/bowtie2 \\\r\n    --metaphlan_db path/to/db/metaphlan \\\r\n    --humann_db path/to/db/humann \\\r\n    --cat_pack_db path/to/db/catpack \\\r\n    --busco_db path/to/db/busco_downloads \\\r\n    -work-dir <work/dir> \\\r\n    -profile <singularity,docker>\r\n```\r\n\r\n<details>\r\n<summary>Manual database setup</summary>\r\n\r\n### HUMAnN3 and MetaPhlan3 DB\r\nMake sure the `path/to/db/humann` should contain a `chocophlan`, `uniref` and `utility_mapping` directory. These can be obtained by the following command:\r\n```bash\r\ndocker pull biobakery/humann:latest\r\n\r\ndocker run --rm -v $(pwd):/scripts biobakery/humann:latest \\\r\n    humann_databases --download chocophlan full ./path/to/db/humann \\\r\n    && humann_databases --download uniref uniref90_diamond ./path/to/db/humann \\\r\n    && humann_databases --download utility_mapping full ./path/to/db/humann\r\n```\r\n\r\n### MetaPhlAn DB\r\n```bash\r\nwget http://cmprod1.cibio.unitn.it/biobakery4/metaphlan_databases/mpa_vJun23_CHOCOPhlAnSGB_202403.tar \\\r\n    && tar -xvf mpa_vJun23_CHOCOPhlAnSGB_202403.tar -C path/to/db/metaphlan \\\r\n    && rm mpa_vJun23_CHOCOPhlAnSGB_202403.tar\r\n\r\nwget http://cmprod1.cibio.unitn.it/biobakery4/metaphlan_databases/bowtie2_indexes/mpa_vJun23_CHOCOPhlAnSGB_202403_bt2.tar \\\r\n    && tar -xvf mpa_vJun23_CHOCOPhlAnSGB_202403_bt2.tar -C path/to/db/metaphlan \\\r\n    && rm mpa_vJun23_CHOCOPhlAnSGB_202403_bt2.tar\r\n\r\necho 'mpa_vJun23_CHOCOPhlAnSGB_202403' > path/to/db/metaphlan/mpa_latest\r\n```\r\n\r\n### Kneaddata DB\r\n```bash\r\ndocker pull agusinac/kneaddata:latest\r\n\r\ndocker run --rm -v $(pwd):/scripts agusinac/kneaddata:latest \\\r\n    kneaddata_database \\\r\n        --download human_genome bowtie2 ./path/to/db/bowtie2\r\n```\r\n\r\n### CAT_pack DB\r\nA pre-constructed diamond database can be [downloaded](https://tbb.bio.uu.nl/tina/CAT_pack_prepare/) manually or by command:\r\n```bash\r\ndocker pull agusinac/catpack:latest\r\n\r\ndocker run --rm -v $(pwd):/scripts agusinac/catpack:latest \\\r\n    CAT_pack download \\\r\n        --db nr \\\r\n        -o path/to/db/catpack\r\n\r\n```\r\n\r\n### busco DB\r\nBUSCO expects that the directory is called `busco_downloads`.\r\n```bash\r\ndocker pull ezlabgva/busco:v5.8.2_cv1\r\n\r\ndocker run --rm -v $(pwd):/scripts ezlabgva/busco:v5.8.2_cv1 \\\r\n    busco \\\r\n        --download bacteria_odb12 \\\r\n        --download_path path/to/db/busco_downloads\r\n```\r\n</details>\r\n\r\n## Support\r\n\r\nIf you are having issues, please [create an issue](https://github.com/CMG-GUTS/metabiomx/issues)\r\n\r\n## Citations\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md)\r\nfile.\r\n",
        "doi": "10.48546/workflowhub.workflow.1787.6",
        "edam_operation": [
            "Gene functional annotation",
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomic sequencing",
            "Metagenomics",
            "Microbial ecology",
            "Microbiology"
        ],
        "filtered_on": "edam",
        "id": "1787",
        "keep": "Keep",
        "latest_version": 6,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1787?version=6",
        "name": "metaBIOMx: Metagenomics pipeline for Microbial shot-gun sequencing data",
        "number_of_steps": 0,
        "projects": [
            "CMG-GUTS"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "metagenomics",
            "nextflow"
        ],
        "tools": [
            "AdapterRemoval",
            "BCFtools",
            "BBTools",
            "Trimmomatic",
            "FastQC",
            "MultiQC",
            "humann",
            "MetaPhlAn",
            "SPAdes",
            "BUSCO",
            "Bowtie 2",
            "CAT and BAT",
            "OmicFlow"
        ],
        "type": "Nextflow",
        "update_time": "2025-10-08",
        "versions": 3
    },
    {
        "create_time": "2025-10-06",
        "creators": [
            "Jasmin Frangenberg",
            "Anan Ibrahim",
            "James A. Fellows Yates"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-funcscan_logo_flat_dark.png\">\n    <img alt=\"nf-core/funcscan\" src=\"nf-core-funcscan_logo_flat_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/funcscan/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/funcscan/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/funcscan/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/funcscan/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/funcscan/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7643099-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7643099)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/funcscan)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23funcscan-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/funcscan)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n![HiRSE Code Promo Badge](https://img.shields.io/badge/Promo-8db427?style=plastic&label=HiRSE&labelColor=005aa0&link=https%3A%2F%2Fgo.fzj.de%2FCodePromo)\n\n## Introduction\n\n**nf-core/funcscan** is a bioinformatics best-practice analysis pipeline for the screening of nucleotide sequences such as assembled contigs for functional genes. It currently features mining for antimicrobial peptides, antibiotic resistance genes and biosynthetic gene clusters.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/funcscan/results).\n\nThe nf-core/funcscan AWS full test dataset are contigs generated by the MGnify service from the ENA. We used contigs generated from assemblies of chicken cecum shotgun metagenomes (study accession: MGYS00005631).\n\n## Pipeline summary\n\n1. Quality control of input sequences with [`SeqKit`](https://bioinf.shenwei.me/seqkit/)\n2. Taxonomic classification of contigs of **prokaryotic origin** with [`MMseqs2`](https://github.com/soedinglab/MMseqs2)\n3. Annotation of assembled prokaryotic contigs with [`Prodigal`](https://github.com/hyattpd/Prodigal), [`Pyrodigal`](https://github.com/althonos/pyrodigal), [`Prokka`](https://github.com/tseemann/prokka), or [`Bakta`](https://github.com/oschwengers/bakta)\n4. Annotation of coding sequences from 3. to obtain general protein families and domains with [`InterProScan`](https://github.com/ebi-pf-team/interproscan)\n5. Screening contigs for antimicrobial peptide-like sequences with [`ampir`](https://cran.r-project.org/web/packages/ampir/index.html), [`Macrel`](https://github.com/BigDataBiology/macrel), [`HMMER`](http://hmmer.org/), [`AMPlify`](https://github.com/bcgsc/AMPlify)\n6. Screening contigs for antibiotic resistant gene-like sequences with [`ABRicate`](https://github.com/tseemann/abricate), [`AMRFinderPlus`](https://github.com/ncbi/amr), [`fARGene`](https://github.com/fannyhb/fargene), [`RGI`](https://card.mcmaster.ca/analyze/rgi), [`DeepARG`](https://bench.cs.vt.edu/deeparg). [`argNorm`](https://github.com/BigDataBiology/argNorm) is used to map the outputs of `DeepARG`, `AMRFinderPlus`, and `ABRicate` to the [`Antibiotic Resistance Ontology`](https://www.ebi.ac.uk/ols4/ontologies/aro) for consistent ARG classification terms.\n7. Screening contigs for biosynthetic gene cluster-like sequences with [`antiSMASH`](https://antismash.secondarymetabolites.org), [`DeepBGC`](https://github.com/Merck/deepbgc), [`GECCO`](https://gecco.embl.de/), [`HMMER`](http://hmmer.org/)\n8. Creating aggregated reports for all samples across the workflows with [`AMPcombi`](https://github.com/Darcy220606/AMPcombi) for AMPs, [`hAMRonization`](https://github.com/pha4ge/hAMRonization) for ARGs, and [`comBGC`](https://raw.githubusercontent.com/nf-core/funcscan/master/bin/comBGC.py) for BGCs\n9. Software version and methods text reporting with [`MultiQC`](http://multiqc.info/)\n\n![funcscan metro workflow](docs/images/funcscan_metro_workflow.png)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fasta\nCONTROL_REP1,AEG588A1_001.fasta\nCONTROL_REP2,AEG588A1_002.fasta\nCONTROL_REP3,AEG588A1_003.fasta\n```\n\nEach row represents a (multi-)fasta file of assembled contig sequences.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/funcscan \\\n   -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR> \\\n   --run_amp_screening \\\n   --run_arg_screening \\\n   --run_bgc_screening\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/funcscan/usage) and the [parameter documentation](https://nf-co.re/funcscan/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/funcscan/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/funcscan/output).\n\n## Credits\n\nnf-core/funcscan was originally written by Jasmin Frangenberg, Anan Ibrahim, Louisa Perelo, Moritz E. Beber, James A. Fellows Yates.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\nAdam Talbot, Alexandru Mizeranschi, Hugo Tavares, J\u00falia Mir Pedrol, Martin Klapper, Mehrdad Jaberi, Robert Syme, Rosa Herbst, Vedanth Ramji, @Microbion.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#funcscan` channel](https://nfcore.slack.com/channels/funcscan) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/funcscan for your analysis, please cite it using the following doi: [10.5281/zenodo.7643099](https://doi.org/10.5281/zenodo.7643099)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "987",
        "keep": "To Curate",
        "latest_version": 12,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/987?version=12",
        "name": "nf-core/funcscan",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amr",
            "assembly",
            "metagenomics",
            "amp",
            "antibiotic-resistance",
            "antimicrobial-peptides",
            "antimicrobial-resistance-genes",
            "arg",
            "bgc",
            "biosynthetic-gene-clusters",
            "contigs",
            "function",
            "natural-products",
            "screening",
            "secondary-metabolites"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-10-06",
        "versions": 12
    },
    {
        "create_time": "2025-10-03",
        "creators": [
            "Jasper Koehorst",
            "Bart Nijsse"
        ],
        "description": "  **Workflow for Metagenomics binning from assembly.<br>**\r\n\r\n  Minimal inputs are: Identifier, assembly (fasta) and an associated sorted BAM file\r\n\r\n  Summary<br>\r\n    - MetaBAT2 (binning)<br>\r\n    - MaxBin2 (binning)<br>\r\n    - SemiBin2 (binning)<br>\r\n    - Binette (bin merging)<br>\r\n    - EukRep (eukaryotic classification)<br>\r\n    - CheckM2 (bin completeness and contamination)<br>\r\n    - BUSCO (bin completeness)<br>\r\n    - GTDB-Tk (bin taxonomic classification)<br>\r\n    - CoverM (bin abundances)<br>\r\n    \r\nIncluding:<br>\r\n   **Bin annotation (workflow: https://workflowhub.eu/workflows/1170):**<br>\r\n        - Bakta<br>\r\n        - Interproscan<br>\r\n        - Eggnog<br>\r\n        - KOfamscan<br>\r\n        - To RDF conversion with SAPP (optional, default on) --> https://workflowhub.eu/workflows/1174/<br>\r\n\r\n  Other UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br>\r\n  \r\n  **All tool CWL files and other workflows can be found here:**<br>\r\n    https://gitlab.com/m-unlock/cwl<br>\r\n\r\n  **How to setup and use an UNLOCK workflow:**<br>\r\n  https://docs.m-unlock.nl/docs/workflows/setup.html<br>\r\n",
        "doi": null,
        "edam_operation": [
            "Sequence assembly"
        ],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "64",
        "keep": "To Curate",
        "latest_version": 12,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/64?version=12",
        "name": "Metagenomic Binning from Assembly",
        "number_of_steps": 31,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "binning",
            "metagenome",
            "microbial"
        ],
        "tools": [
            "MetaBAT 2",
            "SemiBin",
            "MaxBin",
            "Binette",
            "BUSCO",
            "CheckM2"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-03",
        "versions": 6
    },
    {
        "create_time": "2025-10-02",
        "creators": [
            "Ryo Mameda",
            "Sora Yonezawa"
        ],
        "description": "\r\n![GitHub last commit (branch)](https://img.shields.io/github/last-commit/RyoMameda/workflow_cwl/main)\r\n![Status](https://img.shields.io/badge/status-development-yellow)\r\n[![cwltool](https://img.shields.io/badge/cwltool-3.1.20250110105449-success)](https://github.com/common-workflow-language/cwltool/releases/tag/3.1.20250110105449)\r\n[![License](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE)\r\n![Version](https://img.shields.io/badge/version-1.0-brightgreen)\r\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=python3.11&color=blue&logo=docker)](https://github.com/yonesora56/plant2human/tree/main/.devcontainer)\r\n\r\n# Gene Expression Analysis Workflow in Complex Microbiomes\r\n\r\n&nbsp;\r\n\r\n## Workflow Schema \r\n- more details: [Optimization of Mapping Tools and Investigation of Ribosomal RNA Influence for Data-Driven Gene Expression Analysis in Complex Microbiomes](https://doi.org/10.3390/microorganisms13050995)\r\n\r\n![image](./image/microorganisms-13-00995-g001.png)\r\n\r\n&nbsp;\r\n\r\n### 1. Overview of the Workflow\r\n\r\nThis analysis focuses on transcriptional profiling of complex microbiomes. It requires both metagenomic and metatranscriptomic NGS short-read data, along with annotation reference information (e.g., ribosomal RNA sequences and referenced protein databases, listed below). The metagenomic and metatranscriptomic reads should be derived from the same microbiome samples. Assembled metagenomic contigs are then used as reference sequences to map both types of reads, enabling gene-level quantification.\r\n\r\n&nbsp;\r\n\r\n### 2. Minimum Requirements\r\n\r\n- `Docker`\r\n- `cwltool`\r\n\r\n&nbsp;\r\n\r\n### 3. Workflow Component\r\n\r\nThis analysis workflow is composed of three sub-workflows; metagenomic contig assembling, reads mapping and annotation. \r\n\r\n&nbsp;\r\n\r\n#### Metagenomic contig assembling\r\n\r\nIn this process, the following steps are performed:\r\n\r\n\r\n1. Assembly process using `Megahit`. \r\n2. Prediction Protein sequences using `Prodigal`.\r\n3. Statical analysis of contigs useing `SeqKit`.\r\n\r\n&nbsp;\r\n\r\n#### Reads mapping\r\n\r\nIn this process, the following steps are performed:\r\n\r\n1. Mapping process using `BWA MEM`.\r\n2. Statical analysis of mapping results using `SAMtools`\r\n\r\n&nbsp;\r\n\r\n#### Annotation\r\n\r\nIn this process, the following steps are performed:\r\n\r\n1. Searching contaminated ribosomal RNA sequences using `BLAST`.\r\n2. Searching referenced proteins using `DIAMOND`.\r\n3. Creation GTF formated file contained annotation informations.\r\n\r\n&nbsp;\r\n\r\n### 4. Test Dataset and Your Own Dataset\r\n\r\n- If you are testing with the following files, please place them in the `Data` directory!\r\n- You can also obtain metagenomic and metatranscriptomic FASTQ files either by downloading them from public databases or by using your own samples, and then place them in your `Data` directory.\r\n\r\n#### Metagenome data\r\n\r\n- [SRR27548858](https://www.ncbi.nlm.nih.gov/sra/?term=SRR27548858)\r\n\r\n#### Metatranscriptome data\r\n\r\n- [SRR27548863](https://www.ncbi.nlm.nih.gov/sra/?term=SRR27548863)\r\n- [SRR27548864](https://www.ncbi.nlm.nih.gov/sra/?term=SRR27548864)\r\n- [SRR27548865](https://www.ncbi.nlm.nih.gov/sra/?term=SRR27548865)\r\n\r\n&nbsp;\r\n\r\n### 5. Annotation References\r\n\r\nThese reference files are used in the BLAST and DIAMOND processes. The downloaded files are available in the `Data` directory (accessed on September 17, 2025). If you wish to use the latest versions of the references, please download them using the following scripts.\r\n\r\n```bash\r\n# rRNA data from SILVA website (release138.1; accessed on 17,September,2025)\r\ncurl -O https://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_LSUParc_tax_silva.fasta.gz\r\ncurl -O https://ftp.arb-silva.de/release_138.1/Exports/SILVA_138.1_SSUParc_tax_silva.fasta.gz\r\n\r\n# Swiss-Prot data from UniProt for diamond makedb process (accessed on 17,September,2025)\r\ncurl -O https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\r\n\r\n# Pfam data from InterPro (accessed on 17,September,2025)) for hmmscan proess. Appling HMMER process in this workflow is on going, however this process takes time. This step will be optional.\r\n# curl -O https://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz\r\n```\r\n\r\n&nbsp;\r\n\r\n### 6. Command Execution\r\n\r\nWe recommend creating a `cache` directory to store cache and intermediate files. Since metagenomic and metatranscriptomic reads are mapped to contigs, the assembled results can be reused to reduce analytical costs. The `cwltool` properly recognizes caches when the `--cachedir` option is specified.\r\n\r\n```bash\r\n# main workflow\r\n\r\ncwltool --debug --cachedir <cache directory> --outdir <output directory> ./Worlkflow/main_w.cwl ./config/main_w.yml\r\n\r\n```\r\n\r\n&nbsp;\r\n\r\n### 7. based shell script & python script\r\n\r\nGitHub: https://github.com/RyoMameda/workflow\r\n\r\nThis workflow is developed at [DBCLS BioHackathon 2025](https://github.com/dbcls/bh25/), and the preprint of developing project is https://doi.org/10.37044/osf.io/qd5sz_v1.\r\n",
        "doi": "10.48546/workflowhub.workflow.1955.2",
        "edam_operation": [
            "Gene expression profiling",
            "Genome annotation",
            "Sequence annotation",
            "Sequence assembly",
            "Sequence trimming"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Microbiology",
            "Sequence analysis",
            "Sequence assembly"
        ],
        "filtered_on": "metage.* in tags",
        "id": "1955",
        "keep": "Keep",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1955?version=2",
        "name": "Gene Expression Analysis Workflow in Complex Microbiomes",
        "number_of_steps": 3,
        "projects": [
            "bonohulab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "metagenomics",
            "metatranscriptomics"
        ],
        "tools": [
            "BLASTN rRNA annotation process"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-02",
        "versions": 2
    },
    {
        "create_time": "2025-10-01",
        "creators": [
            "Galaxy",
            " VGP"
        ],
        "description": "Generate a genome assembly based on PacBio HiFi reads. Part of the VGP suite, it needs to be run after the VGP1 k-mer profiling workflow. The assembly contigs are built using HiFiasm, and the workflow generates assembly statistics, BUSCO reports, Merqury plots, and the contigs in fasta and GFA formats.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "612",
        "keep": "Reject",
        "latest_version": 25,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/612?version=25",
        "name": "Assembly-Hifi-only-VGP3/main",
        "number_of_steps": 45,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "",
            "Add_a_column1",
            "tp_grep_tool",
            "Cut1",
            "gfastats",
            "join1",
            "param_value_from_file",
            "tp_cut_tool",
            "pick_value",
            "busco",
            "tp_awk_tool",
            "merqury",
            "multiqc",
            "tp_find_and_replace",
            "__EXTRACT_DATASET__",
            "cutadapt",
            "compose_text_param",
            "tp_replace_in_line",
            "compleasm",
            "Convert characters1",
            "bandage_image",
            "hifiasm"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-01",
        "versions": 25
    },
    {
        "create_time": "2025-10-01",
        "creators": [
            "Galaxy",
            " VGP"
        ],
        "description": "Generate phased assembly based on PacBio HiFi reads and parental Illumina data for phasing. Part of the VGP workflow suite, it needs to be run after the Trio k-mer Profiling workflow VGP2. This workflow uses HiFiasm for contigging, and generates assembly statistics, BUSCO reports, Merqury plots, and the genome assembly contigs in fasta and GFA format. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "642",
        "keep": "Reject",
        "latest_version": 25,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/642?version=25",
        "name": "Assembly-Hifi-Trio-phasing-VGP5/main",
        "number_of_steps": 47,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "",
            "Add_a_column1",
            "tp_grep_tool",
            "Cut1",
            "gfastats",
            "join1",
            "param_value_from_file",
            "tp_cut_tool",
            "pick_value",
            "busco",
            "tp_awk_tool",
            "merqury",
            "multiqc",
            "tp_find_and_replace",
            "__EXTRACT_DATASET__",
            "cutadapt",
            "compose_text_param",
            "tp_replace_in_line",
            "compleasm",
            "Convert characters1",
            "bandage_image",
            "hifiasm"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-01",
        "versions": 25
    },
    {
        "create_time": "2025-09-18",
        "creators": [
            "Samuel Chaffron",
            "Audrey Bihouee",
            "Benjamin Churcheward",
            "Maxime Millet",
            "Guillaume Fertin",
            "Hugo Lefeuvre"
        ],
        "description": "MAGNETO is an automated snakemake workflow dedicated to MAG (Metagenome-Assembled Genomes) reconstruction from metagenomic data. \r\n\r\nIt includes a fully-automated coassembly step informed by optimal clustering of metagenomic distances, and implements complementary genome binning strategies, for improving MAG recovery.\r\n\r\n# Key Features\r\n\r\n - **Quality Control (QC)**: Automatically assesses the quality and the contamination of input reads, ensuring that low-quality data are filtered out to improve downstream analyses.\r\n\r\n - **Assembly**: MAGNETO uses high-performance assembler to construct contigs from metagenomic reads.\r\n\r\n - **Gene Collection**: Extracts and compiles gene sequences from contigs, providing a comprehensive gene catalog directly after assembly.\r\n\r\n - **Binning**: Groups contigs into probable genomes using composition signatures and abundance profiles.\r\n\r\n - **Genomes collection**: Provides taxonomic and functional annotation of reconstructed MAGs.\r\n\r\n - **Metatranscriptomic mapping**: Mapping of transcriptomic reads on genes collection and/or MAGs obtained with previous analysis.\r\n\r\n# Documentation\r\n\r\n**Full description in the [wiki pages](https://gitlab.univ-nantes.fr/bird_pipeline_registry/magneto/-/wikis/home)**\r\n\r\n# Citing the pipeline \r\nChurcheward B, Millet M, Bihou\u00e9e A, Fertin G, Chaffron S.<br>\r\nMAGNETO: An Automated Workflow for Genome-Resolved Metagenomics.<br>\r\nmSystems. 2022 Jun 15:e0043222. doi: [10.1128/msystems.00432-22](https://doi.org/10.1128/msystems.00432-22)\r\n",
        "doi": null,
        "edam_operation": [
            "Gene functional annotation",
            "Genome assembly",
            "Read binning",
            "Read mapping",
            "Sequence assembly",
            "Sequencing quality control",
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomics",
            "Metatranscriptomics"
        ],
        "filtered_on": "edam",
        "id": "1815",
        "keep": "Keep",
        "latest_version": 3,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1815?version=3",
        "name": "MAGNETO (automated workflow dedicated to MAG reconstruction)",
        "number_of_steps": 0,
        "projects": [
            "BiRD"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-09-25",
        "versions": 3
    },
    {
        "create_time": "2025-09-19",
        "creators": [
            "Caner Bagci"
        ],
        "description": "# Soil Metagenome Pipeline\r\n\r\nSoil Metagenome Pipeline is a modular, Nextflow DSL2 workflow for assembling, polishing, binning, annotating, and functionally characterizing complex soil metagenomes. It orchestrates state-of-the-art tools for long- and short-read metagenomics, generates high-quality MAGs, assigns taxonomy, and screens for biosynthetic gene clusters (BGCs).\r\n\r\n## What it does\r\n- Assembles long-read metagenomes (e.g., ONT) with Flye and optionally polishes with Medaka and/or NextPolish using short reads.\r\n- Maps short/long reads to assemblies to compute coverage/depth for downstream binning and QC.\r\n- Bins contigs with multiple strategies (SemiBin2, VAMB, MetaCoAG, ComeBIN) and can integrate results.\r\n- Evaluates MAG quality with CheckM2 and assigns taxonomy with GTDB-Tk.\r\n- Annotates bins and/or assemblies (Bakta, eggNOG) and detects BGCs (antiSMASH) with network-based clustering (BiG-SCAPE).\r\n- Produces organized outputs suitable for downstream comparative genomics.\r\n\r\n## Key features\r\n- Modular DSL2 design: swap/extend modules under `modules/` and `submodules/`.\r\n- Reproducible runtime via Conda/containers (profiles in `conf/`).\r\n- Sensible defaults with overridable parameters via `nextflow.config` or CLI.\r\n- Caching and resumability: supports `-resume` for efficient re-runs.\r\n\r\n## Modules at a glance (non-exhaustive)\r\n- Assembly and polishing: Flye, Medaka, NextPolish\r\n- Coverage mapping: minimap2/samtools, coverm, strobealign\r\n- Binning: SemiBin2, VAMB, MetaCoAG, ComeBIN, plus bin collection utilities\r\n- QC and taxonomy: CheckM2, GTDB-Tk\r\n- Annotation and function: Bakta (assemblies/bins), eggNOG\r\n- BGC discovery: antiSMASH (assemblies/bins), BiG-SCAPE networks\r\n- Taxonomic profiling: MMseqs2/MetaBuli helpers\r\n\r\n## Inputs\r\n- Reads: long reads (ONT/PacBio), optional short reads (Illumina).\r\n- Sample sheet: a tab-separated file like `data/samples.tsv` describing sample IDs and file paths.\r\n- Reference databases: external DBs required by some tools (e.g., GTDB-Tk, antiSMASH, BiG-SCAPE) are not bundled. Configure their locations via params or environment as appropriate.\r\n\r\n## Quick start\r\n- Dry run / graph preview:\r\n  nextflow run . -dsl2 -preview\r\n\r\n- Example execution (adjust paths and profile to your environment):\r\n  nextflow run . -profile conda -resume \\\r\n    --reads '/path/to/*_{R1,R2}.fastq.gz' \\\r\n    --longreads '/path/to/*.fastq.gz' \\\r\n    --samples 'data/samples.tsv' \\\r\n    --outdir 'results'\r\n\r\nSee `conf/` for example profiles (conda, docker, singularity, slurm). Tune resources via `nextflow.config` using `withName:` blocks for process-specific CPU, memory, and time.\r\n\r\n## Citation\r\nIf you use Soil Metagenome Pipeline in your research, please cite the corresponding preprint:\r\n\r\n- bioRxiv abstract: https://www.biorxiv.org/content/10.1101/2025.05.28.656579v1.abstract\r\n- DOI: https://doi.org/10.1101/2025.05.28.656579\r\n\r\nA machine-readable citation file (CITATION.cff) is included in the repository root. GitHub will display a \"Cite this repository\" button.\r\n\r\n## License\r\nThis project is licensed under the GNU General Public License v3.0 or later (GPL-3.0-or-later). See the LICENSE file for the full text.\r\n",
        "doi": "10.48546/workflowhub.workflow.1960.1",
        "edam_operation": [
            "Genome annotation",
            "Sequence assembly",
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Metagenomics",
            "Sequence assembly"
        ],
        "filtered_on": "edam",
        "id": "1960",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1960?version=1",
        "name": "Soil Metagenome Pipeline",
        "number_of_steps": 0,
        "projects": [
            "ZiemertLab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "assembly",
            "bioinformatics",
            "genomics",
            "metagenomics",
            "biosynthetic-gene-clusters"
        ],
        "tools": [
            "Flye",
            "Medaka",
            "NextPolish",
            "Minimap2",
            "SAMtools",
            "CoverM",
            "strobealign",
            "SemiBin",
            "VAMB",
            "MetaCoAG",
            "CheckM2",
            "CheckM",
            "GTDB",
            "Bakta",
            "eggNOG-mapper v2",
            "antiSMASH",
            "MMseqs2",
            "metabuli"
        ],
        "type": "Nextflow",
        "update_time": "2025-09-19",
        "versions": 1
    },
    {
        "create_time": "2021-09-30",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst"
        ],
        "description": "Workflow for quality assessment of paired reads and classification using NGTax 2.0 and functional annotation using picrust2.<br>\r\nIn addition files are exported to their respective subfolders for easier data management in a later stage.<br><br>\r\n\r\nSteps:\r\n  - Quality plots (FastQC)\r\n  - NG-TAX 2 High-throughput Amplicon Analysis\r\n  - PICRUSt 2 - Function prediction from marker gene sequences\r\n  - Export module for ngtax",
        "doi": "10.48546/workflowhub.workflow.154.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in tags",
        "id": "154",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "AFL-3.0",
        "link": "https:/workflowhub.eu/workflows/154?version=2",
        "name": "Quality assessment, amplicon classification and functional prediction",
        "number_of_steps": 10,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "cwl",
            "classification"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2025-09-11",
        "versions": 2
    },
    {
        "create_time": "2025-09-10",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst"
        ],
        "description": "Workflow for quality assessment and taxonomic classification of amplicon long read sequences.<br>\r\nIn addition files are exported to their respective subfolders for easier data management in a later stage.<br>\r\n<br>\r\nInputs are expected to be basecalled fastq files<br>\r\n<br>\r\n**Steps:**<br>\r\n    - NanoPlot read quality control, before and after filtering<br>\r\n    - fastplong read quality and length filtering<br>\r\n    - Emu abundance; species-level taxonomic abundance for full-length 16S read <br>\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Workflows"
        ],
        "filtered_on": "Amplicon in tags",
        "id": "1952",
        "keep": "Keep",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1952?version=1",
        "name": "Longread 16S classification workflow",
        "number_of_steps": 4,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "bioinformatics",
            "long-reads",
            "oxford-nanopore",
            "pacbio"
        ],
        "tools": [
            "NanoPlot",
            "fastplong"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-09-10",
        "versions": 1
    },
    {
        "create_time": "2025-09-09",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst",
            "Changlin Ke"
        ],
        "description": "**Workflow (hybrid) metagenomic assembly and binning**<br>\r\n  - Workflow Illumina Quality: \r\n    - Sequali (control)\r\n    - hostile contamination filter\r\n    - fastp (quality trimming)\r\n  - Workflow Longread Quality:\t\r\n    - NanoPlot (control)\r\n    - fastplong (quality trimming)\r\n    - hostile contamination filter\r\n  - Kraken2 taxonomic classification of FASTQ reads\r\n  - SPAdes/Flye (Assembly)\r\n  - Medaka/PyPolCA (Assembly polishing)\r\n  - QUAST (Assembly quality report)\r\n\r\n  (optional)\r\n  - Workflow binnning\r\n    - Metabat2/MaxBin2/SemiBin\r\n    - Binette\r\n    - BUSCO\r\n    - GTDB-Tk\r\n\r\n  (optional)\r\n  - Workflow Genome-scale metabolic models https://workflowhub.eu/workflows/372\r\n    - CarveMe (GEM generation)\r\n    - MEMOTE (GEM test suite)\r\n    - SMETANA (Species METabolic interaction ANAlysis)\r\n\r\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br><br>\r\n\r\n**All tool CWL files and other workflows can be found here:**<br>\r\n  https://gitlab.com/m-unlock/cwl/ <br>\r\n\r\n**How to setup and use an UNLOCK workflow:**<br>\r\nhttps://docs.m-unlock.nl/docs/workflows/setup.html<br>\r\n",
        "doi": null,
        "edam_operation": [
            "Sequence assembly"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Metagenomic sequencing",
            "Metagenomics",
            "Sequence assembly"
        ],
        "filtered_on": "edam",
        "id": "367",
        "keep": "Keep",
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/367?version=3",
        "name": "(Hybrid) Metagenomics workflow",
        "number_of_steps": 23,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "metagenomics",
            "binning",
            "illumina"
        ],
        "tools": [
            "SPAdes",
            "Flye",
            "Minimap2",
            "NanoPlot",
            "QUAST",
            "SemiBin",
            "MetaBAT 2",
            "MaxBin",
            "kraken2",
            "BUSCO",
            "CheckM",
            "InterProScan (EBI)",
            "eggNOG",
            "kofamscan",
            "KofamKOALA",
            "Sequali",
            "Hostile",
            "Binette"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-09-09",
        "versions": 3
    },
    {
        "create_time": "2025-09-06",
        "creators": [
            "Austyn Trull",
            "Lara Ianov"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-scnanoseq_logo_dark.png\">\n    <img alt=\"nf-core/scnanoseq\" src=\"docs/images/nf-core-scnanoseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/scnanoseq/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/scnanoseq/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/scnanoseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/scnanoseq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/scnanoseq/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.13899279-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.13899279)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/scnanoseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23scnanoseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/scnanoseq)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/scnanoseq** is a bioinformatics best-practice analysis pipeline for 10X Genomics single-cell/nuclei RNA-seq data derived from Oxford Nanopore Q20+ chemistry ([R10.4 flow cells (>Q20)](https://nanoporetech.com/about-us/news/oxford-nanopore-announces-technology-updates-nanopore-community-meeting)). Due to the expectation of >Q20 quality, the input data for the pipeline does not depend on Illumina paired data. **Please note `scnanoseq` can also process Oxford data with older chemistry, but we encourage usage of the Q20+ chemistry when possible**.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/scnanoseq/results).\n\n## Pipeline summary\n\n![scnanoseq diagram](assets/scnanoseq_tube_map.png)\n\n1. Raw read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/), [`NanoPlot`](https://github.com/wdecoster/NanoPlot), [`NanoComp`](https://github.com/wdecoster/nanocomp) and [`ToulligQC`](https://github.com/GenomiqueENS/toulligQC))\n2. Unzip and split FASTQ ([`pigz`](https://github.com/madler/pigz))\n   1. Optional: Split FASTQ for faster processing ([`split`](https://linux.die.net/man/1/split))\n3. Trim and filter reads ([`Nanofilt`](https://github.com/wdecoster/nanofilt))\n4. Post trim QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/), [`NanoPlot`](https://github.com/wdecoster/NanoPlot), [`NanoComp`](https://github.com/wdecoster/nanocomp) and [`ToulligQC`](https://github.com/GenomiqueENS/toulligQC))\n5. Barcode detection using a custom whitelist or 10X whitelist. ([`BLAZE`](https://github.com/shimlab/BLAZE))\n6. Extract barcodes. Consists of the following steps:\n   1. Parse FASTQ files into R1 reads containing barcode and UMI and R2 reads containing sequencing without barcode and UMI (custom script `./bin/pre_extract_barcodes.py`)\n   2. Re-zip FASTQs ([`pigz`](https://github.com/madler/pigz))\n7. Barcode correction (custom script `./bin/correct_barcodes.py`)\n8. Post-extraction QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/), [`NanoPlot`](https://github.com/wdecoster/NanoPlot), [`NanoComp`](https://github.com/wdecoster/nanocomp) and [`ToulligQC`](https://github.com/GenomiqueENS/toulligQC))\n9. Alignment to the genome, transcriptome, or both ([`minimap2`](https://github.com/lh3/minimap2))\n10. Post-alignment filtering of mapped reads and gathering mapping QC ([`SAMtools`](http://www.htslib.org/doc/samtools.html))\n11. Post-alignment QC in unfiltered BAM files ([`NanoComp`](https://github.com/wdecoster/nanocomp), [`RSeQC`](https://rseqc.sourceforge.net/))\n12. Barcode (BC) tagging with read quality, BC quality, UMI quality (custom script `./bin/tag_barcodes.py`)\n13. Read deduplication ([`UMI-tools`](https://github.com/CGATOxford/UMI-tools) OR [`Picard MarkDuplicates`](https://broadinstitute.github.io/picard/))\n14. Gene and transcript level matrices generation with [`IsoQuant`](https://github.com/ablab/IsoQuant) and/or transcript level matrices with [`oarfish`](https://github.com/COMBINE-lab/oarfish)\n15. Preliminary matrix QC ([`Seurat`](https://github.com/satijalab/seurat))\n16. Compile QC for raw reads, trimmed reads, pre and post-extracted reads, mapping metrics and preliminary single-cell/nuclei QC ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n```csv title=\"samplesheet.csv\"\nsample,fastq,cell_count\nCONTROL_REP1,AEG588A1_S1.fastq.gz,5000\nCONTROL_REP1,AEG588A1_S2.fastq.gz,5000\nCONTROL_REP2,AEG588A2_S1.fastq.gz,5000\nCONTROL_REP3,AEG588A3_S1.fastq.gz,5000\nCONTROL_REP4,AEG588A4_S1.fastq.gz,5000\nCONTROL_REP4,AEG588A4_S2.fastq.gz,5000\nCONTROL_REP4,AEG588A4_S3.fastq.gz,5000\n```\n\nEach row represents a single-end fastq file. Rows with the same sample identifier are considered technical replicates and will be automatically merged. `cell_count` refers to the expected number of cells you expect.\n\n```bash\nnextflow run nf-core/scnanoseq \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/scnanoseq/usage) and the [parameter documentation](https://nf-co.re/scnanoseq/parameters).\n\n## Pipeline output\n\nThis pipeline produces feature-barcode matrices as the main output. These feature-barcode matrices are able to be ingested directly by most packages used for downstream analyses such as `Seurat`. Additionally, the pipeline produces a number of quality control metrics to ensure that the samples processed meet expected metrics for single-cell/nuclei data.\n\nThe pipeline provides two tools to produce the aforementioned feature-barcode matrices, `IsoQuant` and `oarfish`, and the user is given the ability to choose whether to run both or just one. `IsoQuant` will require a genome fasta to be used as input to the pipeline, and will produce both gene and transcript level matrices. `oarfish` will require a transcriptome fasta to be used as input to the pipeline and will produce only transcript level matrices.\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/scnanoseq/results) tab on the nf-core website pipeline page.\nFor more details about the full set of output files and reports, please refer to the\n[output documentation](https://nf-co.re/scnanoseq/output).\n\n## Troubleshooting\n\nIf you experience any issues, please make sure to reach out on the [#scnanoseq slack channel](https://nfcore.slack.com/archives/C03TUE2K6NS) or [open an issue on our GitHub repository](https://github.com/nf-core/scnanoseq/issues/new/choose). However, some resolutions for common issues will be noted below:\n\n- Due to the nature of the data this pipeline analyzes, some tools may experience increased runtimes. For some of the custom tools made for this pipeline (`preextract_fastq.py` and `correct_barcodes.py`), we have leveraged the splitting done via the `split_amount` parameter to decrease their overall runtimes. The `split_amount` parameter will split the input FASTQs into a number of FASTQ files, each containing a number of lines based on the value used for this parameter. As a result, it is important not to set this parameter to be too low as doing so would cause the creation of a large number of files the pipeline will be processed. While this value can be highly dependent on the data, a good starting point for an analysis would be to set this value to `500000`. If you find that `PREEXTRACT_FASTQ` and `CORRECT_BARCODES` are still taking long amounts of time to run, it would be worth reducing this parameter to `200000` or `100000`, but keeping the value on the order of hundred of thousands or tens of thousands should help with keeping the total number of processes minimal. An example of setting this parameter to be equal to 500000 is shown below:\n\n```yml title=\"params.yml\"\nsplit_amount: 500000\n```\n\n- We have seen a recurrent node failure on slurm clusters that does seem to be related to submission of Nextflow jobs. This issue is not related to this pipeline per se, but rather to Nextflow itself. We are currently working on a resolution. But we have two methods that appear to help overcome should this issue arise:\n  1. Provide a custom config that increases the memory request for the job that failed. This may take a couple attempts to find the correct requests, but we have noted that there does appear to be a memory issue occasionally with these errors.\n  2. Request an interactive session with a decent amount of time and memory and CPUs in order to run the pipeline on the single node. Note that this will take time as there will be minimal parallelization, but this does seem to resolve the issue.\n- We note that umitools dedup can take a large amount of time in order to perform deduplication. One approach we have implemented to assist with speed is to split input files based on chromosome. However for the transcriptome aligned bams, there is some additional work required that involves grouping transcripts into appropriate chromosomes. In order to accomplish this, the pipeline needs to parse the transcript id from the transcriptome FASTA file. The transcript id is often nested in the sequence identifier with additional data and the data is delimited. We have included the delimiters used by reference files obtained from GENCODE, NCBI, and Ensembl. However in case you wish to explicitly control this or if the reference file source uses a different delimiter, you are able to manually set it via the `--fasta_delimiter` parameter.\n- We acknowledge that analyzing PromethION data is a common use case for this pipeline. Currently, the pipeline has been developed with defaults to analyze GridION and average sized PromethION data. For cases, where jobs have fail due for larger PromethION datasets, the defaults can be overwritten by a custom configuation file (provided by the `-c` Nextflow option) where resources can be increased (substantially in some cases). Below are some of the overrides we have used, and while these amounts may not work on every dataset, these will hopefully at least note which processes will need to have their resources increased:\n\n```groovy title=\"custom.config\"\n\nprocess\n{\n    withName: '.*:.*FASTQC.*'\n    {\n        cpus = 20\n    }\n}\n\nprocess\n{\n    withName: '.*:BLAZE'\n    {\n        cpus = 30\n    }\n}\n\nprocess\n{\n    withName: '.*:TAG_BARCODES'\n    {\n        memory = '60.GB'\n    }\n}\n\nprocess\n{\n    withName: '.*:SAMTOOLS_SORT'\n    {\n        cpus = 20\n    }\n}\n\nprocess\n{\n    withName: '.*:MINIMAP2_ALIGN'\n    {\n        cpus = 20\n    }\n}\n\nprocess\n{\n    withName: '.*:ISOQUANT'\n    {\n        cpus = 30\n        memory = '85.GB'\n    }\n}\n```\n\nWe further note that while we encourage the use of `split_amount` as discussed above for larger datasets, the pipeline can be executed without enabling this parameter. When doing this, please consider increasing the time limit to `CORRECT_BARCODES` as it can take hours instead of minutes when `split_amount` is disabled:\n\n```groovy title=\"custom.config\"\n//NOTE: with split_amount disabled, consider increasing the time limit to CORRECT_BARCODES\nprocess\n{\n    withName: '.*:CORRECT_BARCODES'\n    {\n        time = '15.h'\n    }\n}\n```\n\n## Credits\n\nnf-core/scnanoseq was originally written by [Austyn Trull](https://github.com/atrull314), and [Dr. Lara Ianov](https://github.com/lianov).\n\nWe would also like to thank the following people and groups for their support, including financial support:\n\n- Dr. Elizabeth Worthey\n- University of Alabama at Birmingham Biological Data Science Core (U-BDS), RRID:SCR_021766, <https://github.com/U-BDS>\n- Civitan International Research Center\n- Support from: 3P30CA013148-48S8\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#scnanoseq` channel](https://nfcore.slack.com/channels/scnanoseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/scnanoseq for your analysis, please cite the article as follows:\n\n> **scnanoseq: an nf-core pipeline for Oxford Nanopore single-cell RNA-sequencing**\n>\n> Austyn Trull, nf-core community, Elizabeth A. Worthey, Lara Ianov\n>\n> bioRxiv 2025.04.08.647887; doi: https://doi.org/10.1101/2025.04.08.647887\n\nThe specific pipleine version can be cited using the following doi: [10.5281/zenodo.13899279](https://doi.org/10.5281/zenodo.13899279)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1179",
        "keep": "To Curate",
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1179?version=4",
        "name": "nf-core/scnanoseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "10xgenomics",
            "long-read-sequencing",
            "nanopore",
            "scrna-seq",
            "single-cell"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-06",
        "versions": 4
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "James A. Fellows Yates",
            "Sofia Stamouli",
            "Moritz E. Beber",
            "Lauri Mesilaakso",
            "Thomas A. Christensen II",
            "Jianhong Ou",
            "Mahwash Jamy",
            "Maxime Borry",
            "Rafal Stepien",
            "Tanja Normark"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-taxprofiler_logo_custom_dark.png\">\n    <img alt=\"nf-core/taxprofiler\" src=\"docs/images/nf-core-taxprofiler_logo_custom_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/taxprofiler/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/taxprofiler/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/taxprofiler/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/taxprofiler/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/taxprofiler/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7728364-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7728364)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A525.04.2-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/taxprofiler)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23taxprofiler-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/taxprofiler)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n[![Cite Preprint](https://img.shields.io/badge/Cite%20Us!-Cite%20Preprint-orange)](https://doi.org/10.1101/2023.10.20.563221)\n\n## Introduction\n\n**nf-core/taxprofiler** is a bioinformatics best-practice analysis pipeline for taxonomic classification and profiling of shotgun short- and long-read metagenomic data. It allows for in-parallel taxonomic identification of reads or taxonomic abundance estimation with multiple classification and profiling tools against multiple databases, and produces standardised output tables for facilitating results comparison between different tools and databases.\n\n## Pipeline summary\n\n![](docs/images/taxprofiler_tube.png)\n\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) or [`falco`](https://github.com/smithlabcode/falco) as an alternative option)\n2. Performs optional read pre-processing\n   - Adapter clipping and merging (short-read: [fastp](https://github.com/OpenGene/fastp), [AdapterRemoval2](https://github.com/MikkelSchubert/adapterremoval); long-read: [porechop](https://github.com/rrwick/Porechop), [Porechop_ABI](https://github.com/bonsai-team/Porechop_ABI))\n   - Low complexity and quality filtering (short-read: [bbduk](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/), [PRINSEQ++](https://github.com/Adrian-Cantu/PRINSEQ-plus-plus); long-read: [Filtlong](https://github.com/rrwick/Filtlong)), [Nanoq](https://github.com/esteinig/nanoq)\n   - Host-read removal (short-read: [BowTie2](http://bowtie-bio.sourceforge.net/bowtie2/); long-read: [Minimap2](https://github.com/lh3/minimap2))\n   - Run merging\n3. Supports statistics metagenome coverage estimation ([Nonpareil](https://nonpareil.readthedocs.io/en/latest/)) and for host-read removal ([Samtools](http://www.htslib.org/))\n4. Performs taxonomic classification and/or profiling using one or more of:\n   - [Kraken2](https://ccb.jhu.edu/software/kraken2/)\n   - [MetaPhlAn](https://huttenhower.sph.harvard.edu/metaphlan/)\n   - [MALT](https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/algorithms-in-bioinformatics/software/malt/)\n   - [DIAMOND](https://github.com/bbuchfink/diamond)\n   - [Centrifuge](https://ccb.jhu.edu/software/centrifuge/)\n   - [Kaiju](https://kaiju.binf.ku.dk/)\n   - [mOTUs](https://motu-tool.org/)\n   - [KrakenUniq](https://github.com/fbreitwieser/krakenuniq)\n   - [KMCP](https://github.com/shenwei356/kmcp)\n   - [ganon](https://pirovc.github.io/ganon/)\n5. Perform optional post-processing with:\n   - [bracken](https://ccb.jhu.edu/software/bracken/)\n6. Standardises output tables ([`Taxpasta`](https://taxpasta.readthedocs.io))\n7. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n8. Plotting Kraken2, Centrifuge, Kaiju and MALT results ([`Krona`](https://hpc.nih.gov/apps/kronatools.html))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n```csv title=\"samplesheet.csv\"\nsample,run_accession,instrument_platform,fastq_1,fastq_2,fasta\n2612,run1,ILLUMINA,2612_run1_R1.fq.gz,,\n2612,run2,ILLUMINA,2612_run2_R1.fq.gz,,\n2612,run3,ILLUMINA,2612_run3_R1.fq.gz,2612_run3_R2.fq.gz,\n```\n\nEach row represents a fastq file (single-end), a pair of fastq files (paired end), or a fasta (with long reads).\n\nAdditionally, you will need a database sheet that looks as follows:\n\n```csv title=\"databases.csv\"\ntool,db_name,db_params,db_path\nkraken2,db2,--quick,/<path>/<to>/kraken2/testdb-kraken2.tar.gz\nmetaphlan,db1,,/<path>/<to>/metaphlan/metaphlan_database/\n```\n\nThat includes directories or `.tar.gz` archives containing databases for the tools you wish to run the pipeline against.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/taxprofiler \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --databases databases.csv \\\n   --outdir <OUTDIR>  \\\n   --run_kraken2 --run_metaphlan\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/taxprofiler/usage) and the [parameter documentation](https://nf-co.re/taxprofiler/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/taxprofiler/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/taxprofiler/output).\n\n## Credits\n\nnf-core/taxprofiler was originally written by James A. Fellows Yates, Sofia Stamouli, Moritz E. Beber, Lili Andersson-Li, and the nf-core/taxprofiler team.\n\n### Team\n\n- [James A. Fellows Yates](https://github.com/jfy133)\n- [Sofia Stamouli](https://github.com/sofstam)\n- [Moritz E. Beber](https://github.com/Midnighter)\n- [Lili Andersson-Li](https://github.com/LilyAnderssonLee)\n\nWe thank the following people for their contributions to the development of this pipeline:\n\n- [Lauri Mesilaakso](https://github.com/ljmesi)\n- [Tanja Normark](https://github.com/talnor)\n- [Maxime Borry](https://github.com/maxibor)\n- [Thomas A. Christensen II](https://github.com/MillironX)\n- [Jianhong Ou](https://github.com/jianhong)\n- [Rafal Stepien](https://github.com/rafalstepien)\n- [Mahwash Jamy](https://github.com/mjamy)\n- [Alex Caswell](https://github.com/AlexHoratio)\n- [Aidan Epstein](https://github.com/epstein6)\n\n### Acknowledgments\n\nWe also are grateful for the feedback and comments from:\n\n- The general [nf-core/community](https://nf-co.re/community)\n\nAnd specifically to\n\n- [Alex H\u00fcbner](https://github.com/alexhbnr)\n\n\u2764\ufe0f also goes to [Zandra Fagern\u00e4s](https://github.com/ZandraFagernas) for the logo.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#taxprofiler` channel](https://nfcore.slack.com/channels/taxprofiler) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/taxprofiler for your analysis, please cite it using the following doi: [10.1101/2023.10.20.563221](https://doi.org/10.1101/2023.10.20.563221).\n\n> Stamouli, S., Beber, M. E., Normark, T., Christensen II, T. A., Andersson-Li, L., Borry, M., Jamy, M., nf-core community, & Fellows Yates, J. A. (2023). nf-core/taxprofiler: Highly parallelised and flexible pipeline for metagenomic taxonomic classification and profiling. In bioRxiv (p. 2023.10.20.563221). https://doi.org/10.1101/2023.10.20.563221\n\nFor the latest version of the code, cite the Zenodo doi: [10.5281/zenodo.7728364](https://doi.org/10.5281/zenodo.7728364)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1025",
        "keep": "To Curate",
        "latest_version": 16,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1025?version=16",
        "name": "nf-core/taxprofiler",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "classification",
            "metagenomics",
            "profiling",
            "illumina",
            "long-reads",
            "microbiome",
            "nanopore",
            "pathogen",
            "shotgun",
            "taxonomic-classification",
            "taxonomic-profiling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 16
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "@praveenraj2018 None",
            "@praveenraj2018 None"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-rnavar_logo_dark.png\">\n    <img alt=\"nf-core/rnavar\" src=\"docs/images/nf-core-rnavar_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/rnavar/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/rnavar/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/rnavar/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/rnavar/actions/workflows/linting.yml)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/rnavar/results)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.6669636-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.6669636)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/rnavar)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23rnavar-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/rnavar)\n[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)\n[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)\n[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/rnavar** is a bioinformatics pipeline for RNA variant calling analysis following GATK4 best practices.\n\n## Pipeline summary\n\n1. Merge re-sequenced FastQ files ([`cat`](http://www.linfo.org/cat.html))\n2. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n3. (Optionally) Extract UMIs from FASTQ reads ([`UMI-tools`](https://github.com/CGATOxford/UMI-tools))\n4. (Optionally) HLATyping from FASTQ reads ([`Seq2HLA`](https://github.com/TRON-Bioinformatics/seq2HLA))\n5. Align reads to reference genome ([`STAR`](https://github.com/alexdobin/STAR))\n6. Sort and index alignments ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\n7. Duplicate read marking ([`Picard MarkDuplicates`](https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard))\n8. Scatter one interval-list into many interval-files ([`GATK4 IntervalListTools`](https://gatk.broadinstitute.org/hc/en-us/articles/4409917392155-IntervalListTools-Picard-))\n9. Splits reads that contain Ns in their cigar string ([`GATK4 SplitNCigarReads`](https://gatk.broadinstitute.org/hc/en-us/articles/4409917482651-SplitNCigarReads))\n10. Estimate and correct systematic bias using base quality score recalibration ([`GATK4 BaseRecalibrator`](https://gatk.broadinstitute.org/hc/en-us/articles/4409897206043-BaseRecalibrator), [`GATK4 ApplyBQSR`](https://gatk.broadinstitute.org/hc/en-us/articles/4409897168667-ApplyBQSR))\n11. Convert a BED file to a Picard Interval List ([`GATK4 BedToIntervalList`](https://gatk.broadinstitute.org/hc/en-us/articles/4409924780827-BedToIntervalList-Picard-))\n12. Call SNPs and indels ([`GATK4 HaplotypeCaller`](https://gatk.broadinstitute.org/hc/en-us/articles/4409897180827-HaplotypeCaller))\n13. Merge multiple VCF files into one VCF ([`GATK4 MergeVCFs`](https://gatk.broadinstitute.org/hc/en-us/articles/4409924817691-MergeVcfs-Picard-))\n14. Index the VCF ([`Tabix`](http://www.htslib.org/doc/tabix.html))\n15. Filter variant calls based on certain criteria ([`GATK4 VariantFiltration`](https://gatk.broadinstitute.org/hc/en-us/articles/4409897204763-VariantFiltration))\n16. Annotate variants ([`BCFtools Annotate`](https://samtools.github.io/bcftools/bcftools.html), [`snpEff`](https://pcingola.github.io/SnpEff/se_introduction/), [Ensembl VEP](https://www.ensembl.org/info/docs/tools/vep/index.html))\n17. Present QC for raw read, alignment, gene biotype, sample similarity, and strand-specificity checks ([`MultiQC`](http://multiqc.info/), [`R`](https://www.r-project.org/))\n\n### Summary of tools and version used in the pipeline\n\n| Tool        | Version |\n| ----------- | ------- |\n| BCFtools    | 1.21    |\n| BEDtools    | 2.31.1  |\n| Ensembl VEP | 114.2   |\n| FastQC      | 0.12.1  |\n| GATK        | 4.6.1.0 |\n| mosdepth    | 0.3.10  |\n| MultiQC     | 1.29    |\n| Picard      | 3.3.0   |\n| Samtools    | 1.21    |\n| Seq2HLA     | 2.3     |\n| SnpEff      | 5.1     |\n| STAR        | 2.7.11b |\n| Tabix       | 1.20    |\n| UMI-tools   | 1.1.5   |\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow.Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\nNow, you can run the pipeline using:\n\n```console\nnextflow run nf-core/rnavar -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input samplesheet.csv  --outdir <OUTDIR> --genome GRCh38\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/rnavar/usage) and the [parameter documentation](https://nf-co.re/rnavar/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/rnavar/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/rnavar/output).\n\n## Credits\n\nrnavar was originally written by Praveen Raj and Maxime U Garcia at [The Swedish Childhood Tumor Biobank (Barntum\u00f6rbanken), Karolinska Institutet](https://ki.se/forskning/barntumorbanken).\nNicolas Vannieuwkerke at [CMGG](https://www.cmgg.be/en/) later joined and helped with further development (v 1.1.0 and forward).\n\nMaintenance is now lead by Maxime U Garcia (now at [Seqera](https://seqera.io))\n\nMain developers:\n\n- [Maxime U Garcia](https://github.com/maxulysse)\n- [Nicolas Vannieuwkerke](https://github.com/nvnieuwk)\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Harshil Patel](https://github.com/drpatelh)\n- [Nicol\u00e1s Schcolnicov](https://github.com/nschcolnicov)\n- [\u00d6mer An](https://github.com/bounlu)\n- [Phil Ewels](https://github.com/ewels)\n- [Praveen Raj](https://github.com/praveenraj2018)\n- [Sarah Maman](https://github.com/SarahMaman)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#rnavar` channel](https://nfcore.slack.com/channels/rnavar) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/rnavar for your analysis, please cite it using the following doi: [10.5281/zenodo.6669636](https://doi.org/10.5281/zenodo.6669636)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1019",
        "keep": "To Curate",
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1019?version=4",
        "name": "nf-core/rnavar",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gatk4",
            "rnaseq",
            "rna",
            "variant-calling",
            "worflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 4
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "Harshil Patel",
            "Phil Ewels",
            "Rickard Hammar\u00e9n"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-rnaseq_logo_dark.png\">\n    <img alt=\"nf-core/rnaseq\" src=\"docs/images/nf-core-rnaseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/rnaseq/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/rnaseq/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/rnaseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/rnaseq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/rnaseq/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.1400710-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.1400710)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/rnaseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23rnaseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/rnaseq)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/rnaseq** is a bioinformatics pipeline that can be used to analyse RNA sequencing data obtained from organisms with a reference genome and annotation. It takes a samplesheet and FASTQ files as input, performs quality control (QC), trimming and (pseudo-)alignment, and produces a gene expression matrix and extensive QC report.\n\n![nf-core/rnaseq metro map](docs/images/nf-core-rnaseq_metro_map_grey_animated.svg)\n\n> In case the image above is not loading, please have a look at the [static version](docs/images/nf-core-rnaseq_metro_map_grey.png).\n\n1. Merge re-sequenced FastQ files ([`cat`](http://www.linfo.org/cat.html))\n2. Auto-infer strandedness by subsampling and pseudoalignment ([`fq`](https://github.com/stjude-rust-labs/fq), [`Salmon`](https://combine-lab.github.io/salmon/))\n3. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n4. UMI extraction ([`UMI-tools`](https://github.com/CGATOxford/UMI-tools))\n5. Adapter and quality trimming ([`Trim Galore!`](https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/))\n6. Removal of genome contaminants ([`BBSplit`](http://seqanswers.com/forums/showthread.php?t=41288))\n7. Removal of ribosomal RNA ([`SortMeRNA`](https://github.com/biocore/sortmerna))\n8. Choice of multiple alignment and quantification routes (_For `STAR` the sentieon implementation can be chosen_):\n   1. [`STAR`](https://github.com/alexdobin/STAR) -> [`Salmon`](https://combine-lab.github.io/salmon/)\n   2. [`STAR`](https://github.com/alexdobin/STAR) -> [`RSEM`](https://github.com/deweylab/RSEM)\n   3. [`HiSAT2`](https://ccb.jhu.edu/software/hisat2/index.shtml) -> **NO QUANTIFICATION**\n9. Sort and index alignments ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\n10. UMI-based deduplication ([`UMI-tools`](https://github.com/CGATOxford/UMI-tools))\n11. Duplicate read marking ([`picard MarkDuplicates`](https://broadinstitute.github.io/picard/))\n12. Transcript assembly and quantification ([`StringTie`](https://ccb.jhu.edu/software/stringtie/))\n13. Create bigWig coverage files ([`BEDTools`](https://github.com/arq5x/bedtools2/), [`bedGraphToBigWig`](http://hgdownload.soe.ucsc.edu/admin/exe/))\n14. Extensive quality control:\n    1. [`RSeQC`](http://rseqc.sourceforge.net/)\n    2. [`Qualimap`](http://qualimap.bioinfo.cipf.es/)\n    3. [`dupRadar`](https://bioconductor.org/packages/release/bioc/html/dupRadar.html)\n    4. [`Preseq`](http://smithlabresearch.org/software/preseq/)\n    5. [`DESeq2`](https://bioconductor.org/packages/release/bioc/html/DESeq2.html)\n    6. [`Kraken2`](https://ccb.jhu.edu/software/kraken2/) -> [`Bracken`](https://ccb.jhu.edu/software/bracken/) on unaligned sequences; _optional_\n15. Pseudoalignment and quantification ([`Salmon`](https://combine-lab.github.io/salmon/) or ['Kallisto'](https://pachterlab.github.io/kallisto/); _optional_)\n16. Present QC for raw read, alignment, gene biotype, sample similarity, and strand-specificity checks ([`MultiQC`](http://multiqc.info/), [`R`](https://www.r-project.org/))\n\n> **Note**\n> The SRA download functionality has been removed from the pipeline (`>=3.2`) and ported to an independent workflow called [nf-core/fetchngs](https://nf-co.re/fetchngs). You can provide `--nf_core_pipeline rnaseq` when running nf-core/fetchngs to download and auto-create a samplesheet containing publicly available samples that can be accepted directly as input by this pipeline.\n\n> **Warning**\n> Quantification isn't performed if using `--aligner hisat2` due to the lack of an appropriate option to calculate accurate expression estimates from HISAT2 derived genomic alignments. However, you can use this route if you have a preference for the alignment, QC and other types of downstream analysis compatible with the output of HISAT2.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n**samplesheet.csv**:\n\n```csv\nsample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,auto\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end). Rows with the same sample identifier are considered technical replicates and merged automatically. The strandedness refers to the library preparation and will be automatically inferred if set to `auto`.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/rnaseq \\\n    --input <SAMPLESHEET> \\\n    --outdir <OUTDIR> \\\n    --gtf <GTF> \\\n    --fasta <GENOME FASTA> \\\n    -profile <docker/singularity/.../institute>\n```\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/rnaseq/usage) and the [parameter documentation](https://nf-co.re/rnaseq/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/rnaseq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/rnaseq/output).\n\nThis pipeline quantifies RNA-sequenced reads relative to genes/transcripts in the genome and normalizes the resulting data. It does not compare the samples statistically in order to assign significance in the form of FDR or P-values. For downstream analyses, the output files from this pipeline can be analysed directly in statistical environments like [R](https://www.r-project.org/), [Julia](https://julialang.org/) or via the [nf-core/differentialabundance](https://github.com/nf-core/differentialabundance/) pipeline.\n\n## Online videos\n\nA short talk about the history, current status and functionality on offer in this pipeline was given by Harshil Patel ([@drpatelh](https://github.com/drpatelh)) on [8th February 2022](https://nf-co.re/events/2022/bytesize-32-nf-core-rnaseq) as part of the nf-core/bytesize series.\n\nYou can find numerous talks on the [nf-core events page](https://nf-co.re/events) from various topics including writing pipelines/modules in Nextflow DSL2, using nf-core tooling, running nf-core pipelines as well as more generic content like contributing to Github. Please check them out!\n\n## Credits\n\nThese scripts were originally written for use at the [National Genomics Infrastructure](https://ngisweden.scilifelab.se), part of [SciLifeLab](http://www.scilifelab.se/) in Stockholm, Sweden, by Phil Ewels ([@ewels](https://github.com/ewels)) and Rickard Hammar\u00e9n ([@Hammarn](https://github.com/Hammarn)).\n\nThe pipeline was re-written in Nextflow DSL2 and is primarily maintained by Harshil Patel ([@drpatelh](https://github.com/drpatelh)) from [Seqera Labs, Spain](https://seqera.io/).\n\nThe pipeline workflow diagram was initially designed by Sarah Guinchard ([@G-Sarah](https://github.com/G-Sarah)) and James Fellows Yates ([@jfy133](https://github.com/jfy133)), further modifications where made by Harshil Patel ([@drpatelh](https://github.com/drpatelh)) and Maxime Garcia ([@maxulysse](https://github.com/maxulysse)).\n\nMany thanks to other who have helped out along the way too, including (but not limited to):\n\n- [Alex Peltzer](https://github.com/apeltzer)\n- [Colin Davenport](https://github.com/colindaven)\n- [Denis Moreno](https://github.com/Galithil)\n- [Edmund Miller](https://github.com/edmundmiller)\n- [Gregor Sturm](https://github.com/grst)\n- [Jacki Buros Novik](https://github.com/jburos)\n- [Lorena Pantano](https://github.com/lpantano)\n- [Matthias Zepper](https://github.com/MatthiasZepper)\n- [Maxime Garcia](https://github.com/maxulysse)\n- [Olga Botvinnik](https://github.com/olgabot)\n- [@orzechoj](https://github.com/orzechoj)\n- [Paolo Di Tommaso](https://github.com/pditommaso)\n- [Rob Syme](https://github.com/robsyme)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#rnaseq` channel](https://nfcore.slack.com/channels/rnaseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/rnaseq for your analysis, please cite it using the following doi: [10.5281/zenodo.1400710](https://doi.org/10.5281/zenodo.1400710)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "44",
        "keep": "To Curate",
        "latest_version": 37,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/44?version=37",
        "name": "nf-core/rnaseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "rna",
            "rna-seq"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 37
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "charles-plessy None",
            "charles-plessy None"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-pairgenomealign_logo_dark.png\">\n    <img alt=\"nf-core/pairgenomealign\" src=\"docs/images/nf-core-pairgenomealign_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/pairgenomealign/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/pairgenomealign/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/pairgenomealign/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/pairgenomealign/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/pairgenomealign/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.13910535-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.13910535)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/pairgenomealign)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23pairgenomealign-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/pairgenomealign)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/pairgenomealign** is a bioinformatics pipeline that aligns one or more _query_ genomes to a _target_ genome, and plots pairwise representations.\n\n![Tubemap workflow summary](docs/images/pairgenomealign-tubemap.png \"Tubemap workflow summary\")\n\nThe main steps of the pipeline are:\n\n1. Genome QC ([`assembly-scan`](https://github.com/rpetit3/assembly-scan)).\n2. Genome indexing ([`lastdb`](https://gitlab.com/mcfrith/last/-/blob/main/doc/lastdb.rst)).\n3. Genome pairwise alignments ([`lastal`](https://gitlab.com/mcfrith/last/-/blob/main/doc/lastal.rst)).\n4. Alignment plotting ([`last-dotplot`](https://gitlab.com/mcfrith/last/-/blob/main/doc/last-dotplot.rst)).\n5. Alignment export to various formats with [`maf-convert`](https://gitlab.com/mcfrith/last/-/blob/main/doc/maf-convert.rst), plus [`Samtools`](https://www.htslib.org/) for SAM/BAM/CRAM.\n\nThe pipeline can generate four kinds of outputs, called _many-to-many_, _many-to-one_, _one-to-many_ and _one-to-one_, depending on whether sequences of one genome are allowed match the other genome multiple times or not.\n\nThese alignments are output in [MAF](https://genome.ucsc.edu/FAQ/FAQformat.html#format5) format, and optional line plot representations are output in PNG format.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fasta\nquery_1,path-to-query-genome-file-one.fasta\nquery_2,path-to-query-genome-file-two.fasta\n```\n\nEach row represents a fasta file, this can also contain multiple rows to accomodate multiple query genomes in fasta format.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/pairgenomealign \\\n   -profile <docker/singularity/.../institute> \\\n   --target sequencefile.fa \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/pairgenomealign/usage) and the [parameter documentation](https://nf-co.re/pairgenomealign/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/pairgenomealign/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/pairgenomealign/output).\n\n## Credits\n\n`nf-core/pairgenomealign` was originally written by [charles-plessy](https://github.com/charles-plessy); the original versions are available at <https://github.com/oist/plessy_pairwiseGenomeComparison>.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Mahdi Mohammed](https://github.com/U13bs1125) ported the original pipeline to _nf-core_ template 2.14.x.\n- [Martin Frith](https://github.com/mcfrith/), the author of LAST, gave us extensive feedback and advices.\n- [Michael Mansfield](https://github.com/mjmansfi) tested the pipeline and provided critical comments.\n- [Aleksandra Bliznina](https://github.com/aleksandrabliznina) contributed to the creation of the initial `last/*` modules.\n- [Jiashun Miao](https://github.com/miaojiashun) and [Huyen Pham](https://github.com/ngochuyenpham) tested the pipeline on vertebrate genomes.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#pairgenomealign` channel](https://nfcore.slack.com/channels/pairgenomealign) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use this pipeline, please cite:\n\n> **Extreme genome scrambling in marine planktonic Oikopleura dioica cryptic species.**\n> Charles Plessy, Michael J. Mansfield, Aleksandra Bliznina, Aki Masunaga, Charlotte West, Yongkai Tan, Andrew W. Liu, Jan Gra\u0161i\u010d, Mar\u00eda Sara del R\u00edo Pisula, Gaspar S\u00e1nchez-Serna, Marc Fabrega-Torrus, Alfonso Ferr\u00e1ndez-Rold\u00e1n, Vittoria Roncalli, Pavla Navratilova, Eric M. Thompson, Takeshi Onuma, Hiroki Nishida, Cristian Ca\u00f1estro, Nicholas M. Luscombe.\n> _Genome Res._ 2024. 34: 426-440; doi: [10.1101/2023.05.09.539028](https://doi.org/10.1101/gr.278295.123). PubMed ID: [38621828](https://pubmed.ncbi.nlm.nih.gov/38621828/)\n\n[OIST research news article](https://www.oist.jp/news-center/news/2024/4/25/oikopleura-who-species-identity-crisis-genome-community)\n\nAnd also please cite the [LAST papers](https://gitlab.com/mcfrith/last/-/blob/main/doc/last-papers.rst).\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1108",
        "keep": "To Curate",
        "latest_version": 7,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1108?version=7",
        "name": "nf-core/pairgenomealign",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "comparative-genomics",
            "dot-plot",
            "last",
            "pairwise-alignment",
            "synteny",
            "whole-genome-alignment"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 7
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "Stephen Watts"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-oncoanalyser_logo_dark.png\">\n    <img alt=\"nf-core/oncoanalyser\" src=\"docs/images/nf-core-oncoanalyser_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/oncoanalyser/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/oncoanalyser/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/oncoanalyser/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/oncoanalyser/actions/workflows/linting.yml)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/oncoanalyser/results)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.15189386-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.15189386)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/oncoanalyser)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23oncoanalyser-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/oncoanalyser)\n[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)\n[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)\n[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/oncoanalyser** is a Nextflow pipeline for the comprehensive analysis of cancer DNA and RNA sequencing data\nusing the [WiGiTS](https://github.com/hartwigmedical/hmftools) toolkit from the Hartwig Medical Foundation. The pipeline\nsupports a wide range of experimental setups:\n\n- FASTQ, BAM, and / or CRAM input files\n- WGS (whole genome sequencing), WTS (whole transcriptome sequencing), and targeted / panel sequencing<sup>1</sup>\n- Paired tumor / normal and tumor-only samples, and support for donor samples for further normal subtraction\n- Purity estimate for longitudinal samples using genomic features of the primary sample from the same patient<sup>2</sup>\n- UMI (unique molecular identifier) processing supported for DNA sequencing data\n- Most GRCh37 and GRCh38 reference genome builds\n\n<sub><sup>1</sup> built-in support for the [TSO500\npanel](https://www.illumina.com/products/by-type/clinical-research-products/trusight-oncology-500.html) with other\npanels and exomes requiring [creation of custom panel reference\ndata](https://nf-co.re/oncoanalyser/usage#custom-panels)</sub>\n<br />\n<sub><sup>2</sup> for example a primary WGS tissue biospy and longitudinal low-pass WGS ccfDNA sample taken from the\nsame patient</sub>\n\n## Pipeline overview\n\n<p align=\"center\"><img src=\"docs/images/oncoanalyser_pipeline.png\"></p>\n\nThe pipeline mainly uses tools from [WiGiTS](https://github.com/hartwigmedical/hmftools), as well as some other external\ntools. There are [several workflows available](https://nf-co.re/oncoanalyser/usage#introduction) in `oncoanalyser` and\nthe tool information below primarily relates to the `wgts` and `targeted` analysis modes.\n\n> [!NOTE]\n> Due to the limitations of panel data, certain tools (indicated with `*` below) do not run in `targeted` mode.\n\n- Read alignment: [BWA-MEM2](https://github.com/bwa-mem2/bwa-mem2) (DNA), [STAR](https://github.com/alexdobin/STAR) (RNA)\n- Read post-processing: [REDUX](https://github.com/hartwigmedical/hmftools/tree/master/redux) (DNA), [Picard MarkDuplicates](https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard) (RNA)\n- SNV, MNV, INDEL calling: [SAGE](https://github.com/hartwigmedical/hmftools/tree/master/sage), [PAVE](https://github.com/hartwigmedical/hmftools/tree/master/pave)\n- SV calling: [ESVEE](https://github.com/hartwigmedical/hmftools/tree/master/esvee)\n- CNV calling: [AMBER](https://github.com/hartwigmedical/hmftools/tree/master/amber), [COBALT](https://github.com/hartwigmedical/hmftools/tree/master/cobalt), [PURPLE](https://github.com/hartwigmedical/hmftools/tree/master/purple)\n- SV and driver event interpretation: [LINX](https://github.com/hartwigmedical/hmftools/tree/master/linx)\n- RNA transcript analysis: [ISOFOX](https://github.com/hartwigmedical/hmftools/tree/master/isofox)\n- Oncoviral detection: [VIRUSbreakend](https://github.com/PapenfussLab/gridss)\\*, [VirusInterpreter](https://github.com/hartwigmedical/hmftools/tree/master/virus-interpreter)\\*\n- Telomere characterisation: [TEAL](https://github.com/hartwigmedical/hmftools/tree/master/teal)\\*\n- Immune analysis: [LILAC](https://github.com/hartwigmedical/hmftools/tree/master/lilac), [CIDER](https://github.com/hartwigmedical/hmftools/tree/master/cider), [NEO](https://github.com/hartwigmedical/hmftools/tree/master/neo)\\*\n- Mutational signature fitting: [SIGS](https://github.com/hartwigmedical/hmftools/tree/master/sigs)\\*\n- HRD prediction: [CHORD](https://github.com/hartwigmedical/hmftools/tree/master/chord)\\*\n- Tissue of origin prediction: [CUPPA](https://github.com/hartwigmedical/hmftools/tree/master/cuppa)\\*\n- Pharmacogenomics: [PEACH](https://github.com/hartwigmedical/hmftools/tree/master/peach)\n- Summary report: [ORANGE](https://github.com/hartwigmedical/hmftools/tree/master/orange), [linxreport](https://github.com/umccr/linxreport)\n\nFor the `purity_estimate` mode, several of the above tools are run with adjusted configuration in addition to the following.\n\n- Tumor fraction estimation: [WISP](https://github.com/hartwigmedical/hmftools/tree/master/wisp)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nCreate a samplesheet with your inputs (WGS/WTS BAMs in this example):\n\n```csv\ngroup_id,subject_id,sample_id,sample_type,sequence_type,filetype,filepath\nPATIENT1_WGTS,PATIENT1,PATIENT1-N,normal,dna,bam,/path/to/PATIENT1-N.dna.bam\nPATIENT1_WGTS,PATIENT1,PATIENT1-T,tumor,dna,bam,/path/to/PATIENT1-T.dna.bam\nPATIENT1_WGTS,PATIENT1,PATIENT1-T-RNA,tumor,rna,bam,/path/to/PATIENT1-T.rna.bam\n```\n\nLaunch `oncoanalyser`:\n\n```bash\nnextflow run nf-core/oncoanalyser \\\n  -profile <docker/singularity/.../institute> \\\n  -revision 2.2.0 \\\n  --mode <wgts/targeted> \\\n  --genome <GRCh37_hmf/GRCh38_hmf> \\\n  --input samplesheet.csv \\\n  --outdir output/\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/oncoanalyser/usage) and the [parameter documentation](https://nf-co.re/oncoanalyser/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/oncoanalyser/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/oncoanalyser/output).\n\n## Version information\n\n### Extended support\n\nAs `oncoanalyser` is used in clinical settings and subject to accreditation standards in some instances, there is a need\nfor long-term stability and reliability for feature releases in order to meet operational requirements. This is\naccomplished through long-term support of several nominated feature releases, which all receive bug fixes and security\nfixes during the period of extended support.\n\nEach release that is given extended support is allocated a separate long-lived git branch with the 'stable' prefix, e.g.\n`stable/1.2.x`, `stable/1.5.x`. Feature development otherwise occurs on the `dev` branch with stable releases pushed to\n`master`.\n\nVersions nominated to have current long-term support:\n\n- TBD\n\n## Known issues\n\nPlease refer to [this page](https://github.com/nf-core/oncoanalyser/issues/177) for details regarding any known issues.\n\n## Credits\n\nThe `oncoanalyser` pipeline was written and is maintained by Stephen Watts ([@scwatts](https://github.com/scwatts)) from\nthe [Genomics Platform\nGroup](https://mdhs.unimelb.edu.au/centre-for-cancer-research/our-research/genomics-platform-group) at the [University\nof Melbourne Centre for Cancer Research](https://mdhs.unimelb.edu.au/centre-for-cancer-research).\n\nWe thank the following organisations and people for their extensive assistance in the development of this pipeline,\nlisted in alphabetical order:\n\n- [Hartwig Medical Foundation\n  Australia](https://www.hartwigmedicalfoundation.nl/en/partnerships/hartwig-medical-foundation-australia/)\n- Oliver Hofmann\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#oncoanalyser`\nchannel](https://nfcore.slack.com/channels/oncoanalyser) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nYou can cite the `oncoanalyser` Zenodo record for a specific version using the following DOI:\n[10.5281/zenodo.15189386](https://doi.org/10.5281/zenodo.15189386)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md)\nfile.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia,\n> Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1006",
        "keep": "To Curate",
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1006?version=4",
        "name": "nf-core/oncoanalyser",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dna",
            "ngs",
            "wgs",
            "cancer",
            "clinical",
            "exome",
            "panel",
            "rna",
            "targeted",
            "wigits",
            "wts"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 4
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "Danilo Di Leo",
            "Emelie Nilsson & Daniel Lundin"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-metatdenovo_logo_dark.png\">\n    <img alt=\"nf-core/metatdenovo\" src=\"docs/images/nf-core-metatdenovo_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/metatdenovo/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/metatdenovo/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/metatdenovo/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/metatdenovo/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/metatdenovo/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.10666590-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.10666590)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/metatdenovo)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23metatdenovo-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/metatdenovo)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/metatdenovo** is a bioinformatics best-practice analysis pipeline for assembly and annotation of metatranscriptomic and metagenomic data from prokaryotes, eukaryotes or viruses.\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/metatdenovo/results).\n\n## Usage\n\n![nf-core/metatdenovo metro map](docs/images/metat-metromap.png)\n\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n3. Quality trimming and adapter removal for raw reads ([`Trim Galore!`](https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/))\n4. Optional: Filter sequences with [`BBduk`](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbduk-guide/)\n5. Optional: Normalize the sequencing depth with [`BBnorm`](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/bb-tools-user-guide/bbnorm-guide/)\n6. Merge trimmed, pair-end reads ([`Seqtk`](https://github.com/lh3/seqtk))\n7. Choice of de novo assembly programs:\n   1. [`RNAspades`](https://cab.spbu.ru/software/rnaspades/) suggested for both prokaryote and eukaryote assembly\n   2. [`Megahit`](https://github.com/voutcn/megahit) suggested for both prokaryote and eukaryote assembly; requires less resources\n8. Choice of orf caller:\n   1. [`TransDecoder`](https://github.com/TransDecoder/TransDecoder) suggested for eukaryotes; only ORFs\n   2. [`Prokka`](https://github.com/tseemann/prokka) suggested for prokaryotes; ORFs and other features plus functional annotation\n   3. [`Prodigal`](https://github.com/hyattpd/Prodigal) suggested for Prokaryotes; only ORFs\n9. Quantification of genes identified in assemblies:\n   1. Generate index of assembly ([`BBmap index`](https://sourceforge.net/projects/bbmap/))\n   2. Mapping cleaned reads to the assembly for quantification ([`BBmap`](https://sourceforge.net/projects/bbmap/))\n   3. Get raw counts per each gene present in the assembly ([`Featurecounts`](http://subread.sourceforge.net)) -> TSV table with collected featurecounts output\n10. Functional annotation:\n    1. [`Prokka`](https://github.com/tseemann/prokka) feature identification and annotation for prokaryotes\n    2. [`eggNOG-mapper`](https://github.com/eggnogdb/eggnog-mapper)\n    3. [`KofamScan`](https://github.com/takaram/kofam_scan)\n    4. [`HMMER`](https://www.ebi.ac.uk/Tools/hmmer/search/hmmsearch) search ORFs with a set of HMM profiles, and rank results\n11. Taxonomic annotation:\n    1. [`EUKulele`](https://github.com/AlexanderLabWHOI/EUKulele)\n    2. [`Diamond`](https://github.com/bbuchfink/diamond)\n12. Summary statistics.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```\nsample,fastq_1,fastq_2\nsample1,./data/S1_R1_001.fastq.gz,./data/S1_R2_001.fastq.gz\nsample2,./data/S2_fw.fastq.gz,./data/S2_rv.fastq.gz\nsample3,./S4x.fastq.gz,./S4y.fastq.gz\nsample3,./a.fastq.gz,./b.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired-end).\nThe fastq files need to end with `.fq` or `.fastq`, followed by `.gz` if gzipped.\nRead files from multiple rows with the same sample name will be concatenated and treated as a single sample.\nA mix of single-end and paired-end files is allowed, but do not mix single-end and paired-end for the same sample name.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/metatdenovo \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/metatdenovo/usage) and the [parameter documentation](https://nf-co.re/metatdenovo/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/metatdenovo/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/metatdenovo/output).\n\n> [!NOTE]\n> Tables in the `summary_tables` directory under the output directory are made especially for further analysis in tools like R or Python.\n> Their formats are standardized and column names consistent between tables.\n\n## Credits\n\nnf-core/metatdenovo was originally written by Danilo Di Leo (@danilodileo), Emelie Nilsson (@emnilsson) & Daniel Lundin (@erikrikarddaniel).\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#metatdenovo` channel](https://nfcore.slack.com/channels/metatdenovo) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/metatdenovo for your analysis, please cite it using the following doi: [10.5281/zenodo.10666590](https://doi.org/10.5281/zenodo.10666590)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "997",
        "keep": "To Curate",
        "latest_version": 6,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/997?version=6",
        "name": "nf-core/metatdenovo",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "eukaryotes",
            "metatranscriptomics",
            "prokaryotes",
            "viruses"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 6
    },
    {
        "create_time": "2025-09-03",
        "creators": [
            "Avani Bhojwani and Timothy Little"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-denovotranscript_logo_dark.png\">\n    <img alt=\"nf-core/denovotranscript\" src=\"docs/images/nf-core-denovotranscript_logo_light.png\">\n  </picture>\n</h1>\n[![GitHub Actions CI Status](https://github.com/nf-core/denovotranscript/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/denovotranscript/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/denovotranscript/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/denovotranscript/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/denovotranscript/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.13324371-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.13324371)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/denovotranscript)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23denovotranscript-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/denovotranscript)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/denovotranscript** is a bioinformatics pipeline for de novo transcriptome assembly of paired-end short reads from bulk RNA-seq. It takes a samplesheet and FASTQ files as input, perfoms quality control (QC), trimming, assembly, redundancy reduction, pseudoalignment, and quantification. It outputs a transcriptome assembly FASTA file, a transcript abundance TSV file, and a MultiQC report with assembly quality and read QC metrics.\n\n![nf-core/transfuse metro map](docs/images/denovotranscript_metro_map.drawio.svg)\n\n1. Read QC of raw reads ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n2. Adapter and quality trimming ([`fastp`](https://github.com/OpenGene/fastp))\n3. Read QC of trimmed reads ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n4. Remove rRNA or mitochondrial DNA (optional) ([`SortMeRNA`](https://hpc.nih.gov/apps/sortmeRNA.html))\n5. Transcriptome assembly using any combination of the following:\n\n   - [`Trinity`](https://github.com/trinityrnaseq/trinityrnaseq/wiki) with normalised reads (default=True)\n   - [`Trinity`](https://github.com/trinityrnaseq/trinityrnaseq/wiki) with non-normalised reads\n   - [`rnaSPAdes`](https://ablab.github.io/spades/rna.html) medium filtered transcripts outputted (default=True)\n   - [`rnaSPAdes`](https://ablab.github.io/spades/rna.html) soft filtered transcripts outputted\n   - [`rnaSPAdes`](https://ablab.github.io/spades/rna.html) hard filtered transcripts outputted\n\n6. Redundancy reduction with [`Evidential Gene tr2aacds`](http://arthropods.eugenes.org/EvidentialGene/). A transcript to gene mapping is produced from Evidential Gene's outputs using [`gawk`](https://www.gnu.org/software/gawk/).\n7. Assembly completeness QC ([`BUSCO`](https://busco.ezlab.org/))\n8. Other assembly quality metrics ([`rnaQUAST`](https://github.com/ablab/rnaquast))\n9. Transcriptome quality assessment with [`TransRate`](https://hibberdlab.com/transrate/), including the use of reads for assembly evaluation. This step is not performed if profile is set to `conda` or `mamba`.\n10. Pseudo-alignment and quantification ([`Salmon`](https://combine-lab.github.io/salmon/))\n11. HTML report for raw reads, trimmed reads, BUSCO, and Salmon ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a pair of fastq files (paired end).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/denovotranscript \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/denovotranscript/usage) and the [parameter documentation](https://nf-co.re/denovotranscript/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/denovotranscript/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/denovotranscript/output).\n\n## Credits\n\nnf-core/denovotranscript was written by Avani Bhojwani ([@avani-bhojwani](https://github.com/avani-bhojwani/)) and Timothy Little ([@timslittle](https://github.com/timslittle/)).\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#denovotranscript` channel](https://nfcore.slack.com/channels/denovotranscript) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/denovotranscript for your analysis, please cite it using the following doi: [10.5281/zenodo.13324371](https://doi.org/10.5281/zenodo.13324371)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1100",
        "keep": "To Curate",
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1100?version=4",
        "name": "nf-core/denovotranscript",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "denovo-assembly",
            "rna-seq",
            "transcriptome"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-09-03",
        "versions": 4
    },
    {
        "create_time": "2025-09-02",
        "creators": [
            "Ivna Ivankovic",
            "Todor Gitchev",
            "Zsolt Bal\u00e1zs"
        ],
        "description": "# cfDNA-Flow\r\n\r\n## 1. Overview\r\ncfDNA-Flow facilitates the accurate and reproducible analysis of cfDNA WGS data. It offers various preprocessing options to accommodate different experimental setups and research needs in the field of liquid biopsies.\r\n\r\n## 2. Preprocessing options\r\n### 2.1 Trimming Options\r\ncfDNA-Flow provides the flexibility to either trim or not trim the input reads based on the user's requirements. Trimming removes low-quality bases, which can impact downstream analyses.\r\n\r\n### 2.2 Reference Genome Selection\r\nUsers can choose from the following genome builds: hg19, hs37d5 (hg19decoy), hg38 and hg38 without alternative contigs (hg38noalt). For download links, please refer to the Data Availability section in the accompanying paper.\r\n\r\n### 2.3 Post-Alignment Filtering and GC bias correction\r\nThe pipeline uses the BWA software for alignment, followed by extensive post-alignment filtering steps to ensure reliable alignments. Users can define specific filtering criteria to remove low-quality or ambiguous reads, such as secondary alignments, reads with insertion or deletion, and reads with low mapping qualities.\r\n\r\n## 3. Feature Extraction\r\n\r\n### 3.1 Fragment length features\r\ncfDNA-Flow offers fragment length analysis; calculating the mean, median, and standard deviation values for fragments sized 100 to 220 base pairs (bp), corresponding to the mononucleosomal size range. Additionally, cfDNA-Flow calculates the frequencies of cfDNA fragment sizes ranging from 70 bp to 1000 bp in 10 bp bins.\r\n\r\n### 3.2 Copy number changes\r\ncfDNA-Flow utilizes two copy number analysis tools: [ichorCNA](https://github.com/broadinstitute/ichorCNA) (v0.2.0) and [tMAD](https://github.com/sdchandra/tMAD), to estimate copy number changes and tumor fraction. \r\n\r\n### 3.3 Fragment end motifs\r\ncfDNA-Flow runs [FrEIA](https://github.com/mouliere-lab/FrEIA) to identify differences in fragment end motifs and their diversity between groups.\r\n\r\n### 3.4 Differential coverage analysis over DNase hypersensitivity sites\r\nThese features are calculated using [LIQUORICE](https://github.com/epigen/LIQUORICE/tree/master). You can find more information and access LIQUORICE through the following [link](https://liquorice.readthedocs.io/en/latest/).\r\n\r\n## 4. Usage\r\nTo use the cfDNA-Flow, follow these steps:\r\n\r\n### 4.1 Installation:\r\nClone the cfDNA-Flow repository from GitHub to your local machine.\r\n\r\n        git clone https://github.com/uzh-dqbm-cmi/cfDNA-Flow.git\r\n        cd cfDNA-Flow\r\n\r\nIt is recommended to create a virtual environment to manage the project's dependencies. This ensures that the dependencies do not interfere with other Python projects on your machine. See how to create a Python environment [here](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/).\r\n\r\nOnce the virtual environment is activated, install the required Python dependencies using the `requirements.txt` file.\r\n\r\n        pip install -r requirements.txt\r\n\r\nAdditionally, some R packages are required for the project. Make sure you have R installed (version 4.3). You can install these packages by running the script below:\r\n\r\n        R -f install_packages.R\r\n\r\nAfter following these steps, your environment should be set up with all the necessary dependencies for both Python and R. You are now ready to proceed with using the cfDNA-Flow pipeline. See section 4. Usage. \r\n\r\nOnce you are finished using cfDNA-Flow, deactivate the virtual environment by running:\r\n\r\n        deactivate\r\n\r\n### 4.2 Configuration:\r\nThe configuration file, `test_cfDNA_pipeline.yaml`, is used to specify the input files, reference genome, and desired preprocessing options. User can customize the trimming, alignment, and filtering settings as needed.\r\nSettings for this demo are as follows: reads are trimmed, reference genome is hg38, mapping quality is 30, [SAM flag](https://broadinstitute.github.io/picard/explain-flags.html) is 40, CIGAR string is D. \r\n\r\n### 4.3 Execution/Demo:\r\nTo start preprocessing, execute the following command. Use the -np flag for a dry run to verify everything works correctly.\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_preprocess -np \r\n\r\nIf successful, rerun the command without the -np flag.\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_preprocess \r\n\r\nNext, bed to process BED files, use:\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_bedprocess\r\n\r\nDo global length: \r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_global_length\r\n\r\nDo tMAD:\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_cal_blacklist\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_cal_RefSample\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_cal_t_MAD_forall\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_visualising_t_MAD_forall\r\n\r\nDo ichorCNA:\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_createPoN\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_ichorCNA\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_ichorCNA_results\r\n\r\nDo LIQUORICE:\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_LIQUORICE\r\n\r\nDo FreIA:\r\n\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_FrEIA_preprocessing\r\n        snakemake -s Snakefile --configfile test/test_cfDNA_pipeline.yaml -j 2 do_FrEIA\r\n\r\n\r\n### 4.4 Output:\r\nThe pipeline outputs alignment files (BAM files, BED files), and quality control reports. Additionally, it outputs features described above. \r\n\r\n#### BAM files\r\nProcessed BAM files of studied samples, accompanied by their .bai index files, are stored in the `results/BAM/0memhg19False40` folder and have `.sortByCoord.bam` suffix. \r\n\r\n#### BED files\r\nBED files are stored in the `results/BED/0memhg19False40` folder. Those files store information about chromosome number, start and end positions of cfDNA fragments.\r\n\r\n#### Quality control reports\r\nOutput of QC is stored in the `results/QC/0memhg19False40/multiqc_data` folder. Specifically, `multiqc_report.html` file contains multiple the QC metrics: general statistics, Picard metrics (alignment summary, mean read length, mark duplicates, WGS coverage, WGS filtered bases), FastQC (sequence counts, sequence quality histograms and quality scores, per base sequence content, per sequence GC content, per base N content, sequence length distribution, sequence duplication levels, overrepresented sequences, adapter content, status checks).  \r\n\r\n#### Fragment length features\r\nThe output of fragment length features is stored in the `results/feature/0memhg19False40/global_length.tsv` file. Columns store fragment length features for each studied sample (rows).\r\n\r\n#### Coverage features and fragment lengths in 1 Mbp genomic bins\r\nOutputs of features in 1 Mbp genomic bins can be found in the `results/BED/0memhg19False40` folder. Values for all the samples are stored in `mergeddf.csv` file. Values for each individual sample are stored in the files with suffix `binned.csv`.\r\n\r\nAdditional length features for every sample are stored in the folder `results/BED/0memhg19False40` and have the following suffixes:\r\n\r\n`binned_lengths.csv` - each row contains information about the chromosome number, genomic bin number (1 Mbp wide), and the lengths of all cfDNA fragments corresponding to that bin\r\n\r\n`len.csv` - contains a single column listing the lengths of all cfDNA fragments derived from a sample\r\n\r\n`lenuniqcount.csv` - a two-column format representing the histogram of cfDNA fragment lengths along with their frequencies\r\n\r\n#### ichorCNA\r\nResults of ichorCNA analysis can be found in the `results/feature/0memhg19False40/ichorCNA` folder. For detailed ichorCNA output description see this [link](https://github.com/broadinstitute/ichorCNA/wiki/Output). Shortly, ichorCNA outputs tumor fraction estimates based on CNA analysis. Additionally, it outputs CNA plots representing log2 ratio copy number for each bin in the genome.\r\n\r\n#### tMAD\r\nThe outputs of tMAD are stored in the `results/BED/0memhg19False40/tMAD/tMAD_results.tsv` file. This file contains sample names and their corresponding tMAD values. \r\n\r\n#### LIQUORICE\r\nThe outputs of LIQUORICE are stored in the `results/BED/0memhg19False40/LIQUORICE/summary_across_samples_and_ROIS.csv` file. This file contains sample names and their corresponding Dip depth and Dip area values after z-scaling. We recommend using the dip depth values, as we found them most informative. \r\n\r\n#### FrEIA\r\nThe outputs of FrEIA are stored in the `results/BED/0memhg19False40/FrEIA/0memhg19False40_FrEIA_score.csv` file. This file contains sample names and their corresponding tMAD values. \r\n\r\n## 5. Support\r\nWith issues or questions, please contact the maintainers. \r\n",
        "doi": "10.48546/workflowhub.workflow.1900.1",
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics"
        ],
        "filtered_on": "binn.* in description",
        "id": "1900",
        "keep": "Reject",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1900?version=1",
        "name": "cfDNA-Flow",
        "number_of_steps": 0,
        "projects": [
            "KrauthammerLab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "cfdna"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-09-19",
        "versions": 1
    },
    {
        "create_time": "2025-08-10",
        "creators": [
            "Nikolay Oskolkov"
        ],
        "description": "# GENome EXogenous (GENEX) sequence detection\r\n\r\nThis is a computational workflow for detecting coordinates of microbial-like or human-like sequences in eukaryotic and procaryotic reference genomes. The workflow accepts a reference genome in FASTA-format and outputs coordinates of microbial-like (human-like) regions in BED-format. The workflow builds a Bowtie2 index of the reference genome and aligns pre-computed microbial (GTDB v.214 or NCBI RefSeq release 213) or human (hg38) pseudo-reads to the reference, then custom scripts are used for detection of the positions of covered regions and quantification of most abundant microbial species, the latter is only when screening for microbial-like sequences in eukaryotic referenes.\r\n\r\nThe workflow was developed by Nikolay Oskolkov, Lund University, Sweden, within the NBIS SciLifeLab long-term support project, PI Tom van der Valk, Centre for Palaeogenetics, Stockholm, Sweden.\r\n\r\nIf you use the workflow for your research, please cite our manuscript:\r\n\r\n    Nikolay Oskolkov, Chenyu Jin, Samantha L\u00f3pez Clinton, Flore Wijnands, Ernst Johnson, \r\n    Benjamin Guinet, Verena Kutschera, Cormac Kinsella, Peter D. Heintzman and Tom van der Valk, \r\n    Disinfecting eukaryotic reference genomes to improve taxonomic inference from environmental \r\n    ancient metagenomic data, https://www.biorxiv.org/content/10.1101/2025.03.19.644176v1, \r\n    https://doi.org/10.1101/2025.03.19.644176\r\n\r\nPlease note that in this gitub reporsitory, we provide a small subset of microbial pseudo-reads for demonstration purposes, the full dataset is available at the SciLifeLab Figshare https://doi.org/10.17044/scilifelab.28491956.\r\n\r\nQuestions regarding the dataset should be sent to nikolay.oskolkov@scilifelab.se\r\n\r\n## Quick start\r\n\r\nPlease clone this repository and install the workflow tools as follows:\r\n\r\n    git clone https://github.com/NikolayOskolkov/MCWorkflow\r\n    cd MCWorkflow\r\n    conda env create -f environment.yaml\r\n    conda activate MCWorkflow\r\n\r\nThen you can run the workflow as:\r\n\r\n    ./micr_cont_detect.sh GCF_002220235.fna.gz data GTDB 4 \\\r\n    GTDB_sliced_seqs_sliding_window.fna.gz 10\r\n\r\nHere, `GCF_002220235.fna.gz` is the eukaryotic reference to be screened for microbial-like sequeneces, `data` is the directory containing the eukaryotic reference, `GTDB` is the type of pseudo-reads to be used for detecting exogenous regions in the eukaryotic reference (can be `GTGB`, `RefSeq` or `human`), `4` is the number of available threads in your computational environment, `GTDB_sliced_seqs_sliding_window.fna.gz` is the pre-computed pseudo-reads (small subset is provided in this github repository, the full datasets can be downloaded from the SciLifeLab Figshare https://doi.org/10.17044/scilifelab.28491956), and `10` is the number of allowed Bowtie2 multi-mappers.\r\n\r\n\r\nPlease also read the very detailed `vignette.html` and follow the preparation steps described there. The vignette `vignette.html` walks you through the explanations of the workflow parameters and interpretation of the output files.\r\n\r\n\r\n\r\n## Nextflow implementation\r\n\r\nAlternatively, you can specify the workflow input files and parameters in the `nextflow.config` and run it using Nextflow:\r\n\r\n    nextflow run main.nf\r\n\r\nThe Nextflow implementation is preferred for scalability and reproducibility purposes. Please place your reference genomes (fasta-files) to be screened for exogenous regions in the `data` folder. An example of the config-file, `nextflow.config`, can look like this:\r\n\r\n    params {\r\n        input_dir = \"data\"                                             // folder with multiple reference genomes (fasta-files)\r\n        type_of_pseudo_reads = \"GTDB\"                                  // type of pseudo-reads to be used for screening the input reference genome, can be \"GTDB\", \"RefSeq\" or \"human\"\r\n        threads = 4                                                    // number of available threads\r\n        input_pseudo_reads = \"GTDB_sliced_seqs_sliding_window.fna.gz\"  // name of pre-computed file with pseudo-reads, can be \"GTDB_sliced_seqs_sliding_window.fna.gz\", \"RefSeq_sliced_seqs_sliding_window.fna.gz\" or \"human_sliced_seqs_sliding_window.fna.gz\"\r\n        n_allowed_multimappers = 10                                    // number of multi-mapping pseudo-reads allowed by Bowtie2, do not change this default number unless you know what you are doing\r\n    }\r\n\r\nPlease modify it to adjust for the number of available threads in your computational environment and the type of analysis, i.e. detecting microbial-like or human-like sequeneces in the reference genome, you would like to perform.\r\n",
        "doi": "10.48546/workflowhub.workflow.1846.1",
        "edam_operation": [
            "Alignment"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "1846",
        "keep": "Reject",
        "latest_version": 1,
        "license": "CC0-1.0",
        "link": "https:/workflowhub.eu/workflows/1846?version=1",
        "name": "GENEX",
        "number_of_steps": 0,
        "projects": [
            "GENEX"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-19",
        "versions": 1
    },
    {
        "create_time": "2025-07-31",
        "creators": [
            "Peter van Heusden"
        ],
        "description": "Find and annotate variants in ampliconic SARS-CoV-2 Illumina sequencing data and classify samples with pangolin and nextclade",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "155",
        "keep": "Reject",
        "latest_version": 6,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/155?version=6",
        "name": "sars-cov-2-pe-illumina-artic-ivar-analysis/SARS-COV-2-ILLUMINA-AMPLICON-IVAR-PANGOLIN-NEXTCLADE",
        "number_of_steps": 16,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virology"
        ],
        "tools": [
            "fastp",
            "samtools_stats",
            "samtools_view",
            "snpeff_sars_cov_2",
            "__FLATTEN__",
            "qualimap_bamqc",
            "tp_sed_tool",
            "ivar_trim",
            "pangolin",
            "ivar_consensus",
            "nextclade",
            "ivar_variants",
            "tp_cat",
            "bwa_mem",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-19",
        "versions": 6
    },
    {
        "create_time": "2025-10-11",
        "creators": [
            "Viktoria Isabel Schwarz",
            "Wolfgang Maier"
        ],
        "description": "A workflow for the analysis of pox virus genomes sequenced as half-genomes (for ITR resolution) in a tiled-amplicon approach",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "439",
        "keep": "Reject",
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/439?version=4",
        "name": "pox-virus-amplicon/main",
        "number_of_steps": 40,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virology",
            "pox"
        ],
        "tools": [
            "qualimap_bamqc",
            "bwa_mem",
            "split_file_to_collection",
            "fasta_compute_length",
            "fastp",
            "samtools_stats",
            "samtools_merge",
            "datamash_ops",
            "ivar_trim",
            "\n __FILTER_FAILED_DATASETS__",
            "multiqc",
            "\n Cut1",
            "tp_sed_tool",
            "\n Grep1",
            "collection_element_identifiers",
            "ivar_consensus",
            "tp_cat",
            "\n __FLATTEN__",
            "\n __ZIP_COLLECTION__",
            "compose_text_param",
            "samtools_view",
            "\n __SORTLIST__",
            "EMBOSS: maskseq51",
            "\n __APPLY_RULES__",
            "\n param_value_from_file"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-11",
        "versions": 4
    },
    {
        "create_time": "2025-08-13",
        "creators": [],
        "description": "# Imputation Workflow h3abionet/chipimputation\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.04.0-brightgreen.svg)](https://www.nextflow.io/)\r\n[![Docker](https://img.shields.io/badge/docker%20registry-Quay.io-red)](https://quay.io/h3abionet_org/imputation_tools)\r\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B%20%20%E2%97%8B%20%20%E2%97%8B-orange)](https://fair-software.eu)\r\n\r\n## Introduction\r\nImputation is likely to be run in the context of a GWAS, studying population structure, and admixture studies. It is computationally expensive in comparison to other GWAS steps.\r\nThe basic steps of the pipeline is described in the diagram below:\r\n\r\n![chipimputation pipeline workflow diagram](https://www.h3abionet.org/images/workflows/snp_imputation_workflow.png)\r\n\r\n* The workflow is developed using [![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.04.0-brightgreen.svg)](https://www.nextflow.io/) and imputation performed using [Minimac4](https://genome.sph.umich.edu/wiki/Minimac4). \r\n* It identifies regions to be imputed on the basis of an input file in VCF format, split the regions into small chunks, phase each chunk using the phasing tool [Eagle2](https://data.broadinstitute.org/alkesgroup/Eagle/) and produces output in VCF format that can subsequently be used in a [GWAS](https://github.com/h3abionet/h3agwas) workflow.\r\n* It also produce basic plots and reports of the imputation process including the imputation performance report, the imputation accuracy, the allele frequency of the imputed vs of the reference panel and other metrics.    \r\n\r\n**This pipeline comes with docker/singularity containers making installation trivial and results highly reproducible.**\r\n\r\n\r\n\r\n## Getting started\r\n\r\n### Running the pipeline with test dataset\r\nThis pipeline itself needs no installation - NextFlow will automatically fetch it from GitHub.\r\nYou can run the pipeline using test data hosted in github with singularity without have to install or change any parameters.\r\n\r\n```\r\nnextflow run h3abionet/chipimputation/main.nf -profile test,singularity\r\n```\r\n\r\n- `test` profile will download the testdata from [here](https://github.com/h3abionet/chipimputation_test_data/tree/master/testdata_imputation)\r\n- `singularity` profile will download the singularity image from [quay registry](https://quay.io/h3abionet_org/imputation_tools)\r\n\r\nCheck for results in `./output`\r\n\r\n\r\n### Start running your own analysis\r\n\r\nCopy the `test.config` file from the `conf` folder by doing `cp <conf dir>/test.config .` and edit it to suit the path to where your files are stored.\r\n\r\nOnce you have edited the config file, run the command below.\r\n\r\n```bash\r\nnextflow run h3abionet/chipimputation -c \"name of your config file\" -profile singularity\r\n```\r\n\r\n- `singularity` profile will download the singularity image from [quay registry](https://quay.io/h3abionet_org/imputation_tools)\r\n\r\nCheck for results in `./output`\r\n\r\n\r\n## Documentation\r\nThe h3achipimputation pipeline comes with detailed documentation about the pipeline.\r\nThis is found in the `docs/` directory:\r\n\r\n1. [Installation](docs/installation.md)\r\n2. [Pipeline configuration](docs/configuration/config_files.md)  \r\n    2.1. [Configuration files](docs/configs.md)  \r\n    2.2. [Software requirements](docs/soft_requirements.md)  \r\n    2.3. [Other clusters](docs/other_clusters.md)  \r\n3. [Running the pipeline with test data](docs/usage.md)\r\n4. [Running the pipeline with your own config](docs/usage.md)\r\n5. [Running on local machine or cluster](docs/other_clusters.md)\r\n6. [Running docker and singularity](docs/soft_requirements.md)\r\n\r\n\r\n## Support\r\nWe track our open tasks using github's [issues](https://github.com/h3abionet/chipimputation/issues)\r\n\r\n\r\n## Citation\r\nThis  workflow which was developed as part of the H3ABioNet Hackathon held in Pretoria, SA in 2016. Should want to reference it, please use:  \r\n>Baichoo S, Souilmi Y, Panji S, Botha G, Meintjes A, Hazelhurst S, Bendou H, Beste E, Mpangase PT, Souiai O, Alghali M, Yi L, O'Connor BD, Crusoe M, Armstrong D, Aron S, Joubert F, Ahmed AE, Mbiyavanga M, Heusden PV, Magosi LE, Zermeno J, Mainzer LS, Fadlelmola FM, Jongeneel CV, Mulder N. Developing reproducible bioinformatics analysis workflows for heterogeneous computing environments to support African genomics. BMC Bioinformatics. 2018 Nov 29;19(1):457. doi: 10.1186/s12859-018-2446-1. PubMed PMID: 30486782; PubMed Central PMCID: [PMC6264621](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6264621/).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1871",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1871?version=1",
        "name": "chipimputation",
        "number_of_steps": 0,
        "projects": [
            "BioX Fanatics"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-13",
        "versions": 1
    },
    {
        "create_time": "2025-08-13",
        "creators": [
            "Phil Ewels"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-methylseq_logo_dark.png\">\n    <img alt=\"nf-core/methylseq\" src=\"docs/images/nf-core-methylseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/methylseq/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/methylseq/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/methylseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/methylseq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/methylseq/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.1343417-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.1343417)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/methylseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23methylseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/methylseq)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/methylseq** is a bioinformatics analysis pipeline used for Methylation (Bisulfite) sequencing data. It pre-processes raw data from FastQ inputs, aligns the reads and performs extensive quality-control on the results.\n\n![nf-core/methylseq metro map](docs/images/4.0.0_metromap.png)\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker / Singularity / Podman / Charliecloud / Apptainer containers making installation trivial and results highly reproducible.\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources.The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/methylseq/results).\n\n> Read more about **Bisulfite Sequencing & Three-Base Aligners** used in this pipeline [here](docs/usage/bs-seq-primer.md)\n\n## Pipeline Summary\n\nThe pipeline allows you to choose between running either [Bismark](https://github.com/FelixKrueger/Bismark) or [bwa-meth](https://github.com/brentp/bwa-meth) / [MethylDackel](https://github.com/dpryan79/methyldackel).\n\nChoose between workflows by using `--aligner bismark` (default, uses bowtie2 for alignment), `--aligner bismark_hisat` or `--aligner bwameth`. For higher performance, the pipeline can leverage the [Parabricks implementation of bwa-meth (fq2bammeth)](https://docs.nvidia.com/clara/parabricks/latest/documentation/tooldocs/man_fq2bam_meth.html), which implements the baseline tool `bwa-meth` in a performant method using fq2bam (BWA-MEM + GATK) as a backend for processing on GPU. To use this option, include the `gpu` profile along with `--aligner bwameth`.\n\nNote: For faster CPU runs with BWA-Meth, enable the BWA-MEM2 algorithm using `--use_mem2`. The GPU pathway (Parabricks) requires `-profile gpu` and a container runtime (Docker, Singularity, or Podman); Conda/Mamba are not supported for the GPU module.\n\n| Step                                         | Bismark workflow         | bwa-meth workflow     |\n| -------------------------------------------- | ------------------------ | --------------------- |\n| Generate Reference Genome Index _(optional)_ | Bismark                  | bwa-meth              |\n| Merge re-sequenced FastQ files               | cat                      | cat                   |\n| Raw data QC                                  | FastQC                   | FastQC                |\n| Adapter sequence trimming                    | Trim Galore!             | Trim Galore!          |\n| Align Reads                                  | Bismark (bowtie2/hisat2) | bwa-meth              |\n| Deduplicate Alignments                       | Bismark                  | Picard MarkDuplicates |\n| Extract methylation calls                    | Bismark                  | MethylDackel          |\n| Sample report                                | Bismark                  | -                     |\n| Summary Report                               | Bismark                  | -                     |\n| Alignment QC                                 | Qualimap _(optional)_    | Qualimap _(optional)_ |\n| Sample complexity                            | Preseq _(optional)_      | Preseq _(optional)_   |\n| Project Report                               | MultiQC                  | MultiQC               |\n\nOptional targeted sequencing analysis is available via `--run_targeted_sequencing` and `--target_regions_file`; see the [usage documentation](https://nf-co.re/methylseq/usage) for details.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2,genome\nSRR389222_sub1,https://github.com/nf-core/test-datasets/raw/methylseq/testdata/SRR389222_sub1.fastq.gz,,\nSRR389222_sub2,https://github.com/nf-core/test-datasets/raw/methylseq/testdata/SRR389222_sub2.fastq.gz,,\nSRR389222_sub3,https://github.com/nf-core/test-datasets/raw/methylseq/testdata/SRR389222_sub3.fastq.gz,,\nEcoli_10K_methylated,https://github.com/nf-core/test-datasets/raw/methylseq/testdata/Ecoli_10K_methylated_R1.fastq.gz,https://github.com/nf-core/test-datasets/raw/methylseq/testdata/Ecoli_10K_methylated_R2.fastq.gz,\n```\n\n> Each row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\nNow, you can run the pipeline using default parameters as:\n\n```bash\nnextflow run nf-core/methylseq --input samplesheet.csv --outdir <OUTDIR> --genome GRCh37 -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/methylseq/usage) and the [parameter documentation](https://nf-co.re/methylseq/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/methylseq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the [output documentation](https://nf-co.re/methylseq/output).\n\n## Credits\n\nnf-core/methylseq was originally written by Phil Ewels ([@ewels](https://github.com/ewels)), and Sateesh Peri ([@sateeshperi](https://github.com/sateeshperi)) is its active maintainer.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- Felix Krueger ([@FelixKrueger](https://github.com/FelixKrueger))\n- Edmund Miller ([@EMiller88](https://github.com/emiller88))\n- Rickard Hammar\u00e9n ([@Hammarn](https://github.com/Hammarn/))\n- Alexander Peltzer ([@apeltzer](https://github.com/apeltzer/))\n- Patrick H\u00fcther ([@phue](https://github.com/phue/))\n- Maxime U Garcia ([@maxulysse](https://github.com/maxulysse/))\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#methylseq` channel](https://nfcore.slack.com/channels/methylseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/methylseq for your analysis, please cite it using the following doi: [10.5281/zenodo.1343417](https://doi.org/10.5281/zenodo.1343417)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "998",
        "keep": "To Curate",
        "latest_version": 20,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/998?version=20",
        "name": "nf-core/methylseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "epigenomics",
            "bisulfite-sequencing",
            "dna-methylation",
            "em-seq",
            "epigenome",
            "methyl-seq",
            "pbat",
            "rrbs"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-13",
        "versions": 20
    },
    {
        "create_time": "2025-08-12",
        "creators": [
            "Martijn Melissen"
        ],
        "description": "**Workflow for long read quality control, contamination filtering, assembly, variant calling and annotation.**\r\n\r\nSteps:  \r\n- Preprocessing of reference file : https://workflowhub.eu/workflows/1818  \r\n- LongReadSum before and after filtering (read quality control)  \r\n- Filtlong filter on quality and length  \r\n- Flye assembly  \r\n- Minimap2 mapping of reads and assembly  \r\n- Clair3 variant calling of reads  \r\n- Freebayes variant calling of assembly  \r\n- Optional Bakta annotation of genomes with no reference  \r\n- SnpEff building or downloading of a database  \r\n- SnpEff functional annotation  \r\n- Liftoff annotation lift over  \r\n\r\n**All tool CWL files and other workflows can be found here:**\r\n  Tools: https://git.wur.nl/ssb/automated-data-analysis/cwl/-/tree/main/tools\r\n  Workflows: https://git.wur.nl/ssb/automated-data-analysis/cwl/-/tree/main/workflows\r\n",
        "doi": null,
        "edam_operation": [
            "Conversion",
            "Generation"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Genomics",
            "Sequence assembly"
        ],
        "filtered_on": "plasmid.* in tags",
        "id": "1868",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1868?version=1",
        "name": "Long Read WGS pipeline",
        "number_of_steps": 31,
        "projects": [
            "Systems and Synthetic Biology"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "assembly",
            "bioinformatics",
            "cwl",
            "galaxy",
            "genomics",
            "python",
            "workflows",
            "plasmid",
            "plasmids"
        ],
        "tools": [
            "Flye",
            "Filtlong",
            "Minimap2",
            "Clair3",
            "FreeBayes",
            "Bakta",
            "snpEff",
            "Liftoff"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-07-22",
        "creators": [
            "Martijn Melissen"
        ],
        "description": "**Workflow for preprocessing a reference file. **\r\n\r\nSteps:  \r\n-When a GenBank file is not provided, it is downloaded from NCBI based on a accession number.  \r\n-When multiple plasmid GenBank files are provided, they are merged into one file.  \r\n-When any amount of plasmid GenBank files are provided, the reference is merged with the plasmid GenBank file(s) into one file. A FASTA file is also extracted.  \r\n-When no plasmid Genbank files are provided, a FASTA file is extracted from the reference GenBank file.  \r\n-A GFF3 file is extracted from the final GenBank file.  \r\n-The final step determines the relevant outputs.  \r\n\r\n**All tool CWL files and other workflows can be found here:**  \r\n  Tools: https://git.wur.nl/ssb/automated-data-analysis/cwl/-/tree/main/tools  \r\n  Workflows: https://git.wur.nl/ssb/automated-data-analysis/cwl/-/tree/main/workflows  \r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid.* in tags",
        "id": "1818",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1818?version=1",
        "name": "reference (and plasmid) preprocessing workflow",
        "number_of_steps": 6,
        "projects": [
            "Systems and Synthetic Biology"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "cwl",
            "genbank",
            "gff3",
            "plasmids"
        ],
        "tools": [
            "Determines relevant final outputs.",
            "Merges plasmids when more than one are present.",
            "Merges the plasmid(s) with the reference GenBank file.",
            "Downloads the associated GenBank file from the supplied accession number.",
            "Extracts GFF3 annotation file from the (merged) reference.",
            "Extracts FASTA file from input reference file when no plasmids are provided."
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-08-12",
        "creators": [],
        "description": "# WHALE: (W)orkflow for (H)uman-genome (A)nalysis of (L)ong-read (E)xperiments\r\n\r\n## Introduction\r\n\r\n**WHALE** is a bioinformatics pipeline based on Nextflow and nf-core for long-read DNA sequencing analysis. It takes a samplesheet as input and performs quality control, alignment, variant calling and annotation.\r\n\r\n## Pipeline summary\r\n\r\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\r\n2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\r\n3. Alignment ([`Minimap2`](https://github.com/lh3/minimap2))\r\n4. Variant calling\r\n    - Single Nucleotide Variant (SNV) calling ([`DeepVariant`](https://github.com/google/deepvariant), [`Clair3`](https://github.com/HKU-BAL/Clair3), [`NanoCaller`](https://github.com/WGLab/NanoCaller))\r\n    - Structural Variant (SV) calling ([`Sniffles2`](https://github.com/fritzsedlazeck/Sniffles), [`CuteSV`](https://github.com/tjiangHIT/cuteSV), [`SVIM`](https://github.com/eldariont/svim))\r\n5. Merge variant calling\r\n6. Annotation\r\n    - SNV annotation ([`VEP`](https://github.com/Ensembl/ensembl-vep))\r\n    - SV annotation ([`AnnotSV`](https://github.com/lgmgeo/AnnotSV))\r\n\r\n## Usage\r\n\r\nFirst, prepare a samplesheet with your input data. Depending on which step of the analysis you want to run, the input data type can be: fastq, bam (and bai), vcf or bed. The samplesheet should look as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,fastq\r\nA123,/path/to/your/input/file/A123.fastq.gz\r\nB456,/path/to/your/input/file/B456.fastq.gz\r\n```\r\n\r\nThere are two types of full analysis:\r\n- SNV analysis: -profile snv_analysis\r\n- SV analysis: -profile sv_analysis\r\n    \r\n  Each full analysis can start with:\r\n  - Alignment: --step mapping (input data type: fastq) (default)\r\n  - Variant calling: --step variant_calling (input data type: bam and bai)\r\n    \r\nA specific step of the analysis can be executed:\r\n- SNV calling (and merge): -profile snv_calling (input data type: bam and bai)\r\n- SV calling (and merge): -profile sv_calling (input data type: bam and bai)\r\n- SNV annotation: -profile snv_annotation (input data type: vcf)\r\n- SV annotation: -profile sv_annotation (input data type: bed)\r\n\r\nProfiles to use in the CCC (UAM):\r\n- -profile uam,singularity,batch\r\n- -profile uam_allcontigs,singularity,batch\r\n\r\nProfiles to use in the server:\r\n- -profile tblabserver,singularity\r\n- -profile tblabserver_allcontigs,singularity\r\n\r\n## Examples\r\n\r\nSNV and SV analysis starting with variant calling in the server:\r\n\r\n```bash\r\nnextflow run WHALE \\\r\n   -profile snv_analysis,sv_analysis,tblabserver,singularity \\\r\n   --input samplesheet.csv \\\r\n   --outdir <OUTDIR> \\\r\n   --step variant_calling\r\n```\r\n\r\nSV calling in the CCC:\r\n\r\n```bash\r\nnextflow run WHALE \\\r\n   -profile sv_calling,uam,singularity,batch \\\r\n   --input samplesheet.csv \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\n## Pipeline output\r\n\r\n**WHALE** will create the following subdirectories in the output directory:\r\n- alignment\r\n- snv_calling\r\n  - snv_merge\r\n- snv_annotation\r\n- sv_calling\r\n  - sv_merge\r\n- sv_annotation\r\n  - overlapping_sv_samples\r\n- multiqc\r\n- pipeline_info\r\n\r\n## Citations\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nIllustration by [Yolanda Ben\u00edtez](https://github.com/yolandabq)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Genomics"
        ],
        "filtered_on": "profil.* in description",
        "id": "1865",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1865?version=1",
        "name": "WHALE",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Unit IIS-FJD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "nextflow",
            "long-read-sequencing"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-08-12",
        "creators": [],
        "description": "# PARROT-FJD\r\nPipeline of Analysis and Research of Rare diseases Optimized in Tblab - Fundaci\u00f3n Jim\u00e9nez D\u00edaz. This is a germline variant calling pipeline implemented in Nextflow which performs mapping, SNV/INDEL calling and annotation, and CNV calling and annotation for targeted sequencing (gene panels and WES) and whole genome sequencing. \r\n\r\n## How to run this pipeline\r\nThe different tasks previously mention are divided into different workflows which are specified usig the `--analysis` flag followed by the corresponding letters:\r\n - D (Download): It downloads the FASTQ files from BaseSpace. If CNV calling or no samples are specified all samples from a project will be downloaded. \r\n - M (Mapping): Specified FASTQ files from a directory (or the ones downloaded) are mapped into analysis-ready BAM files.\r\n - S (SNV/INDEL calling): Specified BAM files (or the ones just mapped) are used for SNV and INDEL calling using GATK by default. Dragen and DeepVariant are also available. The variant caller can be selected with the parameter `--vc_tools`. The options are: \r\n     - `gatk`: Haplotypecaller\r\n     - `dragen`\r\n     - `deepvariant`\r\n     - `all` (equivalent to: gatk,dragen,deepvariant). Using this option, resulting vcfs will be merged into a single-sample VCF (single VCFs from each variant caller are also available)\r\n   More than one tool can be chosen using \",\" (`--vc_tools gatk,dragen`)\r\n - A (Annotation of SNVs and INDELS). Specified VCF files from a directory (or the ones just generated in the SNV/INDEL calling step) are annotated and transformed into a TSV file.\r\n - C (CNV calling and annotation). Specified BAM files (or the ones just mapped) are used for CNV calling using Exomedepth, Convading, Panelcn.mops and GATK, and annotation using AnnotSV. In the case of analycing WGS (`--capture G`) the variant calling used is Manta.\r\n - T (mitochondrial SNV/indel calling + annotation). Specified BAM files (or the ones just mapped) are used for mitochondrial SNV and INDEL calling using Mutect2. The reference chrM used is from hg38. \r\n - H (Expansion Hunter). Specified BAM files (or the ones just mapped) are used for estimating repeat sizes using Expansion Hunter. This analysis can be perforned separately including \"H\" flag in `--analysis`, or it can be included in \"C\" (CNV calling) analysis using the option `--expansion_hunter true`, with or without the flag `--analysis H`. \r\n\r\nMapping and variant calling processes can be parallelized to speed up the analysis. These option can be activated using the parameters `--parallel_mapping true` and `--parallel_calling true`.\r\n\r\n`--parallel_mapping true`: FASTP will be executed to split FASTQ files in three chunks that will be mapped in parallel.   \r\n`--parallel_calling true`: BAM file will be split by chromosomes in smaller BAM files that will be processed in parallel. \r\n\r\nFor using this options, using `--cpus-per-task=44` is recommended. \r\n\r\nYou can generate and keep a cram file out of your bam, when running either MS or just S. The cram is generated inside the out folder: /out/cram/. By default: --keep_cram false\r\n`--keep_cram true`: generate and keep cram file\r\n\r\nYou can use cram file as input using the option `--alignment_file=\"cram\"`. \r\n\r\nYou can generate the mosdepth bed file from your bam, when running either MS or just S. The bed is generated inside the out folder: /out/qc/mosdepth_cov/. By default: --mosdepth_bed false\r\n`--mosdepth_bed true`: generate the mosdepth.bed file needed to update the db of allele frequencies. \r\n\r\nYou need to define with what technique your data was generated: WES, WGS or CES. By default is WES\r\n\t`--technique WES` : My data is/are WES samples\r\n\r\nWhen you use the D (Download option) you need to specify the path to your \"bs\" software is:\r\nbs is the BaseSpace Sequence Hub CLI software.\r\nTo download the software visit the lik and follow instructions: https://developer.basespace.illumina.com/docs/content/documentation/cli/cli-overview\r\nAfter downloading you need to authenticate with your basespace account running the following command: bs auth\r\nAfter authentication you can already run PARROT-FJD with the D (download option) by specifying where your \"bs\" software is stored.\r\nExample: TBLAB -> --baseuser /home/graciela/bin/\r\nExample: UAM -> --baseuser /lustre/home/graciela/\r\n\r\n`--baseuser /lustre/home/graciela/ `: download fastqs from basespace for graciela when running in UAM\r\n\r\nThere are different profiles available depending on the reference release to use, where to run it, and type of contenerization:\r\n\r\nMandatory to choose one:\r\n - hg19: use the reference genome hg19\r\n - hg38: use the reference genome hg38\r\n\r\nMandatory to choose one:\r\n - tblabserver: run pipeline in the server just for the canonical chromosomes\r\n - tblabserver_allcontigs: run pipeline in the server just for all contigs\r\n - uam: run pipeline in the CCC (UAM) just for the canonical chromosomes\r\n - uam_allcontigs: run pipeline in the CCC (UAM) just for all contigs\r\n\r\nMandatory to choose one:\r\n - docker: run pipeline using docker containers\r\n - singularity: run pipeline using singularity containers\r\n\r\nMandatory when running in the CCC (UAM):\r\n - batch: run pipeline in for slurm executor in the CCC (UAM)\r\n\r\n\r\nProfiles to use in the CCC (UAM): \r\n - `-profile hg19,singularity,uam,batch` \r\n - `-profile hg38,singularity,uam,batch`\r\n - `-profile hg19,singularity,uam_allcontigs,batch` \r\n - `-profile hg38,singularity,uam_allcontigs,batch`\r\n\r\nProfiles to use in the server:\r\n - `-profile hg19,singularity,tblabserver` \r\n - `-profile hg38,singularity,tblabserver`\r\n - `-profile hg19,docker,tblabserver` \r\n - `-profile hg38,docker,tblabserver`\r\n - `-profile hg19,singularity,tblabserver_allcontigs` \r\n - `-profile hg38,singularity,tblabserver_allcontigs`\r\n - `-profile hg19,docker,tblabserver_allcontigs` \r\n - `-profile hg38,docker,tblabserver_allcontigs`\r\n\r\nCheck the file nexflow.config to see the description of all arguments.\r\n\r\n## Examples\r\n\r\nPerform a complete analysis from downloading samples from Basespace to SNV/INDEL and CNV calling and annotation.\r\n```\r\nnextflow run /home/gonzalo/nextflowtest/NextVariantFJD/main.nf \\\r\n-profile hg38,singularity,tblabserver --analysis DMSAC \\\r\n--input project_name \\\r\n--output /output/path/ \\\r\n--bed /path/to/captured/regions.bed \\\r\n-with-report report.html\r\n```\r\n\r\n\r\nPerform an analysis from mapping to SNV/INDEL calling and annotation.\r\n```\r\nnextflow run /home/gonzalo/nextflowtest/NextVariantFJD/main.nf \\\r\n-profile hg38,singularity,tblabserver --analysis MSA \\\r\n--input /input/path/to/fastq/ \\\r\n--output /output/path/ \\\r\n-with-report report.html\r\n```\r\n\r\n\r\nPerform a complete analysis from downloading samples from Basespace to SNV/INDEL and CNV calling and annotation. Variant calling analysis is performed to a subset of samples and genes.\r\n```\r\nnextflow run /home/gonzalo/nextflowtest/NextVariantFJD/main.nf \\\r\n-profile hg38,singularity,tblabserver --analysis DMSAC \\\r\n--input project_name \\\r\n--output /output/path/ \\\r\n--bed /path/to/captured/regions.bed \\\r\n--samples /path/to/samplefile.txt \\\r\n--genelist /path/to/genelist.txt \\\r\n-with-report report.html\r\n```\r\n\r\n\r\nPerform a SNV/INDEL calling and annotation analysis. Variant calling analysis is performed to a subset of samples. Genes are prioritized using genelista and glowgenes.\r\n```\r\nnextflow run /home/gonzalo/nextflowtest/NextVariantFJD/main.nf \\\r\n-profile hg38,singularity,tblabserver --analysis SA \\\r\n--input /input/path/to/vcfs/ \\\r\n--output /output/path/ \\\r\n--samples /path/to/samplefile.txt \\\r\n--genelist /path/to/genelist.txt \\\r\n--glowgenes /path/to/glowgenes_result.txt \\\r\n-with-report report.html\r\n```\r\n\r\n## Versions\r\n\r\nGATK 4.4.0.0\r\nVEP release 105\r\ndeepvariant v1.4.0\r\nannotsv 3.1.1\r\nmanta 1.6.0\r\nBWA 0.7.17-r1198-dirty\r\nbcftools 1.15\r\nbedtools 2.27.1\r\nR version 4.2.3\r\npython 3.6\r\n\r\n## License\r\n\r\nPARROT-FJD source code is provided under the [**Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)**](https://creativecommons.org/licenses/by-nc-sa/4.0/). PARROT-FJD includes several third party packages provided under other open source licenses, please check them for additional details.\r\n\r\n[![Licencia de Creative Commons](https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-nc-sa/4.0/)\r\n\r\n## Projects\r\nPI22/00579 \u201cDB4DISCOVERY. Desarrollo de m\u00e9todos de priorizaci\u00f3n y descubrimientos de variantes mediante el reuso de informaci\u00f3n gen\u00f3mica agregada en una base de datos espec\u00edfica de una cohorte.\u201d ISCIII\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Genomics"
        ],
        "filtered_on": "profil.* in description",
        "id": "1864",
        "keep": "Reject",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1864?version=1",
        "name": "PARROT-FJD",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Unit IIS-FJD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "nextflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-08-12",
        "creators": [],
        "description": "# Introduction\r\n\r\n**nf-CBRA-snvs** (nf-core - CIBERER Bioinformatics for Rare diseases Analysis - Small Nucleotide Variant) is a workflow optimized for the analysis of rare diseases, designed to detect SNVs and INDELs in targeted sequencing data (CES/WES) as well as whole genome sequencing (WGS).\r\n\r\nThis pipeline is developed using Nextflow, a workflow management system that enables an easy execution across various computing environments. It uses Docker or Singularity containers, simplifying setup and ensuring reproducibility of results. The pipeline assigns a container to each process, which simplifies the management and updating of software dependencies. When possible, processes are sourced from nf-core/modules, promoting reusability across all nf-core pipelines and contributing to the broader Nextflow community.\r\n\r\n\r\n# Pipeline summary\r\n\r\nThe pipeline can perform the following steps:\r\n\r\n- **Mapping** of the reads to reference (BWA-MEM)\r\n- Process BAM file (`GATK MarkDuplicates`, `GATK BaseRecalibrator` and `GATK ApplyBQSR`)\r\n- **Variant calling** with the following tools:\r\n\r\n  - GATK4 Haplotypecaller (`run_gatk = true`). This subworkflow includes:\r\n    - **GATK4 Haplotypecaller**.\r\n    - **Hard Filters** and **VarianFiltration** to mark PASS variants. More information [here](docs/variant_calling.md).\r\n    - **Bcftools Filter** to keep PASS variants on chr1-22, X, Y.\r\n    - **Split Multialletic**.\r\n  - Dragen (`run_dragen = true`). This subworkflow includes:\r\n    - **GATK4 Calibratedragstrmodel**\r\n    - **GATK4 Haplotypecaller** with `--dragen-mode`.\r\n    - **VarianFiltration** with `--filter-expression \"QUAL < 10.4139\" --filter-name \"DRAGENHardQUAL\"`to mark PASS variants. More information [here](https://gatk.broadinstitute.org/hc/en-us/articles/4407897446939--How-to-Run-germline-single-sample-short-variant-discovery-in-DRAGEN-mode).\r\n    - **Bcftools Filter** to keep PASS variants on chr1-22, X, Y.\r\n    - **Split Multialletic**.\r\n  - DeepVariant (`run_deepvariant = true`). This subworkflow includes:\r\n    - **DeepVariant makeexamples**: Converts the input alignment file to a tfrecord format suitable for the deep learning model.\r\n    - **DeepVariant callvariants**: Call variants based on input tfrecords. The output is also in tfrecord format, and needs postprocessing to convert it to vcf.\r\n    - **DeepVariant postprocessvariants**: Convert variant calls from callvariants to VCF, and also create GVCF files based on genomic information from makeexamples. More information [here](https://github.com/nf-core/modules/tree/master/modules/nf-core/deepvariant).\r\n    - **Bcftools Filter** to keep PASS variants on chr1-22, X, Y.\r\n    - **Split Multialletic**.\r\n\r\n- **Merge and integration** of the vcfs obtained with the different tools.\r\n- **Annotation** of the variants:\r\n  - Regions of homozygosity (ROHs) with [AUTOMAP](https://github.com/mquinodo/AutoMap)\r\n  - Effect of the variants with [Ensembl VEP](https://www.ensembl.org/info/docs/tools/vep/index.html) using the flag `--everything`, which includes the following options: `--sift b, --polyphen b, --ccds, --hgvs, --symbol, --numbers, --domains, --regulatory, --canonical, --protein, --biotype, --af, --af_1kg, --af_esp, --af_gnomade, --af_gnomadg, --max_af, --pubmed, --uniprot, --mane, --tsl, --appris, --variant_class, --gene_phenotype, --mirna`\r\n  - Postvep format VEP tab demilited output and filter variants by minor allele frequency (`--maf`).\r\n  - You can enhance the annotation by incorporating gene rankings from [GLOWgenes](https://www.translationalbioinformaticslab.es/tblab-home-page/tools/glowgenes), a network-based algorithm developed to prioritize novel candidate genes associated with rare diseases. Precomputed rankings based on PanelApp gene panels are available [here](https://github.com/TBLabFJD/GLOWgenes/blob/master/precomputed_panelAPP/GLOWgenes_precomputed_panelAPP.tsv). To include a specific GLOWgenes ranking, use the option `--glowgenes_panel (path to the panel.txt)`, for example: `--glowgenes_panel https://raw.githubusercontent.com/TBLabFJD/GLOWgenes/refs/heads/master/precomputed_panelAPP/GLOWgenes_prioritization_Neurological_ciliopathies_GA.txt`. Additionally, you can include the Gene-Disease Specificity Score (SGDS) using: `--glowgenes_sgds https://raw.githubusercontent.com/TBLabFJD/GLOWgenes/refs/heads/master/SGDS.csv`. This score ranges from 0 to 1, where 1 indicates a gene ranks highly for only a few specific diseases (high specificity), and 0 indicates the gene consistently ranks highly across many diseases (low specificity).\r\n\r\n\r\n# Usage\r\n\r\nFirst, prepare a samplesheet with your input data:\r\n\r\n```\r\nsample,fastq_1,fastq_2\r\nSAMPLE_PAIRED_END,/path/to/fastq/files/AEG588A1_S1_L002_R1_001.fastq.gz,/path/to/fastq/files/AEG588A1_S1_L002_R2_001.fastq.gz\r\n```\r\n\r\nEach row represents a pair of paired end fastq files. \r\n\r\nYou can run the pipeline using: \r\n\r\n```\r\nnextflow run nf-cbra-snvs/main.nf \\\r\n   -profile <docker/singularity/.../institute> \\\r\n   --input samplesheet.csv \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\nFor more details and further functionality, please refer to the [usage](docs/usage.md) documentation.\r\n\r\n\r\n# Pipeline output\r\n\r\nFor details about the output files and reports, please refer to the [output](docs/output.md) documentation.\r\n\r\n# Credits\r\n\r\nnf-CBRA-snvs was developed within the framework of a call for intramural cooperative and complementary actions (ACCI) funded by CIBERER (Biomedical Research Network Centre for Rare Diseases).\r\n\r\n**Main Developer**\r\n- [Yolanda Ben\u00edtez Quesada](https://github.com/yolandabq)\r\n\r\n**Coordinator**\r\n- [Carlos Ruiz Arenas](https://github.com/yocra3)\r\n\r\n**Other contributors**\r\n- [Graciela Ur\u00eda Regojo](https://github.com/guriaregojo)\r\n- [Pedro Garrido Rodr\u00edguez](https://github.com/pedro-garridor)\r\n- [Rafa Farias Varona](https://github.com/RafaFariasVarona)\r\n- [Pablo Minguez](https://github.com/pminguez)\r\n- [Daniel Lopez](https://github.com/dlopez-bioinfo)\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Genomics",
            "Rare diseases"
        ],
        "filtered_on": "profil.* in description",
        "id": "1862",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1862?version=1",
        "name": "nf-CBRA-snvs",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Unit IIS-FJD"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "nextflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-12",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "Classification and visualization of ITS regions.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: EMBL's European Bioinformatics Institute, Rand Zoabi, Paul Zierep\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1856",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1856?version=1",
        "name": "MGnify's amplicon pipeline v5.0 - ITS",
        "number_of_steps": 30,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "__FILTER_EMPTY_DATASETS__",
            "",
            "tp_awk_tool",
            "biom_convert",
            "bedtools_maskfastabed",
            "collection_element_identifiers",
            "taxonomy_krona_chart",
            "mapseq",
            "__FILTER_FROM_FILE__"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "The MAPseq to Ampvis workflow processes MAPseq OTU tables and associated metadata for analysis in Ampvis2. This workflow involves reformatting MAPseq output datasets to produce structured output files suitable for Ampvis2.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Rand Zoabi, Mara Besemer\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1855",
        "keep": "Keep",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1855?version=1",
        "name": "MAPseq to ampvis2",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "tp_awk_tool",
            "query_tabular",
            "ampvis2_load",
            "collection_column_join",
            "collapse_dataset"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "Quality control subworkflow for paired-end reads. \n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: EMBL's European Bioinformatics Institute, Rand Zoabi, Paul Zierep\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1854",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1854?version=1",
        "name": "MGnify's amplicon pipeline v5.0 - Quality control PE",
        "number_of_steps": 17,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "fastp",
            "__UNZIP_COLLECTION__",
            "fastq_filter",
            "fastqc",
            "cshl_fasta_formatter",
            "fastq_to_fasta_python",
            "prinseq",
            "tp_find_and_replace",
            "mgnify_seqprep",
            "trimmomatic",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "MGnify's amplicon pipeline v5.0. Including the Quality control for single-end and paired-end reads, rRNA-prediction, and ITS sub-WFs.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Workflow Author(s)**:  EMBL's European Bioinformatics Institute, Rand Zoabi, Paul Zierep\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1853",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1853?version=1",
        "name": "MGnify's amplicon pipeline v5.0",
        "number_of_steps": 20,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "",
            "tp_awk_tool",
            "CONVERTER_gz_to_uncompressed",
            "CONVERTER_uncompressed_to_gz",
            "__MERGE_COLLECTION__",
            "fastq_dl"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "This workflow creates taxonomic summary tables out of the amplicon pipeline results. \n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Rand Zoabi\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1851",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1851?version=1",
        "name": "MGnify amplicon summary tables",
        "number_of_steps": 10,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "tp_awk_tool",
            "filter_tabular",
            "query_tabular",
            "collection_column_join",
            "Grouping1"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-11",
        "creators": [],
        "description": "Quality control subworkflow for single-end reads.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: MGnify - EMBL, Rand Zoabi, Paul Zierep\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1850",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1850?version=1",
        "name": "MGnify's amplicon pipeline v5.0 - Quality control SE",
        "number_of_steps": 14,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "fastq_filter",
            "fastqc",
            "cshl_fasta_formatter",
            "fastq_to_fasta_python",
            "prinseq",
            "tp_find_and_replace",
            "trimmomatic",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-08",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-proteinfamilies_logo_dark.png\">\n    <img alt=\"nf-core/proteinfamilies\" src=\"docs/images/nf-core-proteinfamilies_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/proteinfamilies/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/proteinfamilies/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/proteinfamilies/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/proteinfamilies/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/proteinfamilies/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.14881993-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.14881993)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/proteinfamilies)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23proteinfamilies-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/proteinfamilies)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/proteinfamilies** is a bioinformatics pipeline that generates protein families from amino acid sequences and/or updates existing families with new sequences.\nIt takes a protein fasta file as input, clusters the sequences and then generates protein family Hiden Markov Models (HMMs) along with their multiple sequence alignments (MSAs).\nOptionally, paths to existing family HMMs and MSAs can be given (must have matching base filenames one-to-one) in order to update with new sequences in case of matching hits.\n\n<p align=\"center\">\n    <img src=\"docs/images/proteinfamilies_workflow.png\" alt=\"nf-core/proteinfamilies workflow overview\">\n</p>\n\n### Check quality\n\nGenerate input amino acid sequence statistics with ([`SeqKit`](https://github.com/shenwei356/seqkit/))\n\n### Create families\n\n1. Cluster sequences ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/))\n2. Perform multiple sequence alignment (MSA) ([`FAMSA`](https://github.com/refresh-bio/FAMSA/) or [`mafft`](https://github.com/GSLBiotech/mafft/))\n3. Optionally, clip gap parts of the MSA ([`ClipKIT`](https://github.com/JLSteenwyk/ClipKIT/))\n4. Generate family HMMs and fish additional sequences into the family ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\n5. Optionally, remove redundant families by comparing family representative sequences against family models with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\n6. Optionally, from the remaining families, remove in-family redundant sequences by strictly clustering with ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/)) and keep cluster representatives\n7. Optionally, if in-family redundancy was not removed, reformat the `.sto` full MSAs to `.fas` with ([`HH-suite3`](https://github.com/soedinglab/hh-suite))\n8. Present statistics for remaining/updated family size distributions and representative sequence lengths ([`MultiQC`](http://multiqc.info/))\n\n### Update families\n\n1. Find which families to update by comparing the input sequences against existing family models with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\n2. For non hit sequences continue with the above: A. Create families. For hit sequences and families continue to: 3\n3. Extract family sequences ([`SeqKit`](https://github.com/shenwei356/seqkit/)) and concatenate with filtered hit sequences of each family\n4. Optionally, remove in-family redundant sequences by strictly clustering with ([`MMseqs2`](https://github.com/soedinglab/MMseqs2/)) and keeping cluster representatives\n5. Perform multiple sequence alignment (MSA) ([`FAMSA`](https://github.com/refresh-bio/FAMSA/) or [`mafft`](https://github.com/GSLBiotech/mafft/))\n6. Optionally, clip gap parts of the MSA ([`ClipKIT`](https://github.com/JLSteenwyk/ClipKIT/))\n7. Update family HMM with ([`hmmer`](https://github.com/EddyRivasLab/hmmer/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fasta,existing_hmms_to_update,existing_msas_to_update\nCONTROL_REP1,input/mgnifams_input_small.fa,,\n```\n\nEach row contains a fasta file with amino acid sequences (can be zipped or unzipped).\nOptionally, a row may contain tarball archives (tar.gz) of existing families' HMM and MSA folders, in order to be updated.\nIn this case, the HMM and MSA files must be matching in numbers and in base filenames (not the extension).\nHit families/sequences will be updated, while no hit sequences will create new families.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/proteinfamilies \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/proteinfamilies/usage) and the [parameter documentation](https://nf-co.re/proteinfamilies/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/proteinfamilies/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/proteinfamilies/output).\n\n## Credits\n\nnf-core/proteinfamilies was originally written by Evangelos Karatzas.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Martin Beracochea](https://github.com/mberacochea)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#proteinfamilies` channel](https://nfcore.slack.com/channels/proteinfamilies) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/proteinfamilies for your analysis, please cite it using the following doi: [10.5281/zenodo.14881993](https://doi.org/10.5281/zenodo.14881993).\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1294",
        "keep": "To Curate",
        "latest_version": 5,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1294?version=5",
        "name": "nf-core/proteinfamilies",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "proteomics",
            "protein-families"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-08",
        "versions": 5
    },
    {
        "create_time": "2025-08-08",
        "creators": [
            "Jannik Seidel"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-detaxizer_logo_dark.png\">\n    <img alt=\"nf-core/detaxizer\" src=\"docs/images/nf-core-detaxizer_logo_light.png\">\n  </picture>\n</h1>\n\n[![Cite Preprint](https://img.shields.io/badge/Cite%20Us!-Cite%20Publication-important?labelColor=000000)](https://doi.org/10.1101/2025.03.27.645632)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.10877147-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.10877147)\n\n[![GitHub Actions CI Status](https://github.com/nf-core/detaxizer/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/detaxizer/actions/workflows/nf-test.yml)[![GitHub Actions Linting Status](https://github.com/nf-core/detaxizer/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/detaxizer/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/detaxizer/results)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/detaxizer)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23detaxizer-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/detaxizer)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/detaxizer** is a bioinformatics pipeline that checks for the presence of a specific taxon in (meta)genomic fastq files and to filter out this taxon or taxonomic subtree. The process begins with quality assessment via FastQC and optional preprocessing (adapter trimming, quality cutting and optional length and quality filtering) using fastp, followed by taxonomic classification with kraken2 and/or bbduk, and optionally employs blastn for validation of the reads associated with the identified taxa. Users must provide a samplesheet to indicate the fastq files and, if utilizing bbduk in the classification and/or the validation step, fasta files for usage of bbduk and creating the blastn database to verify the targeted taxon.\n\n![detaxizer metro workflow](docs/images/Detaxizer_metro_workflow.png)\n\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n2. Optional pre-processing ([`fastp`](https://github.com/OpenGene/fastp))\n3. Classification of reads ([`Kraken2`](https://ccb.jhu.edu/software/kraken2/), and/or [`bbduk`](https://sourceforge.net/projects/bbmap/))\n4. Optional validation of searched taxon/taxa ([`blastn`](https://blast.ncbi.nlm.nih.gov/Blast.cgi))\n5. Filtering of the searched taxon/taxa from the reads (either from the raw files or the preprocessed reads, using either the output from the classification (kraken2 and/or bbduk) or blastn)\n6. Summary of the processes (how many were classified and optionally how many were validated)\n7. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n```csv title=\"samplesheet.csv\"\nsample,short_reads_fastq_1,short_reads_fastq_2,long_reads_fastq_1\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,AEG588A1_S1_L002_R3_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end). A third fastq file can be provided if long reads are present in your project. For more detailed information about the samplesheet, see the [usage documentation](docs/usage.md).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/detaxizer \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --classification_bbduk \\\n   --classification_kraken2 \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/detaxizer/usage) and the [parameter documentation](https://nf-co.re/detaxizer/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/detaxizer/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/detaxizer/output).\n\nGenerated samplesheets from the directory `/downstream_samplesheets/` can be used for the pipelines:\n\n- [nf-core/mag](https://nf-co.re/mag)\n- [nf-core/taxprofiler](https://nf-co.re/taxprofiler)\n\n## Credits\n\nnf-core/detaxizer was originally written by [Jannik Seidel](https://github.com/jannikseidelQBiC) at the [Quantitative Biology Center (QBiC)](http://qbic.life/).\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Daniel Straub](https://github.com/d4straub)\n\nThis work was initially funded by the German Center for Infection Research (DZIF).\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#detaxizer` channel](https://nfcore.slack.com/channels/detaxizer) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/detaxizer for your analysis, please cite it using the following preprint:\n\n> **nf-core/detaxizer: A Benchmarking Study for Decontamination from Human Sequences**\n>\n> Jannik Seidel, Camill Kaipf, Daniel Straub, Sven Nahnsen\n>\n> bioRxiv 2025.03.27.645632 [doi: 10.1101/2025.03.27.645632](https://doi.org/10.1101/2025.03.27.645632).\n\nAdditionally, the following doi can be cited: [10.5281/zenodo.10877147](https://doi.org/10.5281/zenodo.10877147)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "979",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/979?version=3",
        "name": "nf-core/detaxizer",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "fastq",
            "metabarcoding",
            "metagenomics",
            "de-identification",
            "decontamination",
            "edna",
            "filter",
            "long-reads",
            "microbiome",
            "nanopore",
            "short-reads",
            "shotgun",
            "taxonomic-classification",
            "taxonomic-profiling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-08",
        "versions": 3
    },
    {
        "create_time": "2025-08-04",
        "creators": [],
        "description": "Classification and visualization of SSU, LSU sequences.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [MGnify v5.0 Amplicon Pipeline](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mgnify-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [subworkflows](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_subworkflows.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: EMBL's European Bioinformatics Institute, Rand Zoabi, Paul Zierep\n\n**Tutorial Author(s)**: [Rand Zoabi](https://training.galaxyproject.org/training-material/hall-of-fame/RZ9082/)\n\n**Funder(s)**: [German Competence Center Cloud Technologies for Data Management and Processing (de.KCD)](https://training.galaxyproject.org/training-material/hall-of-fame/deKCD/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1842",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1842?version=1",
        "name": "MGnify's amplicon pipeline v5.0 - rRNA prediction",
        "number_of_steps": 47,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amplicon",
            "gtn",
            "galaxy",
            "metagenomics",
            "mgnify_amplicon",
            "name:microgalaxy"
        ],
        "tools": [
            "__FILTER_EMPTY_DATASETS__",
            "",
            "bedtools_getfastabed",
            "tp_awk_tool",
            "query_tabular",
            "biom_convert",
            "infernal_cmsearch",
            "cshl_fasta_formatter",
            "gops_concat_1",
            "collection_element_identifiers",
            "taxonomy_krona_chart",
            "cmsearch_deoverlap",
            "mapseq",
            "__FILTER_FROM_FILE__"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-11",
        "versions": 1
    },
    {
        "create_time": "2025-08-02",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-variantbenchmarking_logo_dark.png\">\n    <img alt=\"nf-core/variantbenchmarking\" src=\"docs/images/nf-core-variantbenchmarking_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/variantbenchmarking/actions/workflows/nf-test.yml/badge.svg)](https://github.com/nf-core/variantbenchmarking/actions/workflows/nf-test.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/variantbenchmarking/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/variantbenchmarking/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/variantbenchmarking/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.14916661-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.14916661)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/variantbenchmarking)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23variantbenchmarking-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/variantbenchmarking)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/variantbenchmarking** is designed to evaluate and validate the accuracy of variant calling methods in genomic research. Initially, the pipeline is tuned well for available gold standard truth sets (for example, Genome in a Bottle and SEQC2 samples) but it can be used to compare any two variant calling results. The workflow provides benchmarking tools for small variants including SNVs and INDELs, Structural Variants (SVs) and Copy Number Variations (CNVs) for germline and somatic analysis.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\n<p align=\"center\">\n    <img title=\"variantbenchmarking metro map\" src=\"docs/images/variantbenchmarking_metromap.png\" width=100%>\n</p>\n\nThe workflow involves several key processes to ensure reliable and reproducible results as follows:\n\n### Standardization and normalization of variants:\n\nThis initial step ensures consistent formatting and alignment of variants in test and truth VCF files for accurate comparison.\n\n- Subsample if input test vcf is multisample ([bcftools view](https://samtools.github.io/bcftools/bcftools.html#view))\n- Homogenization of multi-allelic variants, MNPs and SVs (including imprecise paired breakends and single breakends) ([variant-extractor](https://github.com/EUCANCan/variant-extractor))\n- Reformatting test VCF files from different SV callers ([svync](https://github.com/nvnieuwk/svync))\n- Rename sample names in test and truth VCF files ([bcftools reheader](https://samtools.github.io/bcftools/bcftools.html#reheader))\n- Splitting multi-allelic variants in test and truth VCF files ([bcftools norm](https://samtools.github.io/bcftools/bcftools.html#norm))\n- Deduplication of variants in test and truth VCF files ([bcftools norm](https://samtools.github.io/bcftools/bcftools.html#norm))\n- Left aligning of variants in test and truth VCF files ([bcftools norm](https://samtools.github.io/bcftools/bcftools.html#norm))\n- Use prepy in order to normalize test files. This option is only applicable for happy benchmarking of germline analysis ([prepy](https://github.com/Illumina/hap.py/tree/master))\n- Split SNVs and indels if the given test VCF contains both. This is only applicable for somatic analysis ([bcftools view](https://samtools.github.io/bcftools/bcftools.html#view))\n\n### Filtering options:\n\nApplying filtering on the process of benchmarking itself might makes it impossible to compare different benchmarking strategies. Therefore, for whom like to compare benchmarking methods this subworkflow aims to provide filtering options for variants.\n\n- Filtration of contigs ([bcftools view](https://samtools.github.io/bcftools/bcftools.html#view))\n- Include or exclude SNVs and INDELs ([bcftools filter](https://samtools.github.io/bcftools/bcftools.html#filter))\n- Size and quality filtering for SVs ([SURVIVOR filter](https://github.com/fritzsedlazeck/SURVIVOR/wiki))\n\n### Liftover of vcfs:\n\nThis sub-workflow provides option to convert genome coordinates of truth VCF and test VCFs and high confidence BED file to a new assembly. Golden standard truth files are build upon specific reference genomes which makes the necessity of lifting over depending on the test VCF in query. Lifting over one or more test VCFs is also possible.\n\n- Create sequence dictionary for the reference ([picard CreateSequenceDictionary](https://gatk.broadinstitute.org/hc/en-us/articles/360037068312-CreateSequenceDictionary-Picard)). This file can be saved and reused.\n- Lifting over VCFs ([picard LiftoverVcf](https://gatk.broadinstitute.org/hc/en-us/articles/360037060932-LiftoverVcf-Picard))\n- Lifting over high confidence coordinates ([UCSC liftover](http://hgdownload.cse.ucsc.edu/admin/exe))\n\n### Statistical inference of input test and truth variants:\n\nThis step provides insights into the distribution of variants before benchmarking by extracting variant statistics:.\n\n- SNVs, INDELs and complex variants ([bcftools stats](https://samtools.github.io/bcftools/bcftools.html#stats))\n- SVs by type ([SURVIVOR stats](https://github.com/fritzsedlazeck/SURVIVOR/wiki))\n\n### Benchmarking of variants:\n\nActual benchmarking of variants are split between SVs and small variants:\n\nAvailable methods for germline and somatic _structural variant (SV)_ benchmarking are:\n\n- Truvari ([truvari bench](https://github.com/acenglish/truvari/wiki/bench))\n- SVanalyzer ([svanalyzer benchmark](https://github.com/nhansen/SVanalyzer/blob/master/docs/svbenchmark.rst))\n- Rtgtools (only for BND) ([rtg bndeval](https://realtimegenomics.com/products/rtg-tools))\n\n> [!NOTE]\n> Please note that there is no somatic specific tool for SV benchmarking in this pipeline.\n\nAvailable methods for germline and somatic _CNVs (copy number variations)_ are:\n\n- Truvari ([truvari bench](https://github.com/acenglish/truvari/wiki/bench))\n- Wittyer ([witty.er](https://github.com/Illumina/witty.er/tree/master))\n- Intersection ([bedtools intersect](https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html))\n\n> [!NOTE]\n> Please note that there is no somatic specific tool for CNV benchmarking in this pipeline.\n\nAvailable methods for *small variants: SNVs and INDEL*s:\n\n- Germline variant benchmarking using ([rtg vcfeval](https://realtimegenomics.com/products/rtg-tools))\n- Germline variant benchmarking using ([hap.py](https://github.com/Illumina/hap.py/blob/master/doc/happy.md))\n- Somatic variant benchmarking using ([rtg vcfeval --squash-ploidy](https://realtimegenomics.com/products/rtg-tools))\n- Somatic variant benchmarking using ([som.py](https://github.com/Illumina/hap.py/tree/master?tab=readme-ov-file#sompy))\n\n> [!NOTE]\n> Please note that using happ.py and som.py with rtgtools as comparison engine is also possible. Check conf/tests/test_ga4gh.config as an example.\n\n### Intersection of benchmark regions:\n\nIntersecting test and truth BED regions produces benchmark metrics. Intersection analysis is especially recommended for _CNV benchmarking_ where result reports may variate per tool.\n\n- Convert SV or CNV VCF file to BED file, if no regions file is provided for test case using ([SVTK vcf2bed](https://github.com/broadinstitute/gatk-sv/blob/main/src/svtk/scripts/svtk))\n- Convert VCF file to BED file, if no regions file is provided for test case using ([Bedops convert2bed](https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/convert2bed.html#convert2bed))\n- Intersect the regions and gether benchmarking statistics using ([bedtools intersect](https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html))\n\n### Comparison of benchmarking results per TP, FP and FN files\n\nIt is essential to compare benchmarking results in order to infer uniquely or commonly seen TPs, FPs and FNs.\n\n- Merging TP, FP and FN results for happy, rtgtools and sompy ([bcftools merge](https://samtools.github.io/bcftools/bcftools.html#merge))\n- Merging TP, FP and FN results for Truvari and SVanalyzer ([SURVIVOR merge](https://github.com/fritzsedlazeck/SURVIVOR/wiki))\n- Conversion of VCF files to CSV to infer common and unique variants per caller (python script)\n\n### Reporting of benchmark results\n\nThe generation of comprehensive report that consolidates all benchmarking results.\n\n- Merging summary statistics per benchmarking tool (python script)\n- Plotting benchmark metrics per benchmarking tool (R script)\n- Create visual HTML report for the integration of NCBENCH ([datavzrd](https://datavzrd.github.io/docs/index.html))\n- Apply _MultiQC_ to visualize results\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nid,test_vcf,caller\ntest1,test1.vcf.gz,delly\ntest2,test2.vcf,gatk\ntest3,test3.vcf.gz,cnvkit\n```\n\nEach row represents a vcf file (test-query file). For each vcf file and variant calling method (caller) have to be defined.\n\nUser _has to provide truth_vcf and truth_id in config files_.\n\n> [!NOTE]\n> There are publicly available truth sources. For germline analysis, it is common to use [genome in a bottle (GiAB)](https://www.nist.gov/programs-projects/genome-bottle) variants. There are variate type of golden truths and high confidence regions for hg37 and hg38 references. Please select and use carefully.\n> For somatic analysis, [SEQC2 project](https://sites.google.com/view/seqc2/home/data-analysis/high-confidence-somatic-snv-and-indel-v1-2) released SNV, INDEL and CNV regions. One, can select and use those files.\n\nHere you can find example combinations of [truth files](docs/truth.md)\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/variantbenchmarking/usage) and the [parameter documentation](https://nf-co.re/variantbenchmarking/parameters).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/variantbenchmarking \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR> \\\n   --genome GRCh37 \\\n   --analysis germline \\\n   --truth_id HG002 \\\n   --truth_vcf truth.vcf.gz\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n> Conda profile is not available for SVanalyzer (SVBenchmark) tool, if you are planing to use the tool either choose docker or singularity.\n\n### Example usages\n\nThis pipeline enables quite a number of subworkflows suitable for different benchmarking senarios. Please go through [this documentation](docs/testcases.md) to learn some example usages which discusses about the test config files under conf/tests and tests/.\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/variantbenchmarking/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/variantbenchmarking/output).\n\nThis pipeline outputs benchmarking results per method besides to the inferred and compared statistics.\n\n## Credits\n\nnf-core/variantbenchmarking was originally written by K\u00fcbra Narc\u0131 ([@kubranarci](https://github.com/kubranarci)) as a part of benchmarking studies in German Human Genome Phenome Archieve Project ([GHGA](https://www.ghga.de/)).\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- Nicolas Vannieuwkerke ([@nvnienwk](https://github.com/nvnieuwk)),\n- Maxime Garcia ([@maxulysse](https://github.com/maxulysse)),\n- Sameesh Kher ([@khersameesh24](https://github.com/khersameesh24))\n- Florian Heyl ([@heylf](https://github.com/heyl))\n- Kre\u0161imir Be\u0161tak ([@kbestak](https://github.com/kbestak))\n- Elad Herz ([@EladH1](https://github.com/EladH1))\n\n## Acknowledgements\n\n<a href=\"https://www.ghga.de/\">\n  <img src=\"docs/images/GHGA_short_Logo_orange.png\" alt=\"GHGA\" width=\"200\"/>\n</a>\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#variantbenchmarking` channel](https://nfcore.slack.com/channels/variantbenchmarking) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/variantbenchmarking for your analysis, please cite it using the following doi: [110.5281/zenodo.14916661](https://doi.org/10.5281/zenodo.14916661)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1307",
        "keep": "To Curate",
        "latest_version": 4,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1307?version=4",
        "name": "nf-core/variantbenchmarking",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "benchmark",
            "draft",
            "structural-variants",
            "variant-calling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-08-02",
        "versions": 4
    },
    {
        "create_time": "2025-07-29",
        "creators": [
            "Tong LI"
        ],
        "description": "PaSTa is a nextflow-based end-to-end image analysis pipeline for decoding image-based spatial transcriptomics data. It performs imaging cycle registration, cell segmentation and transcripts peak decoding. It is currently supports analysis of three types of ST technology:\r\n\r\n- in-situ sequencing-like encoding\r\n- MERFISH-like encoding\r\n- RNAScope-like labelling\r\n\r\nPrerequisites:\r\n1. Nextflow. Installation guide: https://www.nextflow.io/docs/latest/getstarted.html\r\n2. Docker or Singularity. Installation guide: https://docs.docker.com/get-docker/ or https://sylabs.io/guides/3.7/user-guide/quick_start.html\r\n\r\nDemo run with GitPod\r\n1. Clone the repository\r\n```\r\ngit clone https://github.com/cellgeni/Image-ST.git\r\n```\r\n2. Prepare the run.config file *\r\n```\r\nprocess {\r\n        withName: CELLPOSE {\r\n                ext.args = \"--channels [0,0]\"\r\n                storeDir = \"./output/naive_cellpose_segmentation/\"\r\n        }\r\n\r\n        withName: POSTCODE {\r\n                memory = {20.Gb * task.attempt}\r\n                storeDir = \"./output/PoSTcode_decoding_output\"\r\n        }\r\n\r\n        withName: TO_SPATIALDATA {\r\n                memory = {20.Gb * task.attempt}\r\n                ext.args = \"--feature_col 'Name' --expansion_in_pixels 30 --save_label_img False\"\r\n        }\r\n\r\n        withName: MERGE_OUTLINES {\r\n                storeDir = \"./output/merged_cellpose_segmentation/\"\r\n        }\r\n\r\n        withName: BIOINFOTONGLI_MICROALIGNER {\r\n                memory = {50.Gb * task.attempt}\r\n                storeDir = \"./output/registered_stacks\"\r\n        }\r\n\r\n        withName: BIOINFOTONGLI_TILEDSPOTIFLOW {\r\n                memory = {30.Gb * task.attempt}\r\n                storeDir = \"./output/spotiflow_peaks/\"\r\n        }\r\n\r\n        withName: BIOINFOTONGLI_MERGEPEAKS {\r\n                memory = {50.Gb * task.attempt}\r\n                storeDir = \"./output/spotiflow_peaks/\"\r\n        }\r\n\r\n        withName: BIOINFOTONGLI_CONCATENATEWKTS {\r\n                memory = {50.Gb * task.attempt}\r\n                storeDir = \"./output/spotiflow_peaks/\"\r\n        }\r\n\r\n        withName: EXTRACT_PEAK_PROFILE {\r\n                memory = {50.Gb * task.attempt}\r\n                storeDir = \"./output/peak_profiles/\"\r\n        }\r\n}\r\n```\r\n3. Prepare the parameters file (e.g. iss.yaml)\r\n```\r\nimages:\r\n   - ['id': \"test\",\r\n       [\r\n         \"cycle1.ome.tiff\",\r\n         \"cycle2.ome.tiff\",\r\n         \"cycle3.ome.tiff\",\r\n         \"cycle4.ome.tiff\",\r\n         \"cycle5.ome.tiff\",\r\n         \"cycle6.ome.tiff\",\r\n       ]\r\n     ]\r\ncell_diameters: [30]\r\nchs_to_call_peaks: [1,2] // channels to call peaks, can be multiple\r\ncodebook:\r\n  - ['id': \"test\", \"./codebook.csv\", \"./dummy.txt\"] // has to match the meta in `images` variable\r\nsegmentation_method: \"CELLPOSE\" // or DEEPCELL or STARDIST or INSTANSEG\r\n\r\nout_dir: \"./output\"\r\n```\r\n4. Run the pipeline\r\n```\r\nnextflow run ./Image-ST/main.nf -profile lsf,singularity -c run.config -params-file iss.yaml -entry RUN_DECODING -resume\r\n```\r\n5. Check the output in the specified storeDir.\r\n\r\nSpin up Napari with napari-spatialdata plugin installed (https://spatialdata.scverse.org/projects/napari/en/latest/notebooks/spatialdata.html)\r\n\r\nThen use the following command to visualize the output\r\n```\r\nfrom napari_spatialdata import Interactive\r\nimport spatialdata as spd\r\n\r\ndata = spd.read_zarr([path-to-.sdata-folder])\r\nInteractive(data)\r\n```\r\n\r\n*: You may leave the process block empty if you want to use the default parameters.\r\n\r\nFAQ\r\n---\r\n\r\n1. My HOME dir is full when running Singularity image conversion on HPC.\r\n\r\nA quick solution is to manually specify singularity dir by setting:\r\n\r\n\r\n```\r\nsingularity cache clean\r\nexport SINGULARITY_CACHEDIR=./singularity_image_dir\r\nexport NXF_SINGULARITY_CACHEDIR=./singularity_image_dir\r\n```\r\n\r\n2. How do I modify parameters to specific process/step?\r\n\r\nBy following nf-core standard, it is possible to add any parameters to the main script using ext.args=\u201d--[key] [value]\u201d in the run.config file.\r\n\r\nAn example is\r\n\r\nwithName: POSTCODE {\r\n    ext.args = \"--codebook_targer_col L-probe --codebook_code_col code \"\r\n}\r\n\r\n3. Cannot download pretrained model for the deep-learning tools (Spotiflow/CellPose)\r\n\r\n> Exception: URL fetch failure on https://drive.switch.ch/index.php/s/6AoTEgpIAeQMRvX/download: None -- [Errno -3] Temporary failure in name resolution\r\nOr CellPose\r\nurllib.error.URLError: <urlopen error [Errno -3] Temporary failure in name resolution>\r\n\r\nMostly likely you've reached max download (?), wait a bit and try later OR manually download those models and update the configuration file.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1841",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1841?version=1",
        "name": "A nextflow pipeline to run the end-to-end image-based in-situ sequencing decoding and RNAScope-like analysis",
        "number_of_steps": 0,
        "projects": [
            "Euro-BioImaging"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "decoding",
            "image-analysis",
            "spatial",
            "spatial transcriptomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-07-29",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [
            "Marie Joss\u00e9"
        ],
        "description": "Secondary metabolite biosynthetic gene cluster (SMBGC) Annotation using Neural Networks Trained on Interpro Signatures \r\n\r\n## Associated Tutorial\r\n\r\nThis workflows is part of the tutorial [Marine Omics identifying biosynthetic gene clusters](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/marine_omics_bgc/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\r\n\r\n## Features\r\n\r\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\r\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\r\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\r\n\r\n## Thanks to...\r\n\r\n**Workflow Author(s)**: Marie Joss\u00e9\r\n\r\n**Tutorial Author(s)**: [Marie Josse](https://training.galaxyproject.org/training-material/hall-of-fame/Marie59/)\r\n\r\n**Tutorial Contributor(s)**: [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\r\n\r\n**Grants(s)**: [Fair-Ease](https://training.galaxyproject.org/training-material/hall-of-fame/fairease/), [EuroScienceGateway](https://training.galaxyproject.org/training-material/hall-of-fame/eurosciencegateway/)\r\n\r\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": "10.48546/workflowhub.workflow.1663.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metab.* in description",
        "id": "1663",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1663?version=1",
        "name": "Marine Omics identifying biosynthetic gene clusters",
        "number_of_steps": 5,
        "projects": [
            "Galaxy Training Network",
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [
            "earth-system",
            "gtn",
            "galaxy",
            "marineomics",
            "ocean"
        ],
        "tools": [
            "interproscan",
            "regex1",
            "sanntis_marine",
            "prodigal"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-28",
        "versions": 1
    },
    {
        "create_time": "2025-07-22",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-genomeassembler_logo_dark.png\">\n    <img alt=\"nf-core/genomeassembler\" src=\"docs/images/nf-core-genomeassembler_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/genomeassembler/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/genomeassembler/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/genomeassembler/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/genomeassembler/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/genomeassembler/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.14986998-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.14986998)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.10.5-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.2-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.2)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/genomeassembler)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23genomeassembler-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/genomeassembler)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23genomeassembler-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/genomeassembler)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/genomeassembler** is a bioinformatics pipeline that carries out genome assembly, polishing and scaffolding from long reads (ONT or pacbio). Assembly can be done via `flye` or `hifiasm`, polishing can be carried out with `medaka` (ONT), or `pilon` (requires short-reads), and scaffolding can be done using `LINKS`, `Longstitch`, or `RagTag` (if a reference is available). Quality control includes `BUSCO`, `QUAST` and `merqury` (requires short-reads).\nCurrently, this pipeline does not implement phasing of polyploid genomes or HiC scaffolding.\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/genomeassembler.dark.png\">\n  <img alt=\"nf-core/genomeassembler\" src=\"docs/images/genomeassembler.light.png\">\n</picture>\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,ontreads,hifireads,ref_fasta,ref_gff,shortread_F,shortread_R,paired\nsampleName,ontreads.fa.gz,hifireads.fa.gz,assembly.fasta.gz,reference.fasta,reference.gff,short_F1.fastq,short_F2.fastq,true\n```\n\nEach row represents one genome to be assembled. `sample` should contain the name of the sample, `ontreads` should contain a path to ONT reads (fastq.gz), `hifireads` a path to HiFi reads (fastq.gz), `ref_fasta` and `ref_gff` contain reference genome fasta and annotations. `shortread_F` and `shortread_R` contain paths to short-read data, `paired` indicates if short-reads are paired. Columns can be omitted if they contain no data, with the exception of `shortread_R`, which needs to be present if `shortread_F` is there, even if it is empty.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/genomeassembler \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/genomeassembler/usage) and the [parameter documentation](https://nf-co.re/genomeassembler/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/genomeassembler/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/genomeassembler/output).\n\n## Credits\n\nnf-core/genomeassembler was originally written by [Niklas Schandry](https://github.com/nschan), of the Faculty of Biology of the Ludwig-Maximilians University (LMU) in Munich, Germany.\n\nI thank the following people for their extensive assistance and constructive reviews during the development of this pipeline:\n\n- [Mahesh Binzer-Panchal](https://github.com/mahesh-panchal)\n- [Matthias H\u00f6rtenhuber](https://github.com/mashehu)\n- [Louis Le N\u00e9zet](https://github.com/LouisLeNezet)\n- [J\u00falia Mir Pedrol](https://github.com/mirpedrol)\n- [Daniel Straub](https://github.com/d4straub)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#genomeassembler` channel](https://nfcore.slack.com/channels/genomeassembler) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/genomeassembler for your analysis, please cite it using the following doi: [10.5281/zenodo.14986998](https://doi.org/10.5281/zenodo.14986998)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1325",
        "keep": "To Curate",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1325?version=3",
        "name": "nf-core/genomeassembler",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genome-assembly"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-07-22",
        "versions": 3
    },
    {
        "create_time": "2025-07-14",
        "creators": [],
        "description": "Metatranscriptomics analysis using microbiome RNA-seq data (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metatranscriptomics analysis using microbiome RNA-seq data (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metatranscriptomics-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pratik Jagtap, Subina Mehta, Ray Sajulga, Emma Leith, Praveen Kumar, Saskia Hiltemann, Paul Zierep\n\n**Tutorial Author(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/)\n\n**Tutorial Contributor(s)**: [Christine Oger](https://training.galaxyproject.org/training-material/hall-of-fame/ogerdfx/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1447",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1447?version=2",
        "name": "Workflow 3: Functional Information (quick)",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "asaim",
            "gtn",
            "galaxy",
            "metagenomics"
        ],
        "tools": [
            "humann_renorm_table",
            "Cut1",
            "humann_split_stratified_table",
            "humann_unpack_pathways",
            "tp_find_and_replace",
            "humann_regroup_table",
            "humann_rename_table",
            "Grep1",
            "combine_metaphlan2_humann2"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-14",
        "versions": 2
    },
    {
        "create_time": "2025-07-13",
        "creators": [
            "Ignacio Garach"
        ],
        "description": "Code and supporting data for the article: \"Exploring the role of normalization and feature selection in microbiome disease classification pipelines.\"\r\n\r\nThe repository contains the following folders:\r\n\r\n* **1. data:** contains OTU/ASV tables and class annotations for the 15 curated datasets considered.\r\n* **2. src:** code writen to perform the analyses from the article and the statistical tests\r\n* **3. results:** tables containing global nested cross validation results\r\n* **4. figures**\r\n\r\n\r\n## License: This project is licensed under GNU GPL 3.0 - check LICENSE file for more details.\r\n\r\n\r\n| Dataset | Samples (Cases, Controls) | Features | IR  | Project ID |\r\n|---------|---------------------------|----------|-----|------------|\r\n| ART     | 114 (86, 28)               | 10733    | 3.07 | PRJNA203810    |\r\n| CDI     | 336 (93, 243)              | 3456     | 2.61 | 10.1128/mbio.01021-14 (DOI)              |\r\n| CRC1    | 490 (229, 261)             | 6920     | 1.14 | PRJNA290926    |\r\n| CRC2    | 102 (46, 56)               | 837      | 1.22 | SRP005150      |\r\n| HIV     | 350 (293, 57)              | 14425    | 5.14 | PRJNA307231    |\r\n| CD1     | 140 (78, 62)               | 3547     | 1.26 | PRJNA237362    |\r\n| CD2     | 160 (68, 92)               | 3547     | 1.35 | PRJNA237362    |\r\n| IBD1    | 91 (67, 24)                | 2742     | 2.79 | PRJNA82109     |\r\n| IBD2    | 114 (68, 46)               | 1496     | 1.48 | 10.1053/j.gastro.2010.08.049 (DOI)              |\r\n| CIR     | 77 (51, 26)                | 3104     | 1.96 | PRJNA174838    |\r\n| MHE     | 77 (26, 51)                | 3104     | 1.96 | PRJNA174838    |\r\n| OB      | 281 (220, 61)              | 6386     | 3.61 | PRJNA32089     |\r\n| PAR1    | 148 (74, 74)               | 10232    | 1.00 | PRJEB4927      |\r\n| PAR2    | 333 (201, 132)             | 6844     | 1.52 | PRJNA601994    |\r\n| PAR3    | 507 (323, 184)             | 12198    | 1.76 | PRJNA601994    |\r\n\r\n*Notes:*  \r\n- ART: Arthritis; CDI: Clostridium difficile Infection; CRC1 and CRC2: Colorectal Cancer; HIV: Human Immunodeficiency Virus; CD1 and CD2: Crohn's Disease; IBD1 and IBD2: Inflammatory Bowel Disease; CIR: Cirrhosis; MHE: Minimal Hepatic Encephalopathy; OB: Obesity; PAR1, PAR2, and PAR3: Parkinson's Disease.  \r\n- CD1 and CD2 were taken from MLRepo, PAR2 and PAR3 were retrieved from their respective article sources, and the remaining datasets were obtained from MicrobiomeHD.\r\n- Project IDs from NCBI for raw data. IBD2 data is only available via MicrobiomeHD repository, CDI raw data is available at mothur (https://mothur.org/CDI_MicrobiomeModeling/)\r\n",
        "doi": "10.48546/workflowhub.workflow.1807.1",
        "edam_operation": [],
        "edam_topic": [
            "Machine learning",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "1807",
        "keep": "Keep",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1807?version=1",
        "name": "Exploring the role of normalization and feature selection in microbiome disease classification pipelines",
        "number_of_steps": 0,
        "projects": [
            "Machine Learning Techniques in Microbiome"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "machine learning",
            "metagenomics"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2025-07-13",
        "versions": 1
    },
    {
        "create_time": "2025-07-11",
        "creators": [
            "Patrick Austin",
            "Alexander Belozerov",
            "Subindev Devadasan",
            "Leandro Liborio",
            "Abraham Nieva de la Hidalga",
            "Tom Underwood"
        ],
        "description": "Galaxy workflow for the reproduction of the results published in: D. Decarolis, A. H. Clark, T. Pellegrinelli, M. Nachtegaal, E. W. Lynch, C. R. A. .Catlow, E. K. Gibson, A. Goguet, P. P. Wells (2021). Spatial Profiling of a Pd/Al2O3 Catalyst during Selective Ammonia Oxidation DOI: 10.1021/acscatal.0c05356.\r\n\r\nThis workflow is published as part of the research data submitted for the paper Facilitating Reproducibility in Catalysis Research with Managed Workflows and RO-Crates: A Galaxy Case Study, ChemCatChem, DOI: 10.1002/cctc.202401676.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1803",
        "keep": "Reject",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1803?version=1",
        "name": "Pd-Al2O3 Catalyst for Ammonia",
        "number_of_steps": 33,
        "projects": [
            "EuroScienceGateway"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2025-07-11",
        "versions": 1
    },
    {
        "create_time": "2025-07-09",
        "creators": [
            "Huihai Wu",
            "Irene Papatheodorou"
        ],
        "description": "[![GitHub Actions CI Status](https://github.com/nf-core/eisca/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/eisca/actions/workflows/ci.yml)\r\n[![GitHub Actions Linting Status](https://github.com/nf-core/eisca/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/eisca/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/eisca/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\r\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/eisca)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23eisca-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/eisca)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\r\n\r\n## Introduction\r\n\r\n**TGAC/eisca** is a bioinformatics pipeline that perform analysis for single-cell RNA-seq data. The pipeline is built using [Nextflow](https://www.nextflow.io/) and processes (implemented and to be implemented) are as follows:\r\n\r\n- **Primary analysis**\r\n  - FastQC - Raw read QC\r\n  - TrimGalore - Adapter and quality trimming to FastQ files\r\n  - Kallisto & Bustools - Mapping & quantification by Kallisto & Bustools\r\n  - Salmon Alevin - Mapping & quantification by Salmon Alevin\r\n  - STARsolo - Mapping & quantification by STAR\r\n- **Secondary analysis**\r\n  - QC & cell filtering - cell filtering and QC on raw data and filtered data\r\n  - Clustering analysis - single-cell clustering analysis\r\n  - Merging/integration of samples \r\n- **Tertiary analysis**\r\n  - Cell type annotation\r\n  - Differential expression analysis\r\n  - Cell-cell communication analysis\r\n  - Trajectory & pseudotime analysis (to be implemented)\r\n  - Other downstream analyses (to be implemented)\r\n- **Pipeline reporting**\r\n  - Analysis report - Single-ell Analysis Report.\r\n  - MultiQC - Aggregate report describing results and QC for tools registered in nf-core\r\n  - Pipeline information - Report metrics generated during the workflow execution\r\n\r\n\r\n<!-- TODO nf-core:\r\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\r\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\r\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\r\n-->\r\n\r\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\r\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\r\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->\r\n\r\n<!-- 1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\r\n2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/)) -->\r\n\r\n## Usage\r\n\r\n> [!NOTE]\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\r\n\r\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\r\n     Explain what rows and columns represent. For instance (please edit as appropriate):\r\n\r\nFirst, prepare a samplesheet with your input data that looks as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,fastq_1,fastq_2\r\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\r\n```\r\n\r\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\r\n\r\n-->\r\n\r\nFirst, prepare a samplesheet with your input data that looks as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,fastq_1,fastq_2\r\npbmc8k,pbmc8k_S1_L007_R1_001.fastq.gz,pbmc8k_S1_L007_R2_001.fastq.gz\r\npbmc8k,pbmc8k_S1_L008_R1_001.fastq.gz,pbmc8k_S1_L008_R2_001.fastq.gz\r\npbmc5k,pbmc5k_S1_L003_R1_001.fastq.gz,pbmc5k_S1_L003_R2_001.fastq.gz\r\n```\r\n\r\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\r\n\r\n\r\nNow, you can run the pipeline using:\r\n\r\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\r\n\r\n```bash\r\nnextflow run TGAC/eisca \\\r\n   -profile <docker/singularity/.../institute> \\\r\n   --input samplesheet.csv \\\r\n   --genome_fasta GRCm38.p6.genome.chr19.fa \\\r\n   --gtf gencode.vM19.annotation.chr19.gtf \\\r\n   --protocol 10XV2 \\\r\n   --aligner <alevin/kallisto/star/cellranger/universc> \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\n> [!WARNING]\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\nFor more details and further functionality, please refer to the [usage documentation](https://github.com/TGAC/eisca/blob/master/docs/usage.md).\r\n<!-- (https://nf-co.re/eisca/usage). -->\r\n\r\n## Pipeline output\r\n\r\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/eisca/results) tab on the nf-core website pipeline page.\r\nFor more details about the output files and reports, please refer to the\r\n[output documentation](https://github.com/TGAC/eisca/blob/master/docs/output.md).\r\n<!-- (https://nf-co.re/eisca/output). -->\r\n\r\n## Credits\r\n\r\nnf-core/eisca was originally written by Huihai Wu.\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#eisca` channel](https://nfcore.slack.com/channels/eisca) (you can join with [this invite](https://nf-co.re/join/slack)).\r\n\r\n## Citations\r\n\r\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\r\n<!-- If you use nf-core/eisca for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\r\n\r\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Single-cell sequencing"
        ],
        "filtered_on": "profil.* in description",
        "id": "1795",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1795?version=1",
        "name": "EI Single-Cell Analysis pipeline",
        "number_of_steps": 0,
        "projects": [
            "EI Papatheodorou Group"
        ],
        "source": "WorkflowHub",
        "tags": [
            "10x",
            "scrna-seq",
            "single-cell",
            "smart-seq 2"
        ],
        "tools": [
            "kallisto",
            "STAR",
            "Alevin",
            "FastQC",
            "MultiQC",
            "CellChat"
        ],
        "type": "Nextflow",
        "update_time": "2025-07-09",
        "versions": 1
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Pathogens of all samples report generation and visualization\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pathogen detection from (direct Nanopore) sequencing data using Galaxy - Foodborne Edition](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Engy Nasr, B\u00e9r\u00e9nice Batut, Paul Zierep\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Hans-Rudolf Hotz](https://training.galaxyproject.org/training-material/hall-of-fame/hrhotz/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EOSC-Life](https://training.galaxyproject.org/training-material/hall-of-fame/eosc-life/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in description",
        "id": "1487",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1487?version=2",
        "name": "Pathogen Detection PathoGFAIR Samples Aggregation and Visualisation",
        "number_of_steps": 65,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:iwc",
            "name:microgalaxy",
            "name:pathogfair",
            "name:collection"
        ],
        "tools": [
            "bedtools_getfastabed",
            "tp_split_on_column",
            "tab2fasta",
            "Cut1",
            "Grouping1",
            "Remove beginning1",
            "regex1",
            "fasttree",
            "__FILTER_FAILED_DATASETS__",
            "fasta2tab",
            "regexColumn1",
            "Count1",
            "clustalw",
            "collection_column_join",
            "newick_display",
            "fasta_merge_files_and_filter_unique_sequences",
            "__FILTER_EMPTY_DATASETS__",
            "tp_replace_in_column",
            "barchart_gnuplot",
            "collapse_dataset",
            "tp_multijoin_tool",
            "ggplot2_heatmap",
            "tp_sorted_uniq"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 2
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Microbiome - QC and Contamination Filtering\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pathogen detection from (direct Nanopore) sequencing data using Galaxy - Foodborne Edition](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Engy Nasr, Paul Zierep\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Hans-Rudolf Hotz](https://training.galaxyproject.org/training-material/hall-of-fame/hrhotz/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EOSC-Life](https://training.galaxyproject.org/training-material/hall-of-fame/eosc-life/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in description",
        "id": "1492",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1492?version=2",
        "name": "Nanopore Preprocessing",
        "number_of_steps": 25,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:iwc",
            "name:microgalaxy",
            "name:nanopore",
            "name:pathogfair",
            "name:collection"
        ],
        "tools": [
            "nanoplot",
            "fastp",
            "Add_a_column1",
            "regexColumn1",
            "krakentools_extract_kraken_reads",
            "fastqc",
            "minimap2",
            "porechop",
            "Cut1",
            "collection_column_join",
            "samtools_fastx",
            "kraken2",
            "collapse_dataset",
            "__FILTER_FAILED_DATASETS__",
            "bamtools_split_mapped",
            "Grep1",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 2
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "Building an amplicon sequence variant (ASV) table from 16S data using DADA2\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Building an amplicon sequence variant (ASV) table from 16S data using DADA2](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/dada-16S/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: , B\u00e9r\u00e9nice Batut\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Tutorial Contributor(s)**: [Matthias Bernt](https://training.galaxyproject.org/training-material/hall-of-fame/bernt-matthias/), [Clea Siguret](https://training.galaxyproject.org/training-material/hall-of-fame/clsiguret/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Linelle Abueg](https://training.galaxyproject.org/training-material/hall-of-fame/abueg/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Santino Faack](https://training.galaxyproject.org/training-material/hall-of-fame/santamccloud/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1395",
        "keep": "Keep",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1395?version=2",
        "name": "Building an amplicon sequence variant (ASV) table from 16S data using DADA2",
        "number_of_steps": 21,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "dada2_removeBimeraDenovo",
            "__UNZIP_COLLECTION__",
            "__SORTLIST__",
            "tp_replace_in_line",
            "Add_a_column1",
            "dada2_plotQualityProfile",
            "dada2_dada",
            "cat1",
            "phyloseq_from_dada2",
            "dada2_seqCounts",
            "tp_head_tool",
            "tp_replace_in_column",
            "dada2_mergePairs",
            "collection_element_identifiers",
            "dada2_filterAndTrim",
            "dada2_assignTaxonomyAddspecies",
            "dada2_learnErrors",
            "dada2_makeSequenceTable"
        ],
        "type": "Galaxy",
        "update_time": "2025-07-07",
        "versions": 2
    },
    {
        "create_time": "2025-07-07",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-drugresponseeval_logo_dark.png\">\n    <img alt=\"nf-core/drugresponseeval\" src=\"docs/images/nf-core-drugresponseeval_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/drugresponseeval/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/drugresponseeval/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/drugresponseeval/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/drugresponseeval/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/drugresponseeval/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.14779984-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.14779984)\n\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.04.2-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.1)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/drugresponseeval)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23drugresponseeval-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/drugresponseeval)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n# ![drevalpy_summary](assets/dreval_summary.svg)\n\n**DrEval** is a bioinformatics framework that includes a PyPI package (drevalpy) and a Nextflow\npipeline (this repo). DrEval ensures that evaluations are statistically sound, biologically\nmeaningful, and reproducible. DrEval simplifies the implementation of drug response prediction\nmodels, allowing researchers to focus on advancing their modeling innovations by automating\nstandardized evaluation protocols and preprocessing workflows. With DrEval, hyperparameter\ntuning is fair and consistent. With its flexible model interface, DrEval supports any model type,\nranging from statistical models to complex neural networks. By contributing your model to the\nDrEval catalog, you can increase your work's exposure, reusability, and transferability.\n\n1. The response data is loaded\n2. All models are trained and evaluated in a cross-validation setting\n3. For each CV split, the best hyperparameters are determined using a grid search per model\n4. The model is trained on the full training set (train & validation) with the best\n   hyperparameters to predict the test set\n5. If randomization tests are enabled, the model is trained on the full training set with the best\n   hyperparameters to predict the randomized test set\n6. If robustness tests are enabled, the model is trained N times on the full training set with the\n   best hyperparameters\n7. Plots are created summarizing the results\n\nFor baseline models, no randomization or robustness tests are performed.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/drugresponseeval \\\n   -profile <docker/singularity/.../institute> \\\n   --models <RandomForest,model2,...> \\\n   --baselines <NaiveMeanEffectsPredictor,baseline2,...> \\\n   --dataset_name <CTRPv2|CTRPv1|CCLE|GDSC1|GDSC2|custom_dataset>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/drugresponseeval/usage) and the [parameter documentation](https://nf-co.re/drugresponseeval/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/drugresponseeval/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/drugresponseeval/output).\n\n## Credits\n\nnf-core/drugresponseeval was originally written by Judith Bernett (TUM) and Pascal Iversen (FU\nBerlin).\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n## Contributions and Support\n\nContributors to nf-core/drugresponseeval and the drevalpy PyPI package:\n\n- [Judith Bernett](https://github.com/JudithBernett) (TUM)\n- [Pascal Iversen](https://github.com/PascalIversen) (FU Berlin)\n- [Mario Picciani](https://github.com/picciama) (TUM)\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#drugresponseeval` channel](https://nfcore.slack.com/channels/drugresponseeval) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/drugresponseeval for your analysis, please cite it using the following doi: [10.5281/zenodo.14779984](https://doi.org/10.5281/zenodo.14779984)\n\n> Our corresponding publication is at doi [10.1101/2025.05.26.655288](doi.org/10.1101/2025.05.26.655288)\n>\n> Bernett, J., Iversen, P., Picciani, M., Wilhelm, M., Baum, K., & List, M. **From Hype to Health Check: Critical Evaluation of Drug Response Prediction Models with DrEval.**\n>\n> _bioRxiv_, 2025-05.\n\nThe underlying data is available at doi: [10.5281/zenodo.12633909](https://doi.org/10.5281/zenodo.12633909).\n\nThe underlying python package is drevalpy, availably on [PyPI](https://pypi.org/project/drevalpy/) as standalone, for which we also have an extensive [ReadTheDocs Documentation](https://drevalpy.readthedocs.io/en/latest/).\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1266",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1266?version=2",
        "name": "nf-core/drugresponseeval",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cell-lines",
            "cross-validation",
            "deep-learning",
            "drug-response",
            "drug-response-prediction",
            "drugs",
            "fair-principles",
            "generalization",
            "hyperparameter-tuning",
            "machine-learning",
            "randomization-tests",
            "robustness-assessment",
            "training"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-07-07",
        "versions": 2
    },
    {
        "create_time": "2025-07-01",
        "creators": [],
        "description": "# CWL + RO-Crate Workflow Descriptions\r\n\r\nThis repository stores computational workflows described using the **Common Workflow Language (CWL)** and enriched with metadata using **Research Object Crate (RO-Crate)** conforming to the **Workflow Run RO-Crate** profile.\r\n\r\nEach workflow is contained in its own directory (e.g., `WF5201`, `WF6101`, ...). Inside each workflow directory you will typically find at least:\r\n\r\n- The **CWL workflow definition** (with the same name as the directory, e.g., `WF5201.cwl`).\r\n- The **RO-Crate metadata file** (`ro-crate-metadata.json`).\r\n\r\nAdditional files supporting the workflow may also be included.\r\n\r\n## Overview\r\n\r\nThis document explains how to represent workflows by combining:\r\n\r\n- **CWL (Common Workflow Language):** Used to define the computational steps, data flows, and tools.\r\n- **RO-Crate:** Used to capture associated metadata (e.g., authorship, licenses, software, datasets) for the workflow.\r\n\r\nBy separating the abstract workflow definition from its metadata description, you can leverage existing tools for visualization, editing, and validation of your workflows while maintaining a clear structure.\r\n\r\n## Our Approach\r\n\r\nWe represent workflows using a combination of CWL and RO-Crate:\r\n\r\n- **CWL:** Captures the abstract definition of the workflow, detailing its computational steps, data flows, and the tools utilized. It does not include the implementation details of each operation.\r\n- **RO-Crate:** Provides rich metadata for the overall repository, the workflow file(s), software, and datasets. This metadata allows you to understand the context, provenance, and related details of the workflow components.\r\n\r\nThis separation provides flexibility by keeping the execution details (CWL) distinct from descriptive metadata (RO-Crate), yet they remain tightly connected.\r\n\r\n## Describing a Workflow using CWL + RO-Crate\r\n\r\nTo fully describe a workflow, you must separate the **workflow definition** (using CWL) from the **metadata description** (using RO-Crate).\r\n\r\n### Defining the CWL Workflow\r\n\r\n1. **Identify Global Inputs and Outputs:**  \r\n   Decide on the data that enters the workflow (inputs) and the final results (outputs). Optionally, include intermediate outputs if they are of interest.\r\n\r\n2. **Create the CWL File:**  \r\n   Write a CWL file in YAML format. Start with file metadata such as:\r\n\r\n   ```yaml\r\n   cwlVersion: v1.2\r\n   class: Workflow\r\n\r\n   requirements:\r\n     MultipleInputFeatureRequirement: {}\r\n     SubworkflowFeatureRequirement: {}\r\n   ```\r\n\r\n   > [NOTE]\r\n   > The `requirements` section may vary depending on your workflow. For example, if you use sub-workflows, you must include the `SubworkflowFeatureRequirement`.\r\n\r\n3. **Declare Global Inputs and Outputs:**\r\n\r\n   ```yaml\r\n   inputs:\r\n     DT5210: Directory\r\n     DT5211: Directory\r\n\r\n   outputs:\r\n     DT5208:\r\n       type: Directory\r\n       outputSource: SS5213/DT5208\r\n   ```\r\n\r\n   > [NOTE]\r\n   > Although `Directory` is commonly used to represent a dataset, you might choose a different type. Refer to the CWL documentation for additional types.\r\n\r\n### Defining Workflow Steps\r\n\r\nEach workflow step (or subworkflow) follows a consistent structure:\r\n\r\n```yaml\r\nSS5205:\r\n  in:\r\n    DT5210: DT5210\r\n  run:\r\n    class: Operation\r\n    inputs:\r\n      DT5210: Directory\r\n    outputs:\r\n      DT5201: File\r\n      DT5203: Directory\r\n  out:\r\n    - DT5201\r\n    - DT5203\r\n```\r\n\r\nKey elements are:\r\n\r\n- **`in`:** Defines which data this step requires.\r\n- **`run`:**\r\n  - For operations: Uses the `Operation` class to abstract away the underlying execution details.\r\n  - For subworkflows: Points to another CWL file.\r\n- **`out`:** Lists the output data produced by the step.\r\n\r\n### Connecting Steps via Data Dependencies\r\n\r\nCWL does not require an explicit execution order. Instead, dependencies are determined by connecting outputs to inputs:\r\n\r\n```yaml\r\nST520102:\r\n  in:\r\n    DT5201: ST520101/DT5201\r\n  run: ST520102.cwl\r\n  out:\r\n    - DT5255\r\n```\r\n\r\nThis connection means `ST520102` depends on the output (`DT5201`) of `ST520101` and will execute after it, while still allowing independent steps to run in parallel.\r\n\r\n## Validating Your Workflow and Metadata\r\n\r\n- **CWL Validation:**  \r\n  Use [cwltool](https://github.com/common-workflow-language/cwltool) to check your CWL files for syntax errors and to generate a graphical visualization (using Graphviz `dot` format) for verifying the workflow structure.\r\n\r\n- **RO-Crate Validation:**  \r\n  Validate your `ro-crate-metadata.json` file with tools such as the [RO-Crate Validator](https://github.com/crs4/rocrate-validator) and explore your RO-Crate interactively with [ro-crate-html-js](https://github.com/Language-Research-Technology/ro-crate-html-js).\r\n\r\n---\r\n\r\n## Additional Resources\r\n\r\n- [CWL Official Guide and Getting Started](https://www.commonwl.org/getting-started/)\r\n- [CWL User Guide](https://www.commonwl.org/user_guide/)\r\n- [RO-Crate Official Documentation](https://www.researchobject.org/ro-crate/)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1786",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1786?version=1",
        "name": "[DTC-V3] WF5301: Lava flow",
        "number_of_steps": 5,
        "projects": [
            "WP5 - Volcanoes"
        ],
        "source": "WorkflowHub",
        "tags": [
            "lava"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2025-07-01",
        "versions": 1
    },
    {
        "create_time": "2025-06-29",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-pathogensurveillance_logo_dark.png\">\n    <img alt=\"nf-core/pathogensurveillance\" src=\"docs/images/nf-core-pathogensurveillance_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/pathogensurveillance/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/pathogensurveillance/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/pathogensurveillance/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/pathogensurveillance/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/pathogensurveillance/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.04.2-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.1)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/pathogensurveillance)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23pathogensurveillance-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/pathogensurveillance)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/pathogensurveillance** is a population genomics pipeline for pathogen identification, variant detection, and biosurveillance.\nThe pipeline accepts paths to raw reads for one or more organisms and creates reports in the form of an interactive HTML document.\nSignificant features include the ability to analyze unidentified eukaryotic and prokaryotic samples, creation of reports for multiple user-defined groupings of samples, automated discovery and downloading of reference assemblies from [NCBI RefSeq](https://www.ncbi.nlm.nih.gov/refseq/), and rapid initial identification based on k-mer sketches followed by a more robust multi gene phylogeny and SNP-based phylogeny.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner.\nIt uses Docker/Singularity/Conda to make installation trivial and results highly reproducible.\nThe [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies.\nWhere appropriate, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure.\nThis ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world data sets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/pathogensurveillance/results).\n\n## Pipeline summary\n\n![](docs/images/pipeline_diagram.png)\n\n## Quick start guide\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nNote that some form of configuration will be needed so that Nextflow knows how to fetch the required software.\nThis is usually done in the form of a config profile.\nYou can chain multiple config profiles in a comma-separated string.\nIn most cases you will include one profile that defines a tool to reproducibly install and use software needed by the pipeline.\nThis is typically one of `docker`, `singularity`, or `conda`.\nIdeally `conda` should not be used unless `docker` or `singularity` cannot be used.\n\nProfiles can also be used to store parameters for the pipeline, such as input data and pipeline options.\nBefore using you own data, consider trying out a small example dataset included with the pipeline as a profile.\nAvailable test dataset profiles include:\n\n- `test`: Test profile of 1 small genome used to run the pipeline as fast as possible for testing purposes.\n- `test_serratia`: Test profile of 10 serratia isolates from Williams et al. 2022 (https://doi.org/10.1038/s41467-022-32929-2)\n- `test_bordetella`: Test profile of 5 Bordetella pertussis isolates sequenced with with Illumina and Nanopore from Wagner et al. 2023\n- `test_salmonella`: Test profile of 5 salmonella isolates from Hawkey et al. 2024 (https://doi.org/10.1038/s41467-024-54418-4)\n- `test_boxwood_blight`: Test profile of 5 samples of the boxwood blight fungus Cylindrocladium buxicola from LeBlanc et al. 2020 (https://doi.org/10.1094/PHYTO-06-20-0219-FI)\n- `test_mycobacteroides`: Test profile of 5 Mycobacteroides abscessus samples from Bronson et al. 2021 (https://doi.org/10.1038/s41467-021-25484-9)\n- `test_bacteria`: Test profile of 10 mixed bacteria from various sources\n- `test_klebsiella`: Test profile of 10 K. pneumoniae and related species from Holt et al. 2015 (https://doi.org/10.1073/pnas.1501049112)\n- `test_small_genomes`: Test profile consisting of 6 samples from species with small genomes from various sources.\n\nAdding `_full` to the end of any of these profiles will run a larger (often much larger) version of these datasets.\n\nFor example, you can run the `test_bacteria` profile with the `docker` profile:\n\n```bash\nnextflow run nf-core/pathogensurveillance -profile docker,test_bacteria -resume --outdir test_output\n```\n\nYou can see the samplesheets used in these profiles here:\n\nhttps://github.com/nf-core/test-datasets/tree/pathogensurveillance\n\nTo run your own input data, prepare a samplesheet as described in the [usage documentation](docs/usage/#samplesheet-input) section below and run the following command:\n\n```bash\nnextflow run nf-core/pathogensurveillance -profile <REPLACE WITH RUN TOOL> -resume --input <REPLACE WITH TSV/CSV> --outdir <REPLACE WITH OUTPUT PATH>\n```\n\nWhere:\n\n- `<REPLACE WITH RUN TOOL>` is one of `docker`, `singularity`, or `conda`\n- `<REPLACE WITH TSV/CSV>` is the path to the input samplesheet\n- `<REPLACE WITH OUTPUT PATH>` is the path to where to save the output\n\n## Documentation\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/pathogensurveillance/usage) and the [parameter documentation](https://nf-co.re/pathogensurveillance/parameters).\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/pathogensurveillance/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the [output documentation](https://nf-co.re/pathogensurveillance/output).\n\n## Credits\n\nThe following people contributed to the pipeline: Zachary S.L. Foster, Martha Sudermann, Camilo Parada-Rojas, Logan K. Blair, Fernanda I. Bocardo, Ricardo Alcal\u00e1-Brise\u00f1o, Hung Phan, Nicholas C. Cauldron, Alexandra J. Weisberg, Je\ufb00 H. Chang, and Niklaus J. Gr\u00fcnwald.\n\n## Funding\n\nThis work was supported by grants from USDA ARS (2072-22000-045-000-D) to NJG, USDA NIFA (2021-67021-34433; 2023-67013-39918) to JHC and NJG, as well as USDAR ARS NPDRS and FNRI and USDA APHIS to NJG.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#pathogensurveillance` channel](https://nfcore.slack.com/channels/pathogensurveillance) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/pathogensurveillance for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\n<picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/combined_logos_dark.png\">\n    <img alt=\"Logos of University of Oregon and USAD\" src=\"docs/images/combined_logos_light.png\">\n</picture>\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1763",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1763?version=1",
        "name": "nf-core/pathogensurveillance",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-29",
        "versions": 1
    },
    {
        "create_time": "2025-06-29",
        "creators": [
            "Christopher Hakkaart"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-demo_logo_dark.png\">\n    <img alt=\"nf-core/demo\" src=\"docs/images/nf-core-demo_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/demo/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/demo/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/demo/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/demo/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/demo/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.12192442-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.12192442)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.04.2-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.1)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/demo)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23demo-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/demo)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/demo** is a simple nf-core style bioinformatics pipeline for workshops and demonstrations. It was created using the nf-core template and is designed to run quickly using small test data files.\n\n![nf-core/demo metro map](docs/images/nf-core-demo-subway.png)\n\n1. Read QC ([`FASTQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n2. Adapter and quality trimming ([`SEQTK_TRIM`](https://github.com/lh3/seqtk))\n3. Present QC for raw reads ([`MULTIQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nSAMPLE1_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R2.fastq.gz\nSAMPLE2_PE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R2.fastq.gz\nSAMPLE3_SE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample1_R1.fastq.gz,\nSAMPLE3_SE,https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/illumina/amplicon/sample2_R1.fastq.gz,\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/demo \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/demo/usage) and the [parameter documentation](https://nf-co.re/demo/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/demo/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/demo/output).\n\n## Credits\n\nnf-core/demo was originally written by Chris Hakkaart ([@christopher-hakkaart](https://github.com/christopher-hakkaart)).\n\nThe pipeline is currently being maintained by the Nextflow community team as well as [Geraldine Van der Auwera](https://github.com/vdauwera) and [Florian Wuennemann](https://github.com/FloWuenne).\n\n<!-- We thank the following people for their extensive assistance in the development of this pipeline: -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#demo` channel](https://nfcore.slack.com/channels/demo) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/demo for your analysis, please cite it using the following doi: [10.5281/zenodo.12192442](https://doi.org/10.5281/zenodo.12192442)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in description",
        "id": "1055",
        "keep": "To Curate",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1055?version=3",
        "name": "nf-core/demo",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "tutorial",
            "demo",
            "minimal-example",
            "training"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-29",
        "versions": 3
    },
    {
        "create_time": "2025-06-26",
        "creators": [
            "Clinical Genomics Stockholm"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-raredisease_logo_dark.png\">\n    <img alt=\"nf-core/raredisease\" src=\"docs/images/nf-core-raredisease_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/raredisease/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/raredisease/actions/workflows/ci.yml)\n\n[![GitHub Actions Linting Status](https://github.com/nf-core/raredisease/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/raredisease/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/raredisease/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7995798-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7995798)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n[![GitHub Actions Linting Status](https://github.com/nf-core/raredisease/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/raredisease/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/raredisease/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7995798-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7995798)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.1)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/raredisease)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23raredisease-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/raredisease)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n#### TOC\n\n- [Introduction](#introduction)\n- [Pipeline summary](#pipeline-summary)\n- [Usage](#usage)\n- [Pipeline output](#pipeline-output)\n- [Credits](#credits)\n- [Contributions and Support](#contributions-and-support)\n- [Citations](#citations)\n\n## Introduction\n\n**nf-core/raredisease** is a best-practice bioinformatic pipeline for calling and scoring variants from WGS/WES data from rare disease patients. This pipeline is heavily inspired by [MIP](https://github.com/Clinical-Genomics/MIP).\n\n> [!NOTE]\n> Right now, we only support paired-end data from Illumina. If you've got other types of data and the pipeline doesn't work for you, just open an issue. We'd be happy to chat about a solution.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/raredisease/results).\n\n## Pipeline summary\n\n  <picture align=\"center\">\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/raredisease_metromap_dark.png\">\n    <img alt=\"nf-core/raredisease workflow\" src=\"docs/images/raredisease_metromap_light.png\">\n  </picture>\n\n**1. Metrics:**\n\n- [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)\n- [Mosdepth](https://github.com/brentp/mosdepth)\n- [MultiQC](http://multiqc.info/)\n- [Picard's CollectMutipleMetrics, CollectHsMetrics, and CollectWgsMetrics](https://broadinstitute.github.io/picard/)\n- [Qualimap](http://qualimap.conesalab.org/)\n- [Sentieon's WgsMetricsAlgo](https://support.sentieon.com/manual/usages/general/)\n- [TIDDIT's cov](https://github.com/J35P312/)\n- [VerifyBamID2](https://github.com/Griffan/VerifyBamID)\n\n**2. Alignment:**\n\n- [Bwa-mem2](https://github.com/bwa-mem2/bwa-mem2)\n- [BWA-MEME](https://github.com/kaist-ina/BWA-MEME)\n- [BWA](https://github.com/lh3/bwa)\n- [Sentieon DNAseq](https://support.sentieon.com/manual/DNAseq_usage/dnaseq/)\n\n**3. Variant calling - SNV:**\n\n- [DeepVariant](https://github.com/google/deepvariant)\n- [Sentieon DNAscope](https://support.sentieon.com/manual/DNAscope_usage/dnascope/)\n\n**4. Variant calling - SV:**\n\n- [Manta](https://github.com/Illumina/manta)\n- [TIDDIT's sv](https://github.com/SciLifeLab/TIDDIT)\n- Copy number variant calling:\n  - [CNVnator](https://github.com/abyzovlab/CNVnator)\n  - [GATK GermlineCNVCaller](https://github.com/broadinstitute/gatk)\n\n**5. Annotation - SNV:**\n\n- [bcftools roh](https://samtools.github.io/bcftools/bcftools.html#roh)\n- [vcfanno](https://github.com/brentp/vcfanno)\n- [CADD](https://cadd.gs.washington.edu/)\n- [VEP](https://www.ensembl.org/info/docs/tools/vep/index.html)\n- [UPD](https://github.com/bjhall/upd)\n- [Chromograph](https://github.com/Clinical-Genomics/chromograph)\n\n**6. Annotation - SV:**\n\n- [SVDB query](https://github.com/J35P312/SVDB#Query)\n- [VEP](https://www.ensembl.org/info/docs/tools/vep/index.html)\n\n**7. Mitochondrial analysis:**\n\n- [Alignment and variant calling - GATK Mitochondrial short variant discovery pipeline ](https://gatk.broadinstitute.org/hc/en-us/articles/4403870837275-Mitochondrial-short-variant-discovery-SNVs-Indels-)\n- [eKLIPse](https://github.com/dooguypapua/eKLIPse/tree/master)\n- Annotation:\n  - [HaploGrep2](https://github.com/seppinho/haplogrep-cmd)\n  - [Hmtnote](https://github.com/robertopreste/HmtNote)\n  - [vcfanno](https://github.com/brentp/vcfanno)\n  - [CADD](https://cadd.gs.washington.edu/)\n  - [VEP](https://www.ensembl.org/info/docs/tools/vep/index.html)\n\n**8. Variant calling - repeat expansions:**\n\n- [Expansion Hunter](https://github.com/Illumina/ExpansionHunter)\n- [Stranger](https://github.com/Clinical-Genomics/stranger)\n\n**9. Variant calling - mobile elements:**\n\n- [RetroSeq](https://github.com/tk2/RetroSeq)\n\n**10. Rank variants - SV and SNV:**\n\n- [GENMOD](https://github.com/Clinical-Genomics/genmod)\n\n**11. Variant evaluation:**\n\n- [RTG Tools](https://github.com/RealTimeGenomics/rtg-tools)\n\nNote that it is possible to include/exclude certain tools or steps.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,lane,fastq_1,fastq_2,sex,phenotype,paternal_id,maternal_id,case_id\nhugelymodelbat,1,reads_1.fastq.gz,reads_2.fastq.gz,1,2,,,justhusky\n```\n\nEach row represents a pair of fastq files (paired end).\n\nSecond, ensure that you have defined the path to reference files and parameters required for the type of analysis that you want to perform. More information about this can be found [here](https://github.com/nf-core/raredisease/blob/dev/docs/usage.md).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/raredisease \\\n   -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/raredisease/usage) and the [parameter documentation](https://nf-co.re/raredisease/parameters).\n\n## Pipeline output\n\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/raredisease/output).\n\n## Credits\n\nnf-core/raredisease was written in a collaboration between the Clinical Genomics nodes in Sweden, with major contributions from [Ramprasad Neethiraj](https://github.com/ramprasadn), [Anders Jemt](https://github.com/jemten), [Lucia Pena Perez](https://github.com/Lucpen), and [Mei Wu](https://github.com/projectoriented) at Clinical Genomics Stockholm.\n\nAdditional contributors were [Sima Rahimi](https://github.com/sima-r), [Gwenna Breton](https://github.com/Gwennid) and [Emma V\u00e4sterviga](https://github.com/EmmaCAndersson) (Clinical Genomics Gothenburg); [Halfdan Rydbeck](https://github.com/hrydbeck) and [Lauri Mesilaakso](https://github.com/ljmesi) (Clinical Genomics Link\u00f6ping); [Subazini Thankaswamy Kosalai](https://github.com/sysbiocoder) (Clinical Genomics \u00d6rebro); [Annick Renevey](https://github.com/rannick), [Peter Pruisscher](https://github.com/peterpru) and [Eva Caceres](https://github.com/fevac) (Clinical Genomics Stockholm); [Ryan Kennedy](https://github.com/ryanjameskennedy) (Clinical Genomics Lund); [Anders Sune Pedersen](https://github.com/asp8200) (Danish National Genome Center) and [Lucas Taniguti](https://github.com/lmtani).\n\nWe thank the nf-core community for their extensive assistance in the development of this pipeline.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#raredisease` channel](https://nfcore.slack.com/channels/raredisease) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/raredisease for your analysis, please cite it using the following doi: [10.5281/zenodo.7995798](https://doi.org/10.5281/zenodo.7995798)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\nYou can read more about MIP's use in healthcare in,\n\n> Stranneheim H, Lagerstedt-Robinson K, Magnusson M, et al. Integration of whole genome sequencing into a healthcare setting: high diagnostic rates across multiple clinical entities in 3219 rare disease patients. Genome Med. 2021;13(1):40. doi:10.1186/s13073-021-00855-5\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1014",
        "keep": "To Curate",
        "latest_version": 11,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1014?version=11",
        "name": "nf-core/raredisease",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "wgs",
            "diagnostics",
            "rare-disease",
            "snv",
            "structural-variants",
            "variant-annotation",
            "variant-calling",
            "wes"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-26",
        "versions": 11
    },
    {
        "create_time": "2025-06-25",
        "creators": [
            "Joon-Klaps None",
            "Joon-Klaps None"
        ],
        "description": "A bioinformatics best-practice analysis pipeline for reconstructing consensus genomes and to identify intra-host variants from metagenomic sequencing data or enriched based sequencing data like hybrid capture.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1757",
        "keep": "Keep",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1757?version=2",
        "name": "Joon-Klaps/viralgenie",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "fastq",
            "ngs",
            "virology",
            "epidemiology",
            "viral-metagenomics",
            "virus-genomes"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-25",
        "versions": 2
    },
    {
        "create_time": "2025-06-25",
        "creators": [
            "Pixelgen Technologies AB"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-pixelator_logo_dark.png\">\n    <img alt=\"nf-core/pixelator\" src=\"docs/images/nf-core-pixelator_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/pixelator/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/pixelator/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/pixelator/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/pixelator/actions/workflows/linting.yml)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/pixelator/results)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/pixelator)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23pixelator-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/pixelator)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/pixelator** is a bioinformatics best-practice analysis pipeline for analysis of data from the\nMolecular Pixelation (MPX) and Proximity Network (PNA) assays. It takes a samplesheet as input and will process your data\nusing `pixelator` to produce a PXL file containing single-cell protein abundance and protein interactomics data.\n\n![](./docs/images/nf-core-pixelator-metromap.svg)\n\nDepending on the input data the pipeline will run different steps.\n\nFor PNA data, the pipeline will run the following steps:\n\n1. Do quality control checks of input reads and build amplicons ([`pixelator single-cell-pna amplicon`](https://github.com/PixelgenTechnologies/pixelator))\n2. Create groups of amplicons based on their marker assignments ([`pixelator single-cell-pna demux`](https://github.com/PixelgenTechnologies/pixelator))\n3. Derive original molecules to use as edge list downstream by error correcting, and counting input amplicons ([`pixelator single-cell-pna collapse`](https://github.com/PixelgenTechnologies/pixelator))\n4. Compute the components of the graph from the edge list in order to create putative cells ([`pixelator single-cell-pna graph`](https://github.com/PixelgenTechnologies/pixelator))\n5. Analyze the spatial information in the cell graphs ([`pixelator single-cell-pna analysis`](https://github.com/PixelgenTechnologies/pixelator))\n6. Generate 3D graph layouts for visualization of cells ([`pixelator single-cell-pna layout`](https://github.com/PixelgenTechnologies/pixelator))\n7. Report generation ([`pixelator single-cell-pna report`](https://github.com/PixelgenTechnologies/pixelator))\n\nFor MPX data, the pipeline will run the following steps:\n\n1. Build an amplicons from the input reads ([`pixelator single-cell-mpx amplicon`](https://github.com/PixelgenTechnologies/pixelator))\n2. Read QC and filtering, correctness of the pixel binding sequence sequences ([`pixelator single-cell-mpx preqc | pixelator adapterqc`](https://github.com/PixelgenTechnologies/pixelator))\n3. Assign a marker (barcode) to each read ([`pixelator single-cell-mpx demux`](https://github.com/PixelgenTechnologies/pixelator))\n4. Error correction, duplicate removal, compute read counts ([`pixelator single-cell-mpx collapse`](https://github.com/PixelgenTechnologies/pixelator))\n5. Compute the components of the graph from the edge list in order to create putative cells ([`pixelator single-cell-mpx graph`](https://github.com/PixelgenTechnologies/pixelator))\n6. Call and annotate cells ([`pixelator single-cell-mpx annotate`](https://github.com/PixelgenTechnologies/pixelator))\n7. Analyze the cells for polarization and colocalization ([`pixelator single-cell-mpx analysis`](https://github.com/PixelgenTechnologies/pixelator))\n8. Generate 3D graph layouts for visualization of cells ([`pixelator single-cell-mpx layout`](https://github.com/PixelgenTechnologies/pixelator))\n9. Report generation ([`pixelator single-cell-mpx report`](https://github.com/PixelgenTechnologies/pixelator))\n\n> [!WARNING]\n> Since Nextflow 23.07.0-edge, Nextflow no longer mounts the host's home directory when using Apptainer or Singularity.\n> This causes issues in some dependencies. As a workaround, you can revert to the old behavior by setting the environment variable\n> `NXF_APPTAINER_HOME_MOUNT` or `NXF_SINGULARITY_HOME_MOUNT` to `true` in the machine from which you launch the pipeline.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows (the exact values you need to input depend on the design and panel you are using):\n\n`samplesheet.csv`:\n\n```csv\nsample,design,panel,fastq_1,fastq_2\nsample1,pna-2,proxiome-immuno-155,sample1_R1_001.fastq.gz,sample1_R2_001.fastq.gz\n```\n\nEach row represents a sample and gives the design, a panel file and the input fastq files.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/pixelator \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> This version of the pipeline does not support conda environments, due to issues with upstream dependencies.\n> This means you cannot use the `conda` and `mamba` profiles. Please use `docker` or `singularity` instead.\n> We hope to add support for conda environments in the future.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/pixelator/usage) and the [parameter documentation](https://nf-co.re/pixelator/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/pixelator/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/pixelator/output).\n\n## Credits\n\nnf-core/pixelator was originally written for [Pixelgen Technologies AB](https://www.pixelgen.com/) by:\n\n- Florian De Temmerman\n- Johan Dahlberg\n- Alvaro Martinez Barrio\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#pixelator` channel](https://nfcore.slack.com/channels/pixelator) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/pixelator for your analysis, please cite it using the following doi: [10.5281/zenodo.10015112](https://doi.org/10.5281/zenodo.10015112)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\nYou can cite the molecular pixelation technology as follows:\n\n> **Molecular pixelation: spatial proteomics of single cells by sequencing.**\n>\n> Filip Karlsson, Tomasz Kallas, Divya Thiagarajan, Max Karlsson, Maud Schweitzer, Jose Fernandez Navarro, Louise Leijonancker, Sylvain Geny, Erik Pettersson, Jan Rhomberg-Kauert, Ludvig Larsson, Hanna van Ooijen, Stefan Petkov, Marcela Gonz\u00e1lez-Granillo, Jessica Bunz, Johan Dahlberg, Michele Simonetti, Prajakta Sathe, Petter Brodin, Alvaro Martinez Barrio & Simon Fredriksson\n>\n> _Nat Methods._ 2024 May 08. doi: [10.1038/s41592-024-02268-9](https://doi.org/10.1038/s41592-024-02268-9)\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in description",
        "id": "1010",
        "keep": "To Curate",
        "latest_version": 10,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1010?version=10",
        "name": "nf-core/pixelator",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "molecular-pixelation",
            "pixelator",
            "pixelgen-technologies",
            "proteins",
            "single-cell",
            "single-cell-omics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-25",
        "versions": 10
    },
    {
        "create_time": "2024-09-18",
        "creators": [
            "Diego De Panis"
        ],
        "description": "The workflow takes a (trimmed) Long reads collection, runs Meryl to create a K-mer database, Genomescope2 to estimate genome properties and Smudgeplot to estimate ploidy (optional). The main results are K-mer database and genome profiling plots, tables, and values useful for downstream analysis. Default K-mer length and ploidy for Genomescope are 31 and 2, respectively. ",
        "doi": null,
        "edam_operation": [
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Whole genome sequencing"
        ],
        "filtered_on": "profil.* in tags",
        "id": "603",
        "keep": "Reject",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/603?version=2",
        "name": "ERGA Profiling Long Reads v2505 (WF1)",
        "number_of_steps": 17,
        "projects": [
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "erga",
            "profiling"
        ],
        "tools": [
            "Add_a_column1",
            "tp_grep_tool",
            "genomescope",
            "Convert characters1",
            "smudgeplot",
            "Cut1",
            "tp_cut_tool",
            "tp_find_and_replace",
            "param_value_from_file",
            "meryl"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-24",
        "versions": 2
    },
    {
        "create_time": "2025-06-23",
        "creators": [],
        "description": "Metatranscriptomics analysis using microbiome RNA-seq data\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metatranscriptomics analysis using microbiome RNA-seq data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metatranscriptomics/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pratik Jagtap, Subina Mehta, Saskia Hiltemann, Paul Zierep\n\n**Tutorial Author(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Tutorial Contributor(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Christine Oger](https://training.galaxyproject.org/training-material/hall-of-fame/ogerdfx/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1466",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1466?version=2",
        "name": "Metatranscriptomics analysis using microbiome RNA-seq data",
        "number_of_steps": 33,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "metaphlan",
            "Cut1",
            "humann_split_stratified_table",
            "export2graphlan",
            "Grouping1",
            "humann_regroup_table",
            "graphlan_annotate",
            "graphlan",
            "taxonomy_krona_chart",
            "tp_sort_header_tool",
            "multiqc",
            "combine_metaphlan_humann",
            "fastqc",
            "tp_find_and_replace",
            "humann",
            "bg_sortmerna",
            "humann_renorm_table",
            "cutadapt",
            "fastq_paired_end_interlacer",
            "humann_unpack_pathways",
            "humann_rename_table",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-23",
        "versions": 2
    },
    {
        "create_time": "2025-06-23",
        "creators": [],
        "description": "Metatranscriptomics analysis using microbiome RNA-seq data (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metatranscriptomics analysis using microbiome RNA-seq data (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metatranscriptomics-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pratik Jagtap, Subina Mehta, Saskia Hiltemann, Paul Zierep\n\n**Tutorial Author(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/)\n\n**Tutorial Contributor(s)**: [Christine Oger](https://training.galaxyproject.org/training-material/hall-of-fame/ogerdfx/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1451",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1451?version=2",
        "name": "Metatranscriptomics analysis using microbiome RNA-seq data - Workflow 2: Community profile",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "metaphlan",
            "Cut1",
            "export2graphlan",
            "taxonomy_krona_chart",
            "graphlan_annotate",
            "graphlan"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-23",
        "versions": 2
    },
    {
        "create_time": "2025-06-23",
        "creators": [],
        "description": "Metatranscriptomics analysis using microbiome RNA-seq data (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metatranscriptomics analysis using microbiome RNA-seq data (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metatranscriptomics-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pratik Jagtap, Subina Mehta, Saskia Hiltemann, Paul Zierep\n\n**Tutorial Author(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/)\n\n**Tutorial Contributor(s)**: [Christine Oger](https://training.galaxyproject.org/training-material/hall-of-fame/ogerdfx/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1444",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1444?version=2",
        "name": "Metatranscriptomics analysis using microbiome RNA-seq data - Workflow 1: Preprocessing",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "cutadapt",
            "fastqc",
            "fastq_paired_end_interlacer",
            "multiqc",
            "bg_sortmerna"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-23",
        "versions": 2
    },
    {
        "create_time": "2025-06-23",
        "creators": [],
        "description": "Metatranscriptomics analysis using microbiome RNA-seq data (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metatranscriptomics analysis using microbiome RNA-seq data (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metatranscriptomics-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Pratik Jagtap, Subina Mehta, Saskia Hiltemann, Paul Zierep\n\n**Tutorial Author(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Ray Sajulga](https://training.galaxyproject.org/training-material/hall-of-fame/jraysajulga/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Praveen Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/pravs3683/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/)\n\n**Tutorial Contributor(s)**: [Christine Oger](https://training.galaxyproject.org/training-material/hall-of-fame/ogerdfx/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1456",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1456?version=2",
        "name": "Metatranscriptomics analysis using microbiome RNA-seq data - Workflow 3: Functional Information",
        "number_of_steps": 18,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "humann_renorm_table",
            "humann_split_stratified_table",
            "humann_unpack_pathways",
            "tp_find_and_replace",
            "humann",
            "humann_regroup_table",
            "humann_rename_table",
            "Grep1",
            "tp_sort_header_tool"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-23",
        "versions": 2
    },
    {
        "create_time": "2025-06-20",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-createtaxdb_logo_dark_tax.png\">\n    <img alt=\"nf-core/createtaxdb\" src=\"docs/images/nf-core-createtaxdb_logo_light_tax.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/createtaxdb/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/createtaxdb/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/createtaxdb/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/createtaxdb/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/createtaxdb/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/version-%E2%89%A524.04.2-green?style=flat&logo=nextflow&logoColor=white&color=%230DC09D&link=https%3A%2F%2Fnextflow.io)](https://www.nextflow.io/)\n[![nf-core template version](https://img.shields.io/badge/nf--core_template-3.3.1-green?style=flat&logo=nfcore&logoColor=white&color=%2324B064&link=https%3A%2F%2Fnf-co.re)](https://github.com/nf-core/tools/releases/tag/3.3.1)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/createtaxdb)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23createtaxdb-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/createtaxdb)[![Follow on Bluesky](https://img.shields.io/badge/bluesky-%40nf__core-1185fe?labelColor=000000&logo=bluesky)](https://bsky.app/profile/nf-co.re)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/createtaxdb** is a bioinformatics pipeline that constructs custom metagenomic classifier databases for multiple classifiers and profilers from the same input reference genome set in a highly automated and parallelised manner.\nIt supports both nucleotide and protein based classifiers and profilers.\nThe pipeline is designed to be a companion pipeline to [nf-core/taxprofiler](https://nf-co.re/taxprofiler/) for taxonomic profiling of metagenomic data, but can be used for any context.\n\n<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"assets/createtaxdb-metromap-diagram-dark.png\">\n    <img alt=\"nf-core/createtaxdb\" src=\"assets/createtaxdb-metromap-diagram-light.png\">\n  </picture>\n</h1>\n\n1. Prepares input FASTA files for building\n2. Builds databases for:\n   - [Bracken](https://doi.org/10.7717/peerj-cs.104)\n   - [Centrifuge](https://doi.org/10.1101/gr.210641.116)\n   - [DIAMOND](https://doi.org/10.1038/nmeth.3176)\n   - [ganon](https://doi.org/10.1093/bioinformatics/btaa458)\n   - [Kaiju](https://doi.org/10.1038/ncomms11257)\n   - [KMCP](https://doi.org/10.1093/bioinformatics/btac845)\n   - [Kraken2](https://doi.org/10.1186/s13059-019-1891-0)\n   - [KrakenUniq](https://doi.org/10.1186/s13059-018-1568-0)\n   - [MALT](https://doi.org/10.1038/s41559-017-0446-6)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare an input CSV table with your input reference genomes that looks as follows:\n\n```csv\nid,taxid,fasta_dna,fasta_aa\nHuman_Mitochondrial_genome,9606,chrMT.fna,\nSARS-CoV-2_genome,694009,GCA_011545545.1_ASM1154554v1_genomic.fna.gz,GCA_011545545.1_ASM1154554v1_genomic.faa.gz\nBacteroides_fragilis_genome,817,GCF_016889925.1_ASM1688992v1_genomic.fna.gz,GCF_016889925.1_ASM1688992v1_genomic.faa.gz\nCandidatus_portiera_aleyrodidarum_genome,91844,GCF_000292685.1_ASM29268v1_genomic.fna,GCF_000292685.1_ASM29268v1_genomic.faa\nHaemophilus_influenzae_genome,727,GCF_900478275.1_34211_D02_genomic.fna,GCF_900478275.1_34211_D02_genomic.faa\nStreptococcus_agalactiae_genome,1311,,GCF_002881355.1_ASM288135v1_genomic.faa\n```\n\nEach row contains a human readable name, the taxonomic ID of the organism, and then an (optionally gzipped) Nucleotide and/or Amino Acid FASTA file.\n\nNow, with an appropriate set of taxonomy files you can build databases for multiple profilers - such as Kraken2, ganon, and DIAMOND - in parallel:\n\n```bash\nnextflow run nf-core/createtaxdb \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --accession2taxid /<path>/<to>/taxonomy/nucl_gb.accession2taxid \\\n   --nucl2taxid /<path>/<to>/taxonomy/nucl.accession2taxid.gz \\\n   --prot2taxid /<path>/<to>/taxonomy/prot.accession2taxid.gz \\\n   --nodesdmp /<path>/<to>/taxonomy/nodes.dmp \\\n   --namesdmp /<path>/<to>/taxonomy/names.dmp \\\n   --build_kraken2 \\\n   --kraken2_build_options='--kmer-len 45' \\\n   --build_ganon \\\n   --ganon_build_options='--kmer-size 45' \\\n   --build_diamond \\\n   --diamond_build_options='--no-parse-seqids' \\\n   --outdir <OUTDIR>\n```\n\nThe output directory will contain directories containing the database files for each of the profilers you selected to build.\nOptionally you can also package these as `tar.gz` archives.\n\nYou can also generate pre-prepared input sheets for database specifications of pipelines such as [nf-core/taxprofiler](https://nf-co.re/taxprofiler) using `--generate_downstream_samplesheets`.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/createtaxdb/usage) and the [parameter documentation](https://nf-co.re/createtaxdb/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/createtaxdb/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/createtaxdb/output).\n\n## Credits\n\nnf-core/createtaxdb was originally written by James A. Fellows Yates, Sam Wilkinson, Alexander Ramos D\u00edaz, Lili Andersson-Li and the nf-core community.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- Zandra Fagern\u00e4s for logo design\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#createtaxdb` channel](https://nfcore.slack.com/channels/createtaxdb) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/createtaxdb for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1742",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1742?version=1",
        "name": "nf-core/createtaxdb",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "profiling",
            "database",
            "database-builder",
            "metagenomic-profiling",
            "taxonomic-profiling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-20",
        "versions": 1
    },
    {
        "create_time": "2025-06-17",
        "creators": [
            "Alejandra Escobar"
        ],
        "description": "# Introduction\r\n\r\n**ebi-metagenomics/biosiftr** is a bioinformatics pipeline that generates taxonomic and functional profiles for low-yield (shallow shotgun: < 10 M reads) short raw-reads using [`MGnify biome-specific genome catalogues`](https://www.ebi.ac.uk/metagenomics/browse/genomes) as a reference.\r\n\r\nThe biome selection includes all the biomes available in the [`MGnify genome catalogues`](https://www.ebi.ac.uk/metagenomics/browse/genomes).\r\n\r\nThe main sections of the pipeline include the following steps:\r\n\r\n1. Raw-reads quality control ([`fastp`](https://github.com/OpenGene/fastp))\r\n2. HQ reads decontamination versus human, phyX, and host ([`bwa-mem2`](https://github.com/bwa-mem2/bwa-mem2))\r\n3. QC report of decontaminated reads ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\r\n4. Integrated quality report of reads before and after decontamination ([`MultiQC`](http://multiqc.info/))\r\n5. Mapping HQ clean reads using [`Sourmash`](https://github.com/sourmash-bio/sourmash) and bwa-mem2 (optional)\r\n6. Taxonomic profile generation\r\n7. Functional profile inference\r\n\r\nThe final output includes a species relative abundance table, Pfam and KEGG Orthologs (KO) count tables, a KEGG modules completeness table, and DRAM-style visuals (optional). In addition, the shallow-mapping pipeline will integrate the taxonomic and functional tables of all the samples in the input samplesheet.\r\n\r\n## Installation\r\n\r\nThis workflow was built using [Nextflow](https://www.nextflow.io/) and follows [nf-core](https://nf-co.re/) good practices. It is containerised, so users can use either Docker or Apptainer/Singularity to run the pipeline. At the moment, it doesn't support Conda environments.\r\n\r\nThe pipeline requires [Nextflow](https://www.nextflow.io/docs/latest/getstarted.html#installation) and a container technology such as [Apptainer/Singularity](https://github.com/apptainer/singularity/blob/master/INSTALL.md) or [Docker](https://www.docker.com/).\r\n\r\n### Required Reference Databases\r\n\r\nThe first time you run the pipeline, it will download the required MGnify genomes catalogue reference files and the human_phiX BWAMEM2 index. If you select a different host for decontamination, you must provide the index yourself.\r\n\r\nRunning the pipeline using bwamem2 is optional. If you want to run the pipeline with this option set the `--download_bwa true`. This database will occupy considerable storage in your system depending on the biome.\r\n\r\nIn addition, instructions to generate the databases from custom catalogues can be found in the [BioSIFTR paper's repository](https://github.com/EBI-Metagenomics/biosiftr_extended_methods?tab=readme-ov-file#31-processing-custom-genome-catalogues).\r\n\r\n### Usage\r\n\r\nPrepare a samplesheet with your input data that looks as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,fastq_1,fastq_2\r\npaired_sample,/PATH/test_R1.fq.gz,/PATH/test_R2.fq.gz\r\nsingle_sample,/PATH/test.fq.gz\r\n```\r\n\r\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end) where 'sample' is a unique identifier for each dataset, 'fastq_1' is the path to the first FASTQ file, and 'fastq_2' is the path to the second FASTQ file for paired-end data.\r\n\r\nNow, you can run the pipeline using the minimum of arguments:\r\n\r\n```bash\r\nnextflow run ebi-metagenomics/biosiftr \\\r\n   --biome <CATALOGUE_ID> \\\r\n   --input samplesheet.csv \\\r\n   --outdir <PROJECT_NAME> default = `results` \\\r\n   --dbs </path/to/dbs> \\\r\n   --decontamination_indexes </path to folder with bwamem2 indexes>\r\n```\r\n\r\nThe central location for the databases can be set in the config file.\r\n\r\nOptional arguments include:\r\n\r\n```bash\r\n--run_bwa <boolean> default = `false`   # To generate results using bwamem2 besides sourmash\r\n--core_mode <boolean> default = `false` # To use core functions instead of pangenome functions\r\n--run_dram <boolean> default = `false`  # To generate DRAM results\r\n```\r\n\r\nUse `--core_mode true` for large catalogues like the human-gut to avoid over-prediction due to a large number of accessory genes in the pangenome.\r\nNextflow option `-profile` can be used to select a suitable config for your computational resources. You can add profile files to the `config` directory.\r\nNextflow option `-resume` can be used to re-run the pipeline from the last successfully finished step.\r\n\r\n\r\n#### Available biomes\r\n\r\nThis can be any of the MGnify catalogues for which shallow-mapping databases are currently available\r\n\r\n| Biome              | Catalogue Version                                                                    |\r\n| ------------------ | ------------------------------------------------------------------------------------ |\r\n| chicken-gut        | [v1.0.1](https://www.ebi.ac.uk/metagenomics/genome-catalogues/chicken-gut-v1-0-1)    |\r\n| cow-rumen          | [v1.0.1](https://www.ebi.ac.uk/metagenomics/genome-catalogues/cow-rumen-v1-0-1)      |\r\n| human-gut          | [v2.0.2 \u26a0\ufe0f](https://www.ebi.ac.uk/metagenomics/genome-catalogues/human-gut-v2-0-2)   |\r\n| human-oral         | [v1.0.1](https://www.ebi.ac.uk/metagenomics/genome-catalogues/human-oral-v1-0-1)     |\r\n| human-vaginal      | [v1.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/human-vaginal-v1-0)      |\r\n| honeybee-gut       | [v1.0.1](https://www.ebi.ac.uk/metagenomics/genome-catalogues/honeybee-gut-v1-0-1)   |\r\n| marine             | [v2.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/marine-v2-0)             |\r\n| mouse-gut          | [v1.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/mouse-gut-v1-0)          |\r\n| non-model-fish-gut | [v2.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/non-model-fish-gut-v2-0) |\r\n| pig-gut            | [v1.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/pig-gut-v1-0)            |\r\n| sheep-rumen        | [v1.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/sheep-rumen-v1-0)        |\r\n| zebrafish-fecal    | [v1.0](https://www.ebi.ac.uk/metagenomics/genome-catalogues/zebrafish-fecal-v1-0)    |\r\n\r\n> **\u26a0\ufe0f Note for human-gut**:\r\n>\r\n> The human-gut shallow-mapping database was created manually by re-running Panaroo to reconstruct the pangenomes. This is likely to have caused discrepancies in the pangenomes, so please bear that in mind.\r\n\r\n## Test\r\n\r\nTo test the installed tool with your downloaded databases, you can run the pipeline using the small test dataset. Even if there are no hits with the biome you are interested in, the pipeline should finish successfully. Add `-profile` if you have set up a config profile for your compute resources.\r\n\r\n```bash\r\ncd biosiftr/tests\r\nnextflow run ../main.nf \\\r\n    --input test_samplesheet.csv \\\r\n    --biome <CATALOGUE_ID> \\\r\n    --dbs </path/to/dbs> \\\r\n    --decontamination_indexes </path to folder with bwamem2 indexes>\r\n```\r\n\r\n## Credits\r\n\r\nebi-metagenomics/biosiftr pipeline was originally written by @Ales-ibt.\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n@mberacochea\r\n",
        "doi": "10.48546/workflowhub.workflow.1735.1",
        "edam_operation": [],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "1735",
        "keep": "Keep",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1735?version=1",
        "name": "BioSIFTR",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-17",
        "versions": 1
    },
    {
        "create_time": "2025-06-17",
        "creators": [
            "Alejandra Escobar",
            "Martin Beracochea"
        ],
        "description": "# Mobilome Annotation Pipeline (former MoMofy)\r\n\r\nBacteria can acquire genetic material through horizontal gene transfer, allowing them to rapidly adapt to changing environmental conditions. These mobile genetic elements can be classified into three main categories: plasmids, phages, and integrative elements. Plasmids are mostly extrachromosmal; phages can be found extrachromosmal or as temperate phages (prophages); whereas integrons are stable inserted in the chromosome. Autonomous elements are those integrative elements capable of excising themselves from the chromosome and reintegrate elsewhere. They can use a transposase (like insertion sequences and transposons) or an integrase/excisionase (like ICEs and IMEs).\r\n\r\nThe Mobilome Annotation Pipeline is a wrapper that integrates the output of different tools designed for the prediction of plasmids, phages, insertion sequences, and other autonomous integrative mobile genetic elements such as ICEs, IMEs and integrons in prokaryotic genomes and metagenomes. The output is a PROKKA gff file with extra entries for the mobilome.\r\n\r\n## Contents\r\n\r\n- [ Workflow ](#wf)\r\n- [ Setup ](#sp)\r\n- [ Install and dependencies ](#install)\r\n- [ Usage ](#usage)\r\n- [ Inputs ](#in)\r\n- [ Outputs ](#out)\r\n- [ Tests ](#test)\r\n- [ Citation ](#cite)\r\n\r\n<a name=\"wf\"></a>\r\n\r\n## Workflow\r\n\r\nThis workflow has the following main subworkflows:\r\n\r\n- Preprocessing: Rename and filter contigs, and run PROKKA annotation\r\n- Prediction: Run geNomad, ICEfinder, IntegronFinder, and ISEScan\r\n- Annotation: Generate extra-annotation for antimicrobial resistance genes (AMRFinderPlus) and other mobilome-related proteins (MobileOG).\r\n- Integration: Parse and integrate the outputs generated on `Prediction` and `Annotation` subworkflows. In this step optional results of VIRify v3.0.0 can be incorporated. MGEs <500 bp lengh and predictions with no genes are discarded.\r\n- Postprocessing: Write the mobilome fasta file, write a report of the location of AMR genes (either mobilome or chromosome), and generate three new GFF files:\r\n\r\n1. `mobilome_clean.gff`: mobilome + associated CDSs\r\n2. `mobilome_extra.gff`: mobilome + ViPhOGs/mobileOG annotated genes (note that ViPhOG annotation is generated by VIRify)\r\n3. `mobilome_nogenes.gff`: mobilome only\r\n   The output `mobilome_nogenes.gff` is validated in this subworkflow.\r\n\r\n<a name=\"sp\"></a>\r\n\r\n## Setup\r\n\r\nThis workflow is built using [Nextflow](https://www.nextflow.io/). It uses Singularity containers making installation trivial and results highly reproducible.\r\nExplained in this section, there is one manual step required to build the singularity image for [ICEfinder](https://bioinfo-mml.sjtu.edu.cn/ICEfinder/index.php), as we can't distribute that software due to license issues.\r\n\r\n- Install [Nextflow version >=21.10](https://www.nextflow.io/docs/latest/getstarted.html#installation)\r\n- Install [Singularity](https://github.com/apptainer/singularity/blob/master/INSTALL.md)\r\n\r\n<a name=\"install\"></a>\r\n\r\n## Install and dependencies\r\n\r\nTo get a copy of the Mobilome Annotation Pipeline, clone this repo by:\r\n\r\n```bash\r\n$ git clone https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline.git\r\n```\r\n\r\nThe mobileOG-database is required to run an extra step of annotation on the mobilome coding sequences. The first time you run the Mobilome Annotation Pipeline, you will need to download the [Beatrix 1.6 v1](https://mobileogdb.flsi.cloud.vt.edu/entries/database_download) database, move the tarball to `mobilome-annotation-pipeline/databases`, decompress it, and run the script to format the db for diamond:\r\n\r\n```bash\r\n$ mv beatrix-1-6_v1_all.zip /PATH/mobilome-annotation-pipeline/databases\r\n$ cd /PATH/mobilome-annotation-pipeline/databases\r\n$ unzip beatrix-1-6_v1_all.zip\r\n$ nextflow run /PATH/mobilome-annotation-pipeline/format_mobileOG.nf\r\n```\r\n\r\nTwo additional databases need to be manually downloaded and extracted: [AMRFinder plus db](https://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/AMRFinderPlus/database/latest) and the [geNomad database](https://zenodo.org/records/8339387) databases. Then you can provide the paths to your databases using the `mobileog_db`, the `amrfinder_plus_db` and the `genomad_db` respectively when you run the pipeline.\r\n\r\nMost of the tools are available on [quay.io](https://quay.io) and no install is needed. However, in the case of ICEfinder, you will need to contact the author to get a copy of the software, visit the [ICEfinder website](https://bioinfo-mml.sjtu.edu.cn/ICEfinder/download.html) for more information. Once you have the `ICEfinder_linux.tar.gz` tarball, move it to `mobilome-annotation-pipeline/templates` and build the singularity image using the following command:\r\n\r\n```bash\r\n$ mv ICEfinder_linux.tar.gz /PATH/mobilome-annotation-pipeline/templates/\r\n$ cd /PATH/mobilome-annotation-pipeline/templates/\r\n$ sudo singularity build ../../singularity/icefinder-v1.0-local.sif icefinder-v1.0-local.def\r\n```\r\n\r\nThe path to the ICEfinder image needs to be provided when running the pipeline, unless a custom config file is created.\r\n\r\n\r\n<a name=\"usage\"></a>\r\n\r\n\r\n## Inputs\r\n\r\nTo run the Mobilome Annotation Pipeline on multiple samples, prepare a samplesheet with your input data that looks as in the following example. Note that `virify_gff` is an optional input for this pipeline generated with [VIRify](https://github.com/EBI-Metagenomics/emg-viral-pipeline) v3.0.0 tool. \r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\nsample,assembly,user_proteins_gff,virify_gff\r\nminimal,/PATH/assembly.fasta,,\r\nassembly_proteins,/PATH/assembly.fasta,/PATH/proteins.gff,\r\nassembly_proteins_virify,/PATH/assembly.fasta,/PATH/proteins.gff,/PATH/virify_out.gff\r\n```\r\n\r\nEach row represents a sample. The minimal input is the (meta)genome assembly in fasta format.\r\n\r\nBasic run:\r\n\r\n```bash\r\n$ nextflow run /PATH/mobilome-annotation-pipeline/main.nf --input samplesheet.csv [--icefinder_sif icefinder-v1.0-local.sif]\r\n```\r\n\r\nNote that the final output in gff format is created by adding information to PROKKA output. If you have your own protein prediction files, provide the path the the uncompressed gff file in the samplesheet.csv. This file will be used to generate a `user_mobilome_extra.gff` file containing the mobilome plus any extra annotation generated on the annotation subworkflow.\r\n\r\nIf you want to integrate VIRify results to the final output provide the path to the GFF file generated by VIRify v3.0.0 in your samplesheet.csv.\r\n\r\n\r\n<a name=\"out\"></a>\r\n\r\n## Outputs\r\n\r\nResults will be written by default in the `mobilome_results` directory unless the `--outdir` option is used. There, you will find the following outputs:\r\n\r\n```bash\r\nmobilome_results/\r\n\u251c\u2500\u2500 mobilome.fasta\r\n\u251c\u2500\u2500 mobilome_prokka.gff\r\n\u251c\u2500\u2500 overlapping_integrons.txt\r\n\u251c\u2500\u2500 discarded_mge.txt\r\n\u251c\u2500\u2500 func_annot/\r\n\u251c\u2500\u2500 gff_output_files/\r\n\u251c\u2500\u2500 prediction/\r\n\u2514\u2500\u2500 preprocessing\r\n```\r\n\r\nThe AMRFinderPlus results are generated by default. The `func_annot/amr_location.txt` file contains a summary of the AMR genes annotated and their location (either mobilome or chromosome).\r\n\r\nThe file `discarded_mge.txt` contains a list of predictions that were discarded, along with the reason for their exclusion. Possible reasons include:\r\n\r\n1. 'mge < 500bp' Discarded by length.\r\n2. 'no_cds' If there are no genes encoded in the prediction.\r\n\r\nThe file `overlapping_integrons.txt` is a report of long-MGEs with overlapping coordinates. No predictions are discarded in this case.\r\n\r\nThe main output files containing the mobilome predictions are `mobilome.fasta` containing the nucleotide sequences of every prediction, and `mobilome_prokka.gff` containing the mobilome annotation plus any other feature annotated by PROKKA, mobileOG, or ViPhOG (only when VIRify results are provided).\r\n\r\nThe mobilome prediction IDs are build as follows:\r\n\r\n1. Contig ID\r\n2. MGE type:\r\n   flanking_site\r\n   recombination_site\r\n   prophage\r\n   viral_sequence\r\n   plasmid\r\n   phage_plasmid\r\n   integron\r\n   conjugative_integron\r\n   insertion_sequence\r\n3. Start and end coordinates separated by ':'\r\n\r\nExample:\r\n\r\n```bash\r\n>contig_id|mge_type-start:end\r\n```\r\n\r\nAny CDS with a coverage >= 0.9 in the boundaries of a predicted MGE is considered as part of the mobilome and labelled acordingly in the attributes field under the key `location`.\r\n\r\nThe labels used in the Type column of the gff file corresponds to the following nomenclature according to the [Sequence Ontology resource](http://www.sequenceontology.org/browser/current_svn/term/SO:0000001) when possible:\r\n\r\n| Type in gff file                 | Sequence ontology ID                                                              | Element description                                         | Reporting tool            |\r\n| -------------------------------- | --------------------------------------------------------------------------------- | ----------------------------------------------------------- | ------------------------- |\r\n| insertion_sequence               | [SO:0000973](http://www.sequenceontology.org/browser/current_svn/term/SO:0000973) | Insertion sequence                                          | ISEScan, PaliDIS          |\r\n| terminal_inverted_repeat_element | [SO:0000481](http://www.sequenceontology.org/browser/current_svn/term/SO:0000481) | Terminal Inverted Repeat (TIR) flanking insertion sequences | ISEScan, PaliDIS          |\r\n| integron                         | [SO:0000365](http://www.sequenceontology.org/browser/current_svn/term/SO:0000365) | Integrative mobilizable element                             | IntegronFinder, ICEfinder |\r\n| attC_site                        | [SO:0000950](http://www.sequenceontology.org/browser/current_svn/term/SO:0000950) | Integration site of DNA integron                            | IntegronFinder            |\r\n| conjugative_integron             | [SO:0000371](http://www.sequenceontology.org/browser/current_svn/term/SO:0000371) | Integrative Conjugative Element                             | ICEfinder                 |\r\n| direct_repeat                    | [SO:0000314](http://www.sequenceontology.org/browser/current_svn/term/SO:0000314) | Flanking regions on mobilizable elements                    | ICEfinder                 |\r\n| prophage                         | [SO:0001006](http://www.sequenceontology.org/browser/current_svn/term/SO:0001006) | Temperate phage                                             | geNomad, VIRify           |\r\n| viral_sequence                   | [SO:0001041](http://www.sequenceontology.org/browser/current_svn/term/SO:0001041) | Viral genome fragment                                       | geNomad, VIRify           |\r\n| plasmid                          | [SO:0000155](http://www.sequenceontology.org/browser/current_svn/term/SO:0000155) | Plasmid                                                     | geNomad                   |\r\n\r\n<a name=\"test\"></a>\r\n\r\n## Tests\r\n\r\nNextflow tests are executed with [nf-test](https://github.com/askimed/nf-test). It takes around 3 min in executing.\r\n\r\nRun:\r\n\r\n```bash\r\n$ cd mobilome-annotation-pipeline/\r\n$ nf-test test\r\n```\r\n\r\n<a name=\"cite\"></a>\r\n\r\n## Citation\r\n\r\nThe Mobilome Annotation Pipeline parses and integrates the output of the following tools and DBs sorted alphabetically:\r\n\r\n- AMRFinderPlus v3.11.4 with database v2023-02-23.1 [Feldgarden et al., Sci Rep, 2021](https://doi.org/10.1038/s41598-021-91456-0)\r\n- Diamond v2.0.12 [Buchfink et al., Nature Methods, 2021](https://doi.org/10.1038/s41592-021-01101-x)\r\n- geNomad v1.6.1 [Camargo et al., Nature Biotechnology, 2023](https://doi.org/10.1038/s41587-023-01953-y)\r\n- ICEfinder v1.0 [Liu et al., Nucleic Acids Res, 2019](https://doi.org/10.1093/nar/gky1123)\r\n- IntegronFinder2 v2.0.2 [N\u00e9ron et al., Microorganisms, 2022](https://doi.org/10.3390/microorganisms10040700)\r\n- ISEScan v1.7.2.3 [Xie et al., Bioinformatics, 2017](https://doi.org/10.1093/bioinformatics/btx433)\r\n- MobileOG-DB Beatrix 1.6 v1 [Brown et al., Appl Environ Microbiol, 2022](https://doi.org/10.1128/aem.00991-22)\r\n- PROKKA v1.14.6 [Seemann, Bioinformatics, 2014](https://doi.org/10.1093/bioinformatics/btu153)\r\n- VIRify v3.0.0 [Rangel-Pineros et al., PLoS Comput Biol, 2023](https://doi.org/10.1371/journal.pcbi.1011422)\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Metagenomics",
            "Mobile genetic elements"
        ],
        "filtered_on": "edam",
        "id": "452",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/452?version=2",
        "name": "Mobilome Annotation Pipeline",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "mge",
            "metagenomics",
            "mobilome",
            "nextflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-17",
        "versions": 2
    },
    {
        "create_time": "2025-06-16",
        "creators": [],
        "description": "Assembly of metagenomic sequencing data\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Assembly of metagenomic sequencing data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metagenomics-assembly/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Polina Polunina, B\u00e9r\u00e9nice Batut\n\n**Tutorial Author(s)**: [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Tutorial Contributor(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1390",
        "keep": "Keep",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1390?version=2",
        "name": "Assembly of metagenomic sequencing data",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "bandage_info",
            "megahit",
            "megahit_contig2fastg",
            "collection_column_join",
            "bandage_image",
            "bowtie2",
            "metaspades",
            "coverm_contig",
            "quast"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-16",
        "versions": 2
    },
    {
        "create_time": "2025-06-12",
        "creators": [],
        "description": "<div align=\"center\">\r\n\r\n[<img src=\"https://raw.githubusercontent.com/sanjaynagi/AmpSeeker/main/docs/ampseeker-docs/logo.png\" width=\"400\"/>](https://raw.githubusercontent.com/sanjaynagi/AmpSeeker/main/docs/ampseeker-docs/logo.png)   \r\n\r\n\r\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22658.0.0-brightgreen.svg)](https://snakemake.bitbucket.io)\r\n[![GitHub release](https://img.shields.io/github/release/sanjaynagi/AmpSeeker?include_prereleases=&sort=semver&color=blue)](https://github.com/sanjaynagi/AmpSeeker/releases/)\r\n[![License](https://img.shields.io/badge/License-MIT-blue)](#license)\r\n\r\n</div>\r\n\r\n**Documentation**: https://sanjaynagi.github.io/AmpSeeker/ \r\n\r\nAmpSeeker is a snakemake workflow for Amplicon Sequencing data analysis. The pipeline is generic and can work on any data, but is tailored towards insecticide resistance monitoring. It implements:\r\n\r\n- BCL to Fastq conversion\r\n- Genome alignment\r\n- Variant calling\r\n- Quality control\r\n- Coverage\r\n- Visualisation of reads in IGV\r\n- VCF to DataFrame/.xlsx \r\n- Allele frequency calculation\r\n- Population structure\r\n- Geographic sample maps\r\n- Genetic diversity\r\n\r\n- Kdr origin analysis (Ag-vampIR panel)\r\n- Species assignment (Ag-vampIR panel)\r\n\r\nThe workflow uses a combination of papermill and jupyter book, so that users can visually explore the results in a local webpage for convenience.\r\n\r\n## Usage\r\n\r\nPlease see the [documentation](https://sanjaynagi.github.io/AmpSeeker/) for more information on running the workflow.\r\n\r\n## Citation \r\n\r\n**Targeted genomic surveillance of insecticide resistance in African malaria vectors**  \r\nNagi, *et al*., 2025. *bioRxiv*. doi: https://doi.org/10.1101/2025.02.14.637727\r\n\r\n## Testing\r\n\r\nTest cases are in the subfolder `.test`. They are automatically executed via continuous integration with [GitHub Actions](https://github.com/features/actions).\r\n\r\n## Contributing to AmpSeeker\r\n\r\n1. Fork the repository to your own GitHub user account\r\n2. Clone your fork\r\n3. Create a branch to implement the changes or features you would like `git checkout -b my_new_feature-24-03-23`\r\n4. Implement the changes\r\n5. Use `git add FILES`, `git commit -m COMMENT`, and `git push` to push your changes back to the branch\r\n6. Open a Pull request to the main repository \r\n7. Once the pull request is merged, either delete your fork, or switch back to the main branch `git checkout main` and use `git pull upstream main` to incorporate the changes back in your local repo. Prior to `git pull upstream main`, you may need to set sanjaynagi/AmpSeeker as the upstream remote url, with `git remote set-url upstream git@github.com:sanjaynagi/AmpSeeker.git`. \r\n8. At this stage, your local repo should be up to date with the main Ampseeker branch and you are ready to start from #3 if you have more contributions!\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in description",
        "id": "1729",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1729?version=1",
        "name": "AmpSeeker",
        "number_of_steps": 0,
        "projects": [
            "Vector informatics and genomics group"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2025-06-12",
        "versions": 1
    },
    {
        "create_time": "2025-06-11",
        "creators": [
            "Gisela Gabernet",
            "Simon Heumos",
            "Alexander Peltzer"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-airrflow_logo_dark.png\">\n    <img alt=\"nf-core/airrflow\" src=\"docs/images/nf-core-airrflow_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/airrflow/workflows/nf-core%20CI/badge.svg)](https://github.com/nf-core/airrflow/actions?query=workflow%3A%22nf-core+CI%22)\n[![GitHub Actions Linting Status](https://github.com/nf-core/airrflow/workflows/nf-core%20linting/badge.svg)](https://github.com/nf-core/airrflow/actions?query=workflow%3A%22nf-core+linting%22)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/airrflow/results)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.2642009-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.2642009)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/airrflow)\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23airrflow-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/airrflow)\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)\n[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)\n[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n[![AIRR compliant](https://img.shields.io/static/v1?label=AIRR-C%20sw-tools%20v1&message=compliant&color=008AFF&labelColor=000000&style=plastic)](https://docs.airr-community.org/en/stable/swtools/airr_swtools_standard.html)\n\n## Introduction\n\n**nf-core/airrflow** is a bioinformatics best-practice pipeline to analyze B-cell or T-cell repertoire sequencing data. The input data can be targeted amplicon bulk sequencing data of the V, D, J and C regions of the B/T-cell receptor with multiplex PCR or 5' RACE protocol, single-cell VDJ sequencing using the 10xGenomics libraries, or assembled reads (bulk or single-cell). It can also extract BCR and TCR sequences from bulk or single-cell untargeted RNAseq data. It makes use of the [Immcantation](https://immcantation.readthedocs.io) toolset as well as other AIRR-seq analysis tools.\n\n![nf-core/airrflow overview](docs/images/airrflow_workflow_overview.png)\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/airrflow/results).\n\n## Pipeline summary\n\nnf-core/airrflow allows the end-to-end processing of BCR and TCR bulk and single cell targeted sequencing data. Several protocols are supported, please see the [usage documentation](https://nf-co.re/airrflow/usage) for more details on the supported protocols. The pipeline has been certified as [AIRR compliant](https://docs.airr-community.org/en/stable/swtools/airr_swtools_compliant.html) by the AIRR community, which means that it is compatible with downstream analysis tools also supporting this format.\n\n![nf-core/airrflow overview](docs/images/metro-map-airrflow.png)\n\n1. QC and sequence assembly\n\n- Bulk\n  - Raw read quality control, adapter trimming and clipping (`Fastp`).\n  - Filter sequences by base quality (`pRESTO FilterSeq`).\n  - Mask amplicon primers (`pRESTO MaskPrimers`).\n  - Pair read mates (`pRESTO PairSeq`).\n  - For UMI-based sequencing:\n    - Cluster sequences according to similarity (optional for insufficient UMI diversity) (`pRESTO ClusterSets`).\n    - Build consensus of sequences with the same UMI barcode (`pRESTO BuildConsensus`).\n  - Assemble R1 and R2 read mates (`pRESTO AssemblePairs`).\n  - Remove and annotate read duplicates (`pRESTO CollapseSeq`).\n  - Filter out sequences that do not have at least 2 duplicates (`pRESTO SplitSeq`).\n- single cell\n  - cellranger vdj\n    - Assemble contigs\n    - Annotate contigs\n    - Call cells\n    - Generate clonotypes\n\n2. V(D)J annotation and filtering (bulk and single-cell)\n\n- Assign gene segments with `IgBlast` using a germline reference (`Change-O AssignGenes`).\n- Annotate alignments in AIRR format (`Change-O MakeDB`)\n- Filter by alignment quality (locus matching v_call chain, min 200 informative positions, max 10% N nucleotides)\n- Filter productive sequences (`Change-O ParseDB split`)\n- Filter junction length multiple of 3\n- Annotate metadata (`EnchantR`)\n\n3. QC filtering (bulk and single-cell)\n\n- Bulk sequencing filtering:\n  - Remove chimeric sequences (optional) (`SHazaM`, `EnchantR`)\n  - Detect cross-contamination (optional) (`EnchantR`)\n  - Collapse duplicates (`Alakazam`, `EnchantR`)\n- Single-cell QC filtering (`EnchantR`)\n  - Remove cells without heavy chains.\n  - Remove cells with multiple heavy chains.\n  - Remove sequences in different samples that share the same `cell_id` and nucleotide sequence.\n  - Modify `cell_id`s to ensure they are unique in the project.\n\n4. Clonal analysis (bulk and single-cell)\n\n- Find threshold for clone definition (`SHazaM`, `EnchantR`).\n- Create germlines and define clones, repertoire analysis (`SCOPer`, `EnchantR`).\n- Build lineage trees (`Dowser`, `IgphyML`, `RAxML`, `EnchantR`).\n\n5. Repertoire analysis and reporting\n\n- Custom repertoire analysis pipeline report (`Alakazam`).\n- Aggregate QC reports (`MultiQC`).\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, ensure that the pipeline tests run on your infrastructure:\n\n```bash\nnextflow run nf-core/airrflow -profile test,<docker/singularity/podman/shifter/charliecloud/conda/institute> --outdir <OUTDIR>\n```\n\nTo run nf-core/airrflow with your data, prepare a tab-separated samplesheet with your input data. Depending on the input data type (bulk or single-cell, raw reads or assembled reads) the input samplesheet will vary. Please follow the [documentation on samplesheets](https://nf-co.re/airrflow/usage#input-samplesheet) for more details. An example samplesheet for running the pipeline on bulk BCR / TCR sequencing data in fastq format looks as follows:\n\n| sample_id | filename_R1                     | filename_R2                     | filename_I1                     | subject_id | species | pcr_target_locus | tissue | sex    | age | biomaterial_provider | single_cell | intervention   | collection_time_point_relative | cell_subset  |\n| --------- | ------------------------------- | ------------------------------- | ------------------------------- | ---------- | ------- | ---------------- | ------ | ------ | --- | -------------------- | ----------- | -------------- | ------------------------------ | ------------ |\n| sample01  | sample1_S8_L001_R1_001.fastq.gz | sample1_S8_L001_R2_001.fastq.gz | sample1_S8_L001_I1_001.fastq.gz | Subject02  | human   | IG               | blood  | NA     | 53  | sequencing_facility  | FALSE       | Drug_treatment | Baseline                       | plasmablasts |\n| sample02  | sample2_S8_L001_R1_001.fastq.gz | sample2_S8_L001_R2_001.fastq.gz | sample2_S8_L001_I1_001.fastq.gz | Subject02  | human   | TR               | blood  | female | 78  | sequencing_facility  | FALSE       | Drug_treatment | Baseline                       | plasmablasts |\n\nEach row represents a sample with fastq files (paired-end).\n\nA typical command to run the pipeline from **bulk raw fastq files** is:\n\n```bash\nnextflow run nf-core/airrflow \\\n-r <release> \\\n-profile <docker/singularity/podman/shifter/charliecloud/conda/institute> \\\n--mode fastq \\\n--input input_samplesheet.tsv \\\n--library_generation_method specific_pcr_umi \\\n--cprimers CPrimers.fasta \\\n--vprimers VPrimers.fasta \\\n--umi_length 12 \\\n--umi_position R1 \\\n--outdir ./results\n```\n\nFor common **bulk sequencing protocols** we provide pre-set profiles that specify primers, UMI length, etc for common commercially available sequencing protocols. Please check the [Supported protocol profiles](#supported-protocol-profiles) for a full list of available profiles. An example command running the NEBNext UMI protocol profile with docker containers is:\n\n```bash\nnextflow run nf-core/airrflow \\\n-profile nebnext_umi,docker \\\n--mode fastq \\\n--input input_samplesheet.tsv \\\n--outdir results\n```\n\nA typical command to run the pipeline from **single cell raw fastq files** (10X genomics) is:\n\n```bash\nnextflow run nf-core/airrflow -r dev \\\n-profile <docker/singularity/podman/shifter/charliecloud/conda/institute> \\\n--mode fastq \\\n--input input_samplesheet.tsv \\\n--library_generation_method sc_10x_genomics \\\n--reference_10x reference/refdata-cellranger-vdj-GRCh38-alts-ensembl-5.0.0.tar.gz \\\n--outdir ./results\n```\n\nA typical command to run the pipeline from **single-cell AIRR rearrangement tables or assembled bulk sequencing fasta** data is:\n\n```bash\nnextflow run nf-core/airrflow \\\n-r <release> \\\n-profile <docker/singularity/podman/shifter/charliecloud/conda/institute> \\\n--input input_samplesheet.tsv \\\n--mode assembled \\\n--outdir results\n```\n\nSee the [usage documentation](https://nf-co.re/airrflow/usage) and the [parameter documentation](https://nf-co.re/airrflow/parameters) for more details on how to use the pipeline and all the available parameters.\n\n:::warning\nPlease provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\nprovided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\nsee [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\n:::\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/airrflow/usage) and the [parameter documentation](https://nf-co.re/airrflow/parameters).\n\n## Pipeline output\n\nTo see the the results of a test run with a full size dataset refer to the [results](https://nf-co.re/airrflow/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/airrflow/output).\n\n## Credits\n\nnf-core/airrflow was originally written by:\n\n- [Gisela Gabernet](https://github.com/ggabernet)\n- [Susanna Marquez](https://github.com/ssnn-airr)\n- [Alexander Peltzer](https://github.com/apeltzer)\n\nWe thank the following people for their extensive assistance in the development of the pipeline:\n\n- [David Ladd](https://github.com/dladd)\n- [Friederike Hanssen](https://github.com/friederikehanssen)\n- [Simon Heumos](https://github.com/subwaystation)\n- [Mark Polster](https://github.com/mapo9)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#airrflow` channel](https://nfcore.slack.com/channels/airrflow) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/airrflow for your analysis, please cite the article as follows:\n\n> **nf-core/airrflow: an adaptive immune receptor repertoire analysis workflow employing the Immcantation framework**\n>\n> Gisela Gabernet, Susanna Marquez, Robert Bjornson, Alexander Peltzer, Hailong Meng, Edel Aron, Noah Y. Lee, Cole G. Jensen, David Ladd, Mark Polster, Friederike Hanssen, Simon Heumos, nf-core community, Gur Yaari, Markus C. Kowarik, Sven Nahnsen, Steven H. Kleinstein. (2024) PLOS Computational Biology, 20(7), e1012265. doi: [https://doi.org/10.1371/journal.pcbi.1012265](https://doi.org/10.1371/journal.pcbi.1012265). Pubmed PMID: 39058741.\n\nThe specific pipeline version using the following DOI: [10.5281/zenodo.2642009](https://doi.org/10.5281/zenodo.2642009)\n\nPlease also cite all the tools that are being used by the pipeline. An extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in description",
        "id": "963",
        "keep": "To Curate",
        "latest_version": 17,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/963?version=17",
        "name": "nf-core/airrflow",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "airr",
            "b-cell",
            "immcantation",
            "immunorepertoire",
            "repseq"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-11",
        "versions": 17
    },
    {
        "create_time": "2025-06-10",
        "creators": [
            "Cyril Noel",
            "Antoine Veron",
            "Fran\u00e7oise Vincent-Hubert",
            "Julien Schaeffer",
            "Marion Desdouits",
            "Soizick Le Guyader"
        ],
        "description": "## Introduction\r\n\r\n**samba-norovirus** is an adaptation of the [**samba workflow**](https://gitlab.ifremer.fr/bioinfo/workflows/samba) for the specific needs in metabarcoding analyses of norovirus. It is a FAIR scalable workflow integrating, into a unique tool, state-of-the-art bioinformatics and statistical methods to conduct reproducible metabarcoding and eDNA analyses using [Nextflow](https://www.nextflow.io) (Di Tommaso *et al.*, 2017). SAMBA performs complete metabarcoding analysis by:\r\n- processing data using commonly used procedure with [QIIME 2](https://qiime2.org/) (version 2024.2 ; Bolyen *et al.*, 2019):\r\n    - remove primers from raw reads and remove reads without detected primer using the QIIME 2 plugin of [cutadapt](http://dx.doi.org/10.14806/ej.17.1.200): `q2-cutadapt`\r\n    - denoise reads and infering ASV using the QIIME 2 plugin of [DADA2](Callahan *et al.*, 2016): `q2-dada2`\r\n    - cluster ASV by small local linking threshold using [swarm](https://github.com/torognes/swarm) (Mah\u00e9 *et al.*, 2022)\r\n    - detect and remove chimeras using [UCHIME](https://doi.org/10.1093/bioinformatics/btr381) (Edgar *et al.*, 2011)\r\n    - assign the ASV taxonomy using the Naive Bayesian classification from the QIIME 2 plugin `q2-feature-classifier`\r\n- post-process ASV table with different opitonal processes:\r\n    - remove contaminant from biological samples using positive and/or negative control samples with the R package: [microDecon](https://github.com/donaldtmcknight/microDecon) (McKnight *et al.*, 2019)\r\n    - removal of ASVs belonging to undesired taxa using the QIIME 2 plugin `q2-taxa` (filter-table & filter-seqs)\r\n    - removal of ASVs based on their frequency, contingency and heir length using the QIIME 2 plugin `q2-feature-table` (filter-table, filter-seqs & filter-features)\r\n- conducting extended statistical and ecological analyses using homemade Rscript\r\n\r\nThe **samba-norovirus** pipeline can run tasks across multiple compute infrastructures in a very portable manner. It comes with singularity containers making installation trivial and results highly reproducible.\r\n\r\n## Requirements\r\n\r\ni. You must have [`Nextflow (\u2265 v24.04.4)`](https://www.nextflow.io/docs/latest/getstarted.html#installation) installed on your computing machine to run the workflow.\r\n\r\nii. You must have [`Singularity (\u2265 v3.6.4)`](https://www.sylabs.io/guides/3.0/user-guide/) installed on your computing machine for full pipeline reproducibility. \r\n\r\niii. If your HPC nodes don't have any internet access. Please download before any workflow run the singularity images available on the [samba-norovirus singularity image repository](https://data-dataref.ifremer.fr/bioinfo/ifremer/sebimer/tools/samba-norovirus/1.0.0/). Then set the `$NXF_SINGULARITY_CACHEDIR` environment variable to the path where you just downloaded the images.\r\n\r\niv. You must download the norovirus database and sequences formatted for the workflow in order to perform the taxonomic assignment of your ASVs and the chimeras detection. You can download all files on the [samba-norovirus database repository](https://data-dataref.ifremer.fr/bioinfo/ifremer/sebimer/sequence-set/samba-norovirus/1.0.0). Then set the `database` and `uchime_ref` parameters in the base.config file with the path where you placed the downloaded files.\r\n\r\n\r\n## Quick Start\r\n\r\ni. Download the pipeline\r\n\r\n```bash\r\ngit clone https://gitlab.ifremer.fr/bioinfo/workflows/samba-norovirus\r\n```\r\n\r\n> To use SAMBA-norovirus on a computing cluster, it is necessary to provide a configuration file for your system. For some institutes, this one already exists and is referenced on [nf-core/configs](https://github.com/nf-core/configs#documentation). If so, you can simply download your institute custom config file and simply use `-c <institute_config_file>` in your command. This will set the appropriate execution settings for your local compute environment.\r\n\r\nii. Start running your own analysis!\r\n\r\nBefore you start analyzing your data, please read the [SAMBA workflow documentation](./docs/usage.md)\r\n\r\n```bash\r\nnextflow run main.nf -profile norovirus,singularity [-c <institute_config_file>]\r\n```\r\n\r\n## Credits\r\n\r\nsamba-norovirus is written by [Cyril No\u00ebl](https://github.com/cnoel-sebimer) from the [SeBiMER](https://sebimer.ifremer.fr/), the Bioinformatics Core Facility of [IFREMER](https://wwz.ifremer.fr/en/). This workflow was developed in close collaboration with members of the Ifremer LSEM lab.\r\n\r\n## Contributions\r\n\r\nWe welcome contributions to the pipeline. If such case you can do one of the following:\r\n* Use issues to submit your questions \r\n* Fork the project, do your developments and submit a pull request\r\n* Contact us (see email below) \r\n",
        "doi": "10.48546/workflowhub.workflow.1727.1",
        "edam_operation": [
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metabarcoding",
            "Workflows"
        ],
        "filtered_on": "edam",
        "id": "1727",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "AGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1727?version=1",
        "name": "SAMBA-norovirus",
        "number_of_steps": 0,
        "projects": [
            "SeBiMER"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metabarcoding",
            "workflows",
            "taxonomic-classification"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-10",
        "versions": 1
    },
    {
        "create_time": "2021-09-10",
        "creators": [
            "Cyril Noel",
            "Alexandre Cormier",
            "Laura Leroi",
            "Patrick Durand"
        ],
        "description": "SAMBA is a FAIR scalable workflow integrating, into a unique tool, state-of-the-art bioinformatics and statistical methods to conduct reproducible eDNA analyses using Nextflow. SAMBA starts processing by verifying integrity of raw reads and metadata. Then all bioinformatics processing is done using commonly used procedure (QIIME 2 and DADA2) but adds new steps relying on dbOTU3 and microDecon to build high quality ASV count tables. Extended statistical analyses are also performed. Finally, SAMBA produces a full dynamic HTML report including resources used, commands executed, intermediate results, statistical analyses and figures.\r\n\r\nThe SAMBA pipeline can run tasks across multiple compute infrastructures in a very portable manner. It comes with singularity containers making installation trivial and results highly reproducible.",
        "doi": "10.48546/workflowhub.workflow.156.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metabar.* in tags",
        "id": "156",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "AGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/156?version=1",
        "name": "SAMBA: Standardized and Automated MetaBarcoding Analyses workflow",
        "number_of_steps": 0,
        "projects": [
            "SeBiMER"
        ],
        "source": "WorkflowHub",
        "tags": [
            "16s",
            "18s",
            "metabarcoding",
            "nextflow",
            "edna"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-06-05",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Metabarcoding/eDNA through Obitools\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metabarcoding/eDNA through Obitools](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/Obitools-metabarcoding/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Coline Royaux](https://training.galaxyproject.org/training-material/hall-of-fame/colineroyaux/), [Olivier Norvez](https://training.galaxyproject.org/training-material/hall-of-fame/onorvez/), [Eric Coissac](https://training.galaxyproject.org/training-material/hall-of-fame/ecoissac/), [Fr\u00e9d\u00e9ric Boyer](https://training.galaxyproject.org/training-material/hall-of-fame/fboyer/), [Yvan Le Bras](https://training.galaxyproject.org/training-material/hall-of-fame/yvanlebras/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Anne Fouilloux](https://training.galaxyproject.org/training-material/hall-of-fame/annefou/), [Yvan Le Bras](https://training.galaxyproject.org/training-material/hall-of-fame/yvanlebras/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n**Funder(s)**: [P\u00f4le National de Donn\u00e9es de Biodiversit\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/pndb/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metabar.* in description",
        "id": "1702",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1702?version=1",
        "name": "Workflow constructed from history 'Tuto Obitools'",
        "number_of_steps": 23,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "ecology",
            "gtn",
            "galaxy"
        ],
        "tools": [
            "obi_grep",
            "obi_ngsfilter",
            "obi_annotate",
            "seq_filter_by_id",
            "obi_illumina_pairend",
            "obi_uniq",
            "ncbi_blastn_wrapper",
            "fastqc",
            "Cut1",
            "obi_tab",
            "obi_stat",
            "fastq_groomer",
            "join1",
            "Filter1",
            "obi_clean",
            "wc_gnu",
            "unzip"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Assembly of metagenomic sequencing data](https://training.galaxyproject.org/training-material/topics/assembly/tutorials/metagenomics-assembly/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in name",
        "id": "1634",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1634?version=1",
        "name": "Metagenomics assembly tutorial workflow",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy"
        ],
        "tools": [
            "bandage_info",
            "megahit",
            "megahit_contig2fastg",
            "collection_column_join",
            "bandage_image",
            "bowtie2",
            "metaspades",
            "quast"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "From a reference and a primer scheme generate two masked half-genome references for ITR-aware pox virus sequencing data analysis.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pox virus genome analysis from tiled-amplicon sequencing data](https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/pox-tiled-amplicon/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Viktoria Isabel Schwarz, Wolfgang Maier\n\n**Tutorial Author(s)**: [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Tomas Klingstr\u00f6m](https://training.galaxyproject.org/training-material/hall-of-fame/TKlingstrom/)\n\n**Tutorial Contributor(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Anton Nekrutenko](https://training.galaxyproject.org/training-material/hall-of-fame/nekrut/)\n\n**Grants(s)**: [BeYond-COVID](https://training.galaxyproject.org/training-material/hall-of-fame/by-covid/), [ELIXIR-CONVERGE](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-converge/), [Addressing the dual emerging threats of African Swine Fever and Lumpy Skin Disease in Europe](https://training.galaxyproject.org/training-material/hall-of-fame/h2020-defend/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "1632",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1632?version=1",
        "name": "pox-virus-tiled-amplicon-ref-masking",
        "number_of_steps": 14,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "variant-analysis"
        ],
        "tools": [
            "fasta_compute_length",
            "compose_text_param",
            "Add_a_column1",
            "Cut1",
            "datamash_ops",
            "EMBOSS: maskseq51",
            "param_value_from_file",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Assembly of metagenomic sequencing data](https://training.galaxyproject.org/training-material/topics/assembly/tutorials/metagenomics-assembly/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "1631",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1631?version=1",
        "name": "workflow-generate-dataset-for-assembly-tutorial",
        "number_of_steps": 18,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy"
        ],
        "tools": [
            "cutadapt",
            "random_lines1",
            "bamtools",
            "filter_tabular",
            "ngsutils_bam_filter",
            "fastqc",
            "megahit",
            "bowtie2",
            "bg_uniq",
            "seqtk_subseq",
            "tp_cat"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Design plasmids encoding predicted pathways by using the BASIC assembly method.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Designing plasmids encoding predicted pathways by using the BASIC assembly method](https://training.galaxyproject.org/training-material/topics/synthetic-biology/tutorials/basic_assembly_analysis/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Kenza Bazi-Kabbaj](https://training.galaxyproject.org/training-material/hall-of-fame/kenza12/), [Thomas Duigou](https://training.galaxyproject.org/training-material/hall-of-fame/tduigou/), [Joan H\u00e9risson](https://training.galaxyproject.org/training-material/hall-of-fame/breakthewall/), [Guillaume Gricourt](https://training.galaxyproject.org/training-material/hall-of-fame/guillaume-gricourt/), [Ioana Popescu](https://training.galaxyproject.org/training-material/hall-of-fame/ioanagry/), [Jean-Loup Faulon](https://training.galaxyproject.org/training-material/hall-of-fame/jfaulon/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid.* in description",
        "id": "1576",
        "keep": "Reject",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1576?version=1",
        "name": "Genetic Design (BASIC Assembly)",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "synthetic-biology"
        ],
        "tools": [
            "rpbasicdesign",
            "selenzy-wrapper",
            "dnabot"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Starting from the BAM files produced by snippy, generate a table that summarizes the drug-resistance profile for each sample\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identifying tuberculosis transmission links: from SNPs to transmission clusters](https://training.galaxyproject.org/training-material/topics/evolution/tutorials/mtb_transmission/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Galo A. Goig](https://training.galaxyproject.org/training-material/hall-of-fame/GaloGS/), [Daniela Brites](https://training.galaxyproject.org/training-material/hall-of-fame/dbrites/), [Christoph Stritt](https://training.galaxyproject.org/training-material/hall-of-fame/cstritt/)\n\n**Tutorial Contributor(s)**: [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Galo A. Goig](https://training.galaxyproject.org/training-material/hall-of-fame/GaloGS/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Peter van Heusden](https://training.galaxyproject.org/training-material/hall-of-fame/pvanheus/), [Christoph Stritt](https://training.galaxyproject.org/training-material/hall-of-fame/cstritt/), [Lucille Delisle](https://training.galaxyproject.org/training-material/hall-of-fame/lldelisle/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in name",
        "id": "1564",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1564?version=1",
        "name": "From BAMs to drug resistance prediction with TB-profiler",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "evolution"
        ],
        "tools": [
            "tp_replace_in_line",
            "samtools_view",
            "tp_grep_tool",
            "tp_sed_tool",
            "addName",
            "tb_profiler_profile",
            "Merge single-end and paired-end BAMs in a single collection to be analyzed alltogether\n__MERGE_COLLECTION__",
            "tp_cat"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Secondary metabolite discovery](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/secondary-metabolite-discovery/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metab.* in description",
        "id": "1558",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1558?version=1",
        "name": "Gene Cluster Product Similarity Search",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "sempi3"
        ],
        "tools": [
            "tp_awk_tool",
            "ncbi_acc_download",
            "openbabel_remDuplicates",
            "ctb_np-likeness-calculator",
            "interactive_tool_jupyter_notebook",
            "ctb_chemfp_mol2fps",
            "Remove beginning1",
            "collapse_dataset",
            "antismash",
            "ctb_silicos_qed",
            "ctb_simsearch"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "example cellprofiler pipeline\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Object tracking using CellProfiler](https://training.galaxyproject.org/training-material/topics/imaging/tutorials/object-tracking-using-cell-profiler/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Yi Sun](https://training.galaxyproject.org/training-material/hall-of-fame/sunyi000/), [Beatriz Serrano-Solano](https://training.galaxyproject.org/training-material/hall-of-fame/beatrizserrano/), [Jean-Karim H\u00e9rich\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/jkh1/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Beatriz Serrano-Solano](https://training.galaxyproject.org/training-material/hall-of-fame/beatrizserrano/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1503",
        "keep": "Reject",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1503?version=1",
        "name": "CP_object_tracking_example",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cellprofiler",
            "gtn",
            "galaxy",
            "imaging"
        ],
        "tools": [
            "cp_color_to_gray",
            "cp_identify_primary_objects",
            "cp_common",
            "cp_measure_object_size_shape",
            "cp_tile",
            "cp_overlay_outlines",
            "cp_export_to_spreadsheet",
            "cp_measure_object_intensity",
            "cp_save_images",
            "cp_cellprofiler",
            "unzip",
            "cp_track_objects"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Nanopore datasets analysis - Phylogenetic Identification - antibiotic resistance genes detection and contigs building\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pathogen detection from (direct Nanopore) sequencing data using Galaxy - Foodborne Edition](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Engy Nasr, B\u00e9r\u00e9nice Batut, Paul Zierep\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Hans-Rudolf Hotz](https://training.galaxyproject.org/training-material/hall-of-fame/hrhotz/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EOSC-Life](https://training.galaxyproject.org/training-material/hall-of-fame/eosc-life/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in description",
        "id": "1495",
        "keep": "Keep",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1495?version=1",
        "name": "Gene-based Pathogen Identification",
        "number_of_steps": 15,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:iwc",
            "name:microgalaxy",
            "name:pathogfair",
            "name:collection"
        ],
        "tools": [
            "compose_text_param",
            "medaka_consensus_pipeline",
            "tab2fasta",
            "bandage_image",
            "abricate",
            "collection_element_identifiers",
            "tp_find_and_replace",
            "flye",
            "param_value_from_file",
            "__BUILD_LIST__",
            "fasta2tab",
            "split_file_to_collection"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Identification of AMR genes in an assembled bacterial genome\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identification of AMR genes in an assembled bacterial genome](https://training.galaxyproject.org/training-material/topics/genome-annotation/tutorials/amr-gene-detection/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Bazante Sanders, B\u00e9r\u00e9nice Batut\n\n**Tutorial Author(s)**: [Bazante Sanders](https://training.galaxyproject.org/training-material/hall-of-fame/bazante1/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Bazante Sanders](https://training.galaxyproject.org/training-material/hall-of-fame/bazante1/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Miaomiao Zhou](https://training.galaxyproject.org/training-material/hall-of-fame/miaomiaozhou88/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [pimarin](https://training.galaxyproject.org/training-material/hall-of-fame/pimarin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Anthony Bretaudeau](https://training.galaxyproject.org/training-material/hall-of-fame/abretaud/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/)\n\n**Funder(s)**: [Avans Hogeschool](https://training.galaxyproject.org/training-material/hall-of-fame/avans-atgm/), [ABRomics](https://training.galaxyproject.org/training-material/hall-of-fame/abromics/), [ELIXIR Europe](https://training.galaxyproject.org/training-material/hall-of-fame/elixir-europe/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amr in name",
        "id": "1488",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1488?version=1",
        "name": "mrsa AMR gene detection",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microgalaxy",
            "genome-annotation"
        ],
        "tools": [
            "jbrowse",
            "bakta",
            "tbl2gff3",
            "bowtie2",
            "Grep1",
            "staramr_search"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Microbiome - Taxonomy Profiling\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pathogen detection from (direct Nanopore) sequencing data using Galaxy - Foodborne Edition](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Engy Nasr, B\u00e9r\u00e9nice Batut, Paul Zierep\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Hans-Rudolf Hotz](https://training.galaxyproject.org/training-material/hall-of-fame/hrhotz/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EOSC-Life](https://training.galaxyproject.org/training-material/hall-of-fame/eosc-life/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in name",
        "id": "1483",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1483?version=1",
        "name": "Taxonomy Profiling and Visualization with Krona",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:iwc",
            "name:microgalaxy",
            "name:pathogfair",
            "name:collection"
        ],
        "tools": [
            "taxonomy_krona_chart",
            "kraken2",
            "krakentools_kreport2krona"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Interpreting MaxQuant data using MSstats involves applying a rigorous statistical framework to glean meaningful insights from quantitative proteomic datasets\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 5: Data Interpretation](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/clinical-mp-5-data-interpretation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1482",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1482?version=1",
        "name": "WF5_Data_Interpretation_Worklow",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "msstatstmt",
            "unipept",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "## Associated Tutorial\n\nThis workflows is part of the tutorial [Assembly of metagenomic sequencing data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metagenomics-assembly/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n**Tutorial Contributor(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "1480",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1480?version=1",
        "name": "workflow-generate-dataset-for-assembly-tutorial",
        "number_of_steps": 18,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy"
        ],
        "tools": [
            "cutadapt",
            "random_lines1",
            "bamtools",
            "filter_tabular",
            "ngsutils_bam_filter",
            "fastqc",
            "megahit",
            "bowtie2",
            "bg_uniq",
            "seqtk_subseq",
            "tp_cat"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Microbiome - Variant calling and Consensus Building\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Pathogen detection from (direct Nanopore) sequencing data using Galaxy - Foodborne Edition](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n* Uses [Galaxy Workflow Comments](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_comments.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Engy Nasr, B\u00e9r\u00e9nice Batut, Paul Zierep\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Hans-Rudolf Hotz](https://training.galaxyproject.org/training-material/hall-of-fame/hrhotz/), [Wolfgang Maier](https://training.galaxyproject.org/training-material/hall-of-fame/wm75/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Engy Nasr](https://training.galaxyproject.org/training-material/hall-of-fame/EngyNasr/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/), [EOSC-Life](https://training.galaxyproject.org/training-material/hall-of-fame/eosc-life/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in description",
        "id": "1479",
        "keep": "Keep",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1479?version=1",
        "name": "Allele-based Pathogen Identification",
        "number_of_steps": 23,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:iwc",
            "name:microgalaxy",
            "name:pathogfair",
            "name:collection"
        ],
        "tools": [
            "snpSift_filter",
            "regexColumn1",
            "Count1",
            "minimap2",
            "Cut1",
            "CONVERTER_gz_to_uncompressed",
            "tp_head_tool",
            "bcftools_norm",
            "samtools_depth",
            "samtools_coverage",
            "Remove beginning1",
            "table_compute",
            "collapse_dataset",
            "Paste1",
            "clair3",
            "tp_cut_tool",
            "bcftools_consensus",
            "snpSift_extractFields"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Antibiotic resistance detection\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Antibiotic resistance detection](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/plasmid-metagenomics-nanopore/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Willem de Koning](https://training.galaxyproject.org/training-material/hall-of-fame/willemdek11/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1477",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1477?version=1",
        "name": "Copy Of GTN Training - Antibiotic Resistance Detection",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "nanoplot",
            "unicycler",
            "racon",
            "gfa_to_fa",
            "minimap2",
            "miniasm",
            "bandage_image",
            "PlasFlow",
            "staramr_search"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Analyses of metagenomics data - The global picture\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Analyses of metagenomics data - The global picture](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/general-tutorial/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1476",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1476?version=1",
        "name": "Amplicon Tutorial",
        "number_of_steps": 17,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_screen_seqs",
            "mothur_filter_seqs",
            "mothur_make_shared",
            "mothur_classify_otu",
            "mothur_make_biom",
            "mothur_summary_seqs",
            "mothur_cluster_split",
            "krona-text",
            "mothur_align_seqs",
            "mothur_count_seqs",
            "mothur_make_group",
            "mothur_merge_files",
            "mothur_pre_cluster",
            "mothur_unique_seqs",
            "mothur_classify_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Generating a large database and then reducing it to a compact database using Metanovo\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 1: Database-Generation](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/clinical-mp-1-database-generation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1474",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1474?version=1",
        "name": "WF1_Database_Generation_Workflow",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "metanovo",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S rRNA analysis with Nanopore reads\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial analysis with Nanopore data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/nanopore-16S-metagenomics/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1473",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1473?version=1",
        "name": "Training: 16S rRNA Analysis with Nanopore Sequencing Reads",
        "number_of_steps": 11,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "fastp",
            "tp_replace_in_line",
            "datamash_reverse",
            "fastqc",
            "porechop",
            "Remove beginning1",
            "taxonomy_krona_chart",
            "kraken2",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Analyses of metagenomics data - The global picture\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Analyses of metagenomics data - The global picture](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/general-tutorial/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1472",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1472?version=1",
        "name": "WGS Part In \"Analyses Of Metagenomics Data - The Global Picture\"",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "metaphlan2",
            "humann2_regroup_table",
            "taxonomy_krona_chart",
            "humann2",
            "metaphlan2krona",
            "humann2_renorm_table"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "encyclopedia- DIA Metaproteomics\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [EncyclopeDIA](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/encyclopedia/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: GalaxyP\n\n**Tutorial Author(s)**: [Emma Leith](https://training.galaxyproject.org/training-material/hall-of-fame/emmaleith/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/), [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1471",
        "keep": "Reject",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1471?version=1",
        "name": "EncyclopeDIA-GTN",
        "number_of_steps": 4,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "msconvert",
            "encyclopedia_quantify",
            "encyclopedia_searchtolib"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow performs taxonomic profiling of metagenomic data and visualizes microbial community composition using Kraken2 and Bracken as well as MetaPhlAn.\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Taxonomic Profiling and Visualization of Metagenomic Data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/taxonomic-profiling/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Tarnima Omara\n\n**Tutorial Author(s)**: [Sophia Hampe](https://training.galaxyproject.org/training-material/hall-of-fame/sophia120199/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [Tarnima Omara](https://training.galaxyproject.org/training-material/hall-of-fame/Tarnima-Omara/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1470",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1470?version=1",
        "name": "Taxonomic Profiling and Visualization of Metagenomic Data",
        "number_of_steps": 10,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "__UNZIP_COLLECTION__",
            "krakentools_kreport2krona",
            "metaphlan",
            "interactive_tool_phinch",
            "taxonomy_krona_chart",
            "kraken2",
            "interactive_tool_pavian",
            "kraken_biom",
            "est_abundance"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Quantification using the MaxQuant tool\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 4: Quantitation](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/clinical-mp-4-quantitation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1468",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1468?version=1",
        "name": "WF4_Quantitation_Workflow",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "maxquant",
            "Grep1",
            "Cut1",
            "Grouping1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "This workflow query metagenomic raw data against a metaplasmidome database to identify plasmids and annotate them with genes, KO, PFAM\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Query an annotated mobile genetic element database to identify and annotate genetic elements (e.g. plasmids) in metagenomics data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/metaplasmidome_query/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Nadia Gou\u00e9\n\n**Tutorial Author(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Nadia Gou\u00e9](https://training.galaxyproject.org/training-material/hall-of-fame/nagoue/), [Didier Debroas](https://training.galaxyproject.org/training-material/hall-of-fame/debroas/)\n\n**Tutorial Contributor(s)**: [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1469",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1469?version=1",
        "name": "Query a metaplasmidome database to identify and annotate plasmids in metagenomes",
        "number_of_steps": 47,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "metagenomics",
            "metaplasmidome",
            "name:microgalaxy"
        ],
        "tools": [
            "CONVERTER_fasta_to_tabular",
            "Add_a_column1",
            "tab2fasta",
            "minimap2",
            "Cut1",
            "Filter1",
            "join1",
            "Grouping1",
            "histogram_rpy",
            "add_column_headers",
            "MQoutputfilter",
            "count_gff_features",
            "tp_replace_in_column",
            "tp_tail_tool",
            "cat1",
            "sort1",
            "tp_sorted_uniq",
            "ggplot2_histogram"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (extended)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (extended)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1465",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1465?version=1",
        "name": "Training: 16S rRNA Sequencing With Mothur: Main Tutorial",
        "number_of_steps": 38,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_summary_single",
            "mothur_filter_seqs",
            "mothur_make_biom",
            "mothur_seq_error",
            "mothur_cluster",
            "mothur_classify_otu",
            "mothur_cluster_split",
            "mothur_screen_seqs",
            "mothur_chimera_vsearch",
            "mothur_remove_lineage",
            "mothur_rarefaction_single",
            "mothur_venn",
            "mothur_heatmap_sim",
            "mothur_sub_sample",
            "mothur_make_contigs",
            "taxonomy_krona_chart",
            "mothur_pre_cluster",
            "mothur_taxonomy_to_krona",
            "mothur_unique_seqs",
            "mothur_count_groups",
            "mothur_align_seqs",
            "mothur_dist_shared",
            "mothur_summary_seqs",
            "newick_display",
            "mothur_make_shared",
            "mothur_classify_seqs",
            "mothur_remove_seqs",
            "mothur_dist_seqs",
            "mothur_count_seqs",
            "mothur_tree_shared",
            "mothur_remove_groups",
            "mothur_get_groups",
            "XY_Plot_1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "WF3- Peptide verification/validaiton workflow\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 3: Verification](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/clinical-mp-3-verification/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1464",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1464?version=1",
        "name": "WF3_VERIFICATION_WORKFLOW",
        "number_of_steps": 19,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "dbbuilder",
            "uniprotxml_downloader",
            "query_tabular",
            "Cut1",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "collapse_dataset",
            "pepquery2",
            "tp_cat",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Generating a large database and then reducing it to a compact database using Metanovo\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 1: Database-Generation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-1-database-generation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1461",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1461?version=1",
        "name": "WF1_Database_Generation_Workflow",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "metanovo",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Workflow for running LotuS2 tool on fungal ITS paired-end sequencing data, to identify the fungi present in the samples\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identifying Mycorrhizal Fungi from ITS2 sequencing using LotuS2](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/lotus2-identifying-fungi/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Society for the Protection of Underground Networks, Sujai Kumar, Bethan Manley\n\n**Tutorial Author(s)**: [Sujai Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/sujaikumar/)\n\n**Tutorial Contributor(s)**: [Bethan Manley](https://training.galaxyproject.org/training-material/hall-of-fame/bethanmanley/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Sujai Kumar](https://training.galaxyproject.org/training-material/hall-of-fame/sujaikumar/)\n\n**Funder(s)**: [Society for the Protection of Underground Networks](https://training.galaxyproject.org/training-material/hall-of-fame/societyprotectionundergroundnetworks/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1460",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1460?version=1",
        "name": "Workflow for Identifying MF from ITS2 sequencing using LotuS2 - tutorial example run'",
        "number_of_steps": 1,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "ecology",
            "fungi",
            "gtn",
            "galaxy",
            "lotus2",
            "metagenomics"
        ],
        "tools": [
            "lotus2"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Metaproteomics tutorial\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Metaproteomics tutorial](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/metaproteomics/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Tutorial Author(s)**: [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [James Johnson](https://training.galaxyproject.org/training-material/hall-of-fame/jj-umn/), [Clemens Blank](https://training.galaxyproject.org/training-material/hall-of-fame/blankclemens/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in name",
        "id": "1443",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1443?version=1",
        "name": "Metaproteomics_GTN",
        "number_of_steps": 10,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "proteomics"
        ],
        "tools": [
            "search_gui",
            "query_tabular",
            "unipept",
            "peptide_shaker",
            "sqlite_to_tabular"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Identification of the micro-organisms in a beer using Nanopore sequencing\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Identification of the micro-organisms in a beer using Nanopore sequencing](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/beer-data-analysis/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: B\u00e9r\u00e9nice Batut, Teresa M\u00fcller, Polina Polunina\n\n**Tutorial Author(s)**: [Polina Polunina](https://training.galaxyproject.org/training-material/hall-of-fame/plushz/), [Siyu Chen](https://training.galaxyproject.org/training-material/hall-of-fame/chensy96/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/)\n\n**Tutorial Contributor(s)**: [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/), [Siyu Chen](https://training.galaxyproject.org/training-material/hall-of-fame/chensy96/), [Nuwan Goonasekera](https://training.galaxyproject.org/training-material/hall-of-fame/nuwang/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Crist\u00f3bal Gallardo](https://training.galaxyproject.org/training-material/hall-of-fame/gallardoalba/)\n\n**Grants(s)**: [Gallantries: Bridging Training Communities in Life Science, Environment and Health](https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1439",
        "keep": "Keep",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1439?version=1",
        "name": "Identification of the micro-organisms in a beer using Nanopore sequencing",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "fastp",
            "krakentools_kreport2krona",
            "fastqc",
            "porechop",
            "Filter1",
            "taxonomy_krona_chart",
            "kraken2"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Discovery workflow with SG/PS and MaxQuant to generate microbial peptides\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 2: Discovery](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/clinical-mp-2-discovery/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1435",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1435?version=1",
        "name": "WF2_Discovery-Workflow",
        "number_of_steps": 24,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "fasta_merge_files_and_filter_unique_sequences",
            "filter_tabular",
            "search_gui",
            "query_tabular",
            "Grep1",
            "maxquant",
            "Cut1",
            "peptide_shaker",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "msconvert",
            "tp_cat",
            "ident_params",
            "fasta_cli",
            "fasta2tab",
            "dbbuilder"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Interpreting MaxQuant data using MSstats involves applying a rigorous statistical framework to glean meaningful insights from quantitative proteomic datasets\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 5: Data Interpretation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-5-data-interpretation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1433",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1433?version=1",
        "name": "WF5_Data_Interpretation_Worklow",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "msstatstmt",
            "unipept",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Calculating diversity from bracken output\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Calculating \u03b1 and \u03b2 diversity from microbiome taxonomic data](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/diversity/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n## Features\n\n* Includes [Galaxy Workflow Tests](https://training.galaxyproject.org/training-material/faqs/gtn/workflow_run_test.html)\n* Includes a [Galaxy Workflow Report](https://training.galaxyproject.org/training-material/faqs/galaxy/workflows_report_view.html)\n\n## Thanks to...\n\n**Workflow Author(s)**: Paul Zierep, Sophia Hampe, B\u00e9r\u00e9nice Batut\n\n**Tutorial Author(s)**: [Sophia Hampe](https://training.galaxyproject.org/training-material/hall-of-fame/sophia120199/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/)\n\n**Tutorial Contributor(s)**: [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Paul Zierep](https://training.galaxyproject.org/training-material/hall-of-fame/paulzierep/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Teresa M\u00fcller](https://training.galaxyproject.org/training-material/hall-of-fame/teresa-m/), [Deepti Varshney](https://training.galaxyproject.org/training-material/hall-of-fame/deeptivarshney/), [Sophia Hampe](https://training.galaxyproject.org/training-material/hall-of-fame/sophia120199/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in name",
        "id": "1431",
        "keep": "Keep",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1431?version=1",
        "name": "Calculating diversity from microbiome taxonomic data",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:gtn"
        ],
        "tools": [
            "krakentools_beta_diversity",
            "krakentools_alpha_diversity"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1428",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1428?version=1",
        "name": "Workflow 3: Classification [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_summary_seqs",
            "mothur_remove_lineage",
            "mothur_classify_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "WF3- Peptide verification/validaiton workflow\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 3: Verification](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-3-verification/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1425",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1425?version=1",
        "name": "WF3_VERIFICATION_WORKFLOW",
        "number_of_steps": 19,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "dbbuilder",
            "uniprotxml_downloader",
            "query_tabular",
            "Cut1",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "collapse_dataset",
            "pepquery2",
            "tp_cat",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1422",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1422?version=1",
        "name": "Workflow 1: Quality Control [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 5,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_summary_seqs",
            "mothur_screen_seqs",
            "mothur_count_seqs",
            "mothur_unique_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Quantification using the MaxQuant tool\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 4: Quantitation](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-4-quantitation/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1420",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1420?version=1",
        "name": "WF4_Quantitation_Workflow",
        "number_of_steps": 7,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "maxquant",
            "Grep1",
            "Cut1",
            "Grouping1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1418",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1418?version=1",
        "name": "Workflow7: Beta Diversity [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_venn",
            "mothur_heatmap_sim",
            "mothur_dist_shared",
            "collapse_dataset",
            "mothur_tree_shared",
            "newick_display"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1412",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1412?version=1",
        "name": "Workflow 6: Alpha Diversity [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 3,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_summary_single",
            "mothur_rarefaction_single",
            "XY_Plot_1"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1408",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1408?version=1",
        "name": "Workflow 4: Mock OTU Clustering [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 4,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_dist_seqs",
            "mothur_rarefaction_single",
            "mothur_make_shared",
            "mothur_cluster"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1404",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1404?version=1",
        "name": "Workflow 5: OTU Clustering [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_sub_sample",
            "mothur_count_groups",
            "mothur_remove_groups",
            "mothur_classify_otu",
            "mothur_cluster_split",
            "mothur_make_shared"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "16S Microbial Analysis with mothur (short)\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [16S Microbial Analysis with mothur (short)](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/mothur-miseq-sop-short/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Saskia Hiltemann\n\n**Tutorial Author(s)**: [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Dave Clements](https://training.galaxyproject.org/training-material/hall-of-fame/tnabtaf/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "1400",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1400?version=1",
        "name": "Workflow 2: Data Cleaning And Chimera Removal [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 9,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "microbiome"
        ],
        "tools": [
            "mothur_screen_seqs",
            "mothur_filter_seqs",
            "mothur_chimera_vsearch",
            "mothur_remove_seqs",
            "mothur_summary_seqs",
            "mothur_pre_cluster",
            "mothur_unique_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-06-02",
        "creators": [],
        "description": "Discovery workflow with SG/PS and MaxQuant to generate microbial peptides\n\n## Associated Tutorial\n\nThis workflows is part of the tutorial [Clinical Metaproteomics 2: Discovery](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-2-discovery/tutorial.html), available in the [GTN](https://training.galaxyproject.org)\n\n\n\n\n\n## Thanks to...\n\n**Workflow Author(s)**: Subina Mehta\n\n**Tutorial Author(s)**: [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/), [Katherine Do](https://training.galaxyproject.org/training-material/hall-of-fame/katherine-d21/), [Dechen Bhuming](https://training.galaxyproject.org/training-material/hall-of-fame/dechendb/)\n\n**Tutorial Contributor(s)**: [Pratik Jagtap](https://training.galaxyproject.org/training-material/hall-of-fame/pratikdjagtap/), [Timothy J. Griffin](https://training.galaxyproject.org/training-material/hall-of-fame/timothygriffin/), [B\u00e9r\u00e9nice Batut](https://training.galaxyproject.org/training-material/hall-of-fame/bebatut/), [Helena Rasche](https://training.galaxyproject.org/training-material/hall-of-fame/hexylena/), [Saskia Hiltemann](https://training.galaxyproject.org/training-material/hall-of-fame/shiltemann/), [Bj\u00f6rn Gr\u00fcning](https://training.galaxyproject.org/training-material/hall-of-fame/bgruening/), [Subina Mehta](https://training.galaxyproject.org/training-material/hall-of-fame/subinamehta/)\n\n[![gtn star logo followed by the word workflows](https://training.galaxyproject.org/training-material/assets/branding/gtn-workflows.png)](https://training.galaxyproject.org/training-material/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1396",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1396?version=1",
        "name": "WF2_Discovery-Workflow",
        "number_of_steps": 24,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gtn",
            "galaxy",
            "name:clinicalmp"
        ],
        "tools": [
            "fasta_merge_files_and_filter_unique_sequences",
            "filter_tabular",
            "search_gui",
            "query_tabular",
            "Grep1",
            "maxquant",
            "Cut1",
            "peptide_shaker",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "msconvert",
            "tp_cat",
            "ident_params",
            "fasta_cli",
            "fasta2tab",
            "dbbuilder"
        ],
        "type": "Galaxy",
        "update_time": "2025-06-02",
        "versions": 1
    },
    {
        "create_time": "2025-05-29",
        "creators": [
            "Luisa Santus",
            "Jose Espinosa Carrasco"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-multiplesequencealign_logo_dark.png\">\n    <img alt=\"nf-core/multiplesequencealign\" src=\"docs/images/nf-core-multiplesequencealign_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/multiplesequencealign/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/multiplesequencealign/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/multiplesequencealign/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/multiplesequencealign/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/multiplesequencealign/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.13889386-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.13889386)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A525.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/multiplesequencealign)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23multiplesequencealign-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/multiplesequencealign)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\nUse **nf-core/multiplesequencealign** to:\n\n1. **Deploy** one (or many) of the most popular Multiple Sequence Alignment (MSA) tools.\n2. **Benchmark** MSA tools (and their inputs) using various metrics.\n\nMain steps:\n\n  <details>\n      <summary><strong>Inputs summary</strong> (Optional)</summary>\n      <p>Computation of summary statistics on the input files (e.g., average sequence similarity across the input sequences, their length, pLDDT extraction if available).</p>\n  </details>\n\n  <details>\n      <summary><strong>Guide Tree</strong> (Optional)</summary>\n      <p>Renders a guide tree with a chosen tool (list available in <a href=\"https://nf-co.re/multiplesequencealign/usage#2-guide-trees\">usage</a>). Some aligners use guide trees to define the order in which the sequences are aligned.</p>\n  </details>\n\n  <details>\n      <summary><strong>Align</strong> (Required)</summary>\n      <p>Aligns the sequences with a chosen tool (list available in <a href=\"https://nf-co.re/multiplesequencealign/usage#3-align\">usage</a>).</p>\n  </details>\n\n  <details>\n      <summary><strong>Evaluate</strong> (Optional)</summary>\n      <p>Evaluates the generated alignments with different metrics: Sum Of Pairs (SoP), Total Column score (TC), iRMSD, Total Consistency Score (TCS), etc.</p>\n  </details>\n\n  <details>\n      <summary><strong>Report</strong>(Optional)</summary>\n      <p>Reports the collected information of the runs in a Shiny app and a summary table in MultiQC. Optionally, it can also render the <a href=\"https://github.com/steineggerlab/foldmason\">Foldmason</a> MSA visualization in HTML format.</p>\n  </details>\n\n<br>\n\nMore introductory material: [bytesize talk](https://youtu.be/iRY-Y1p5gtc), [nextflow summit talk](https://www.youtube.com/watch?v=suNulysHIN0) from the nextlow summit, [poster](https://github.com/nf-core/multiplesequencealign/blob/dev/docs/images/poster-nf-msa.pdf).\n\n![Alt text](docs/images/nf-core-msa_metro_map.png?raw=true \"nf-core-msa metro map\")\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n### Quick start - test run\n\nTo get a feeling of what the pipeline does, run:\n\n(You don't need to download or provide any file, try it!)\n\n```\nnextflow run nf-core/multiplesequencealign \\\n   -profile test_tiny,docker \\\n   --outdir results\n```\n\nand if you want to see how a more complete run looks like, you can try:\n\n```\nnextflow run nf-core/multiplesequencealign \\\n   -profile test,docker \\\n   --outdir results\n```\n\n## How to set up an easy run:\n\n> [!NOTE]\n> We have a lot more of use cases examples under [FAQs](\"https://nf-co.re/multiplesequencealign/usage/FAQs)\n\n### Input data\n\nYou can provide either (or both) a **fasta** file or a set of **protein structures**.\n\nAlternatively, you can provide a [samplesheet](https://nf-co.re/multiplesequencealign/usage/#samplesheet-input) and a [toolsheet](https://nf-co.re/multiplesequencealign/usage/#toolsheet-input).\n\nSee below how to provide them.\n\n> Find some example input data [here](https://github.com/nf-core/test-datasets/tree/multiplesequencealign)\n\n### CASE 1: One input dataset, one tool.\n\nIf you only have one dataset and want to align it using one specific MSA tool (e.g. FAMSA or FOLDMASON), you can run the pipeline with one single command.\n\nIs your input a fasta file ([example](https://github.com/nf-core/test-datasets/blob/multiplesequencealign/testdata/setoxin-ref.fa))? Then:\n\n```bash\nnextflow run nf-core/multiplesequencealign \\\n   -profile easy_deploy,docker \\\n   --seqs <YOUR_FASTA.fa> \\\n   --aligner FAMSA \\\n   --outdir outdir\n```\n\nIs your input a directory where your PDB files are stored ([example](https://github.com/nf-core/test-datasets/blob/multiplesequencealign/testdata/af2_structures/seatoxin-ref.tar.gz))? Then:\n\n```bash\nnextflow run nf-core/multiplesequencealign \\\n   -profile easy_deploy,docker \\\n   --pdbs_dir <PATH_TO_YOUR_PDB_DIR> \\\n   --aligner FOLDMASON \\\n   --outdir outdir\n```\n\n<details>\n  <summary> FAQ: Which are the available tools I can use?</summary>\n  Check the list here: <a href=\"https://nf-co.re/multiplesequencealign/usage/#3-align\"> available tools</a>.\n</details>\n\n<details>\n  <summary> FAQ: Can I use both <em>--seqs</em> and <em>--pdbs_dir</em>?</summary>\n  Yes, go for it! This might be useful if you want a structural evaluation of a sequence-based aligner for instance.\n</details>\n\n<details>\n  <summary> FAQ: Can I specify also which guidetree to use? </summary>\n  Yes, use the <code>--tree</code> flag. More info: <a href=\"https://nf-co.re/multiplesequencealign/usage\">usage</a> and <a href=\"https://nf-co.re/multiplesequencealign/parameters\">parameters</a>.\n</details>\n\n<details>\n  <summary> FAQ: Can I specify the arguments of the tools (tree and aligner)? </summary>\n  Yes, use the <code>--args_tree</code> and <code>--args_aligner</code> flags. More info: <a href=\"https://nf-co.re/multiplesequencealign/usage\">usage</a> and <a href=\"https://nf-co.re/multiplesequencealign/parameters\">parameters</a>.\n</details>\n\n### CASE 2: Multiple datasets, multiple tools.\n\n```bash\nnextflow run nf-core/multiplesequencealign \\\n   -profile test,docker \\\n   --input <samplesheet.csv> \\\n   --tools <toolsheet.csv> \\\n   --outdir outdir\n```\n\nYou need **2 input files**:\n\n- **samplesheet** (your datasets)\n- **toolsheet** (which tools you want to use).\n\n<details>\n  <summary> What is a samplesheet?</summary>\n  The sample sheet defines the <b>input datasets</b> (sequences, structures, etc.) that the pipeline will process.\n\nA minimal version:\n\n```csv\nid,fasta\nseatoxin,seatoxin.fa\ntoxin,toxin.fa\n```\n\nA more complete one:\n\n```csv\nid,fasta,reference,optional_data\nseatoxin,seatoxin.fa,seatoxin-ref.fa,seatoxin_structures\ntoxin,toxin.fa,toxin-ref.fa,toxin_structures\n```\n\nEach row represents a set of sequences (in this case the seatoxin and toxin protein families) to be aligned and the associated (if available) reference alignments and dependency files (this can be anything from protein structure or any other information you would want to use in your favourite MSA tool).\n\nPlease check: <a href=\"https://nf-co.re/multiplesequencealign/usage/#samplesheet-input\">usage</a>.\n\n> [!NOTE]\n> The only required input is the id column and either fasta or optional_data.\n\n</details>\n\n<details>\n  <summary> What is a toolsheet?</summary>\n  The toolsheet specifies <em>which combination of tools will be deployed and benchmarked in the pipeline</em>.\n\nEach line defines a combination of guide tree and multiple sequence aligner to run with the respective arguments to be used.\n\nThe only required field is `aligner`. The fields `tree`, `args_tree` and `args_aligner` are optional and can be left empty.\n\nA minimal version:\n\n```csv\ntree,args_tree,aligner,args_aligner\n,,FAMSA,\n```\n\nThis will run the FAMSA aligner.\n\nA more complex one:\n\n```csv\ntree,args_tree,aligner,args_aligner\nFAMSA, -gt upgma -medoidtree, FAMSA,\n, ,TCOFFEE,\nFAMSA,,REGRESSIVE,\n```\n\nThis will run, in parallel:\n\n- the FAMSA guidetree with the arguments <em>-gt upgma -medoidtree</em>. This guidetree is then used as input for the FAMSA aligner.\n- the TCOFFEE aligner\n- the FAMSA guidetree with default arguments. This guidetree is then used as input for the REGRESSIVE aligner.\n\nPlease check: <a href=\"https://nf-co.re/multiplesequencealign/usage/#toolsheet-input\">usage</a>.\n\n> [!NOTE]\n> The only required input is `aligner`.\n\n</details>\n\nFor more details on more advanced runs: [usage documentation](https://nf-co.re/multiplesequencealign/usage) and the [parameter documentation](https://nf-co.re/multiplesequencealign/parameters).\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\n## Pipeline resources\n\nWhich resources is the pipeline using? You can find the default resources used in [base.config](conf/base.config).\n\nIf you are using specific profiles, e.g. [test](conf/test.config), these will overwrite the defaults.\n\nIf you want to modify the needed resources, please refer [usage](https://nf-co.re/multiplesequencealign/docs/usage/#custom-configuration).\n\n## Pipeline output\n\nExample results: [results](https://nf-co.re/multiplesequencealign/results) tab on the nf-core website pipeline page.\nFor more details: [output documentation](https://nf-co.re/multiplesequencealign/output).\n\n## Extending the pipeline\n\nFor details on how to add your favourite guide tree, MSA or evaluation step in nf-core/multiplesequencealign please refer to the [extending documentation](https://nf-co.re/multiplesequencealign/usage/adding_a_tool).\n\n## Credits\n\nnf-core/multiplesequencealign was originally written by Luisa Santus ([@luisas](https://github.com/luisas)) and Jose Espinosa-Carrasco ([@JoseEspinosa](https://github.com/JoseEspinosa)) from The Comparative Bioinformatics Group at The Centre for Genomic Regulation, Spain.\n\nThe following people have significantly contributed to the development of the pipeline and its modules: Leon Rauschning ([@lrauschning](https://github.com/lrauschning)), Alessio Vignoli ([@alessiovignoli](https://github.com/alessiovignoli)), Igor Trujnara ([@itrujnara](https://github.com/itrujnara)) and Leila Mansouri ([@l-mansouri](https://github.com/l-mansouri)).\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#multiplesequencealign` channel](https://nfcore.slack.com/channels/multiplesequencealign) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/multiplesequencealign for your analysis, please cite it using the following doi: [10.5281/zenodo.13889386](https://doi.org/10.5281/zenodo.13889386)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1178",
        "keep": "To Curate",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1178?version=3",
        "name": "nf-core/multiplesequencealign",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-29",
        "versions": 3
    },
    {
        "create_time": "2025-04-17",
        "creators": [
            "Dan Parsons",
            "Ben Price"
        ],
        "description": "<div align=\"center\">\r\n    <img src=\"./Gene_fetch_logo.svg\" width=\"300\" alt=\"Gene Fetch Logo\">\r\n</div>\r\n\r\n\r\n# Gene_fetch \r\nThis tool fetches gene sequences from NCBI databases based on taxonomy IDs (taxids) or taxonomic information. It can retrieve both protein and nucleotide sequences for various genes, including protein-coding genes (e.g., cox1, cytb, rbcl, matk) and rRNA genes (e.g., 16S, 18S).\r\n\r\n\r\n## Feature highlight\r\n- Fetch protein and/or nucleotide sequences from NCBI GenBank database.\r\n- Handles both direct nucleotide sequences and protein-linked nucleotide searches (CDS extraction includes fallback mechanisms for atypical annotation formats).\r\n.\r\n- Support for both protein-coding and rDNA genes.\r\n- Single-taxid mode (-s/--single) for retrieving a specified number of target sequences for a particular taxon (default length thresholds are reduced (protein: 50aa, nucleotide: 100bp)).\r\n- Customisable length filtering thresholds for protein and nucleotide sequences.\r\n- Automatic taxonomy traversal: Uses fetched NCBI taxonomic lineage for a given taxid when sequences are not found at the input taxonomic level. I.e., Search at given taxid level (e.g., species), if no sequences are found, escalate species->phylum until a suitable sequence is found.\r\n- Validates fetched sequence using higher taxonomy, avoiding potential taxonomic homonyms.\r\n- Robust error handling, error and progress logging, and NCBI API rate limits (10 requests/second).\r\n- Handles complex sequence features (e.g., complement strands, joined sequences, WGS entries) in addition to 'simple' cds extaction (if --type nucleotide/both). The tool avoids \"unverified\" sequences and WGS entries not containing sequence data (i.e. master records).\r\n- 'Checkpointing': if a run fails/crashes, the script can be rerun using the same arguments and it will resume from where it stopped.\r\n- When more than 50 matching sequences are found for a sample, the tool fetches summary information for all matches (using NCBI esummary API), orders them by length, and processes the top 10 longest sequences.\r\n\r\n## Contents\r\n - [Installation](#installation)\r\n - [Usage](#usage)\r\n - [Examples](#Examples)\r\n - [Input](#input)\r\n - [Output](#output)\r\n - [Cluster](#running-gene_fetch-on-a-cluster)\r\n - [Supported targets](#supported-targets)\r\n - [Notes](#notes)\r\n - [Benchmarking](#benchmarking)\r\n - [Future developments](#future-developments)\r\n - [Contributions and citation](#contributions-and-citations)\r\n\r\n\r\n## Installation\r\nFirst, clone the Gene Fetch GitHub repository to your current path, and enter the Gene Fetch installation directory \r\n```bash\r\ngit clone https://github.com/bge-barcoding/gene_fetch\r\n\r\ncd gene_fetch\r\n```\r\nRun the commands below to install the necessary dependencies and activate the Conda environment. [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) must be installed.\r\n```bash\r\nconda env create -n fetch -f fetch.yaml\r\n\r\nconda activate fetch\r\n```\r\nAlternatively, you can install the dependencies below directly or in your own Conda environment\r\n```\r\nconda install python>=3.9 pip\r\npip install ratelimit>=2.2.1\r\npip install biopython>=1.80\r\n```\r\n\r\n## Usage\r\n```bash\r\npython gene_fetch.py -g/--gene <gene_name> --type <sequence_type> -i/--in <samples.csv> -o/--out <output_directory> \r\n```\r\n* `--h/--help`: Show help and exit.\r\n### Required arguments\r\n* `-g/--gene`: Name of gene to search for in NCBI GenBank database (e.g., cox1/16s/rbcl).\r\n* `--type`: Sequence type to fetch; 'protein', 'nucleotide', or 'both' ('both' will initially search and fetch a protein sequence, and then fetches the corresponding nucleotide CDS for that protein sequence).\r\n* `-i/--in`: Path to input CSV file containing sample IDs and TaxIDs (see [Input](#input) section below).\r\n* `i2/--in2`: Path to alternative input CSV file containing sample IDs and taxonomic information for each sample (see [Input](#input) section below).\r\n* `o/--out`: Path to output directory. The directory will be created if it does not exist.\r\n* `e/--email` and `-k/--api-key`: Email address and associated API key for NCBI account. An NCBI account is required to run this tool (due to otherwise strict API limitations) - information on how to create an NCBI account and find your API key can be found [here](https://support.nlm.nih.gov/kbArticle/?pn=KA-05317).\r\n####= Optional arguments\r\n* `--protein_size`: Minimum protein sequence length filter. Applicable to mode 'normal' and 'single-taxid' search modes (default: 500).\r\n* `--nucleotide_size`: Minimum nucleotide sequence length filter. Applicable to mode 'normal' and 'single-taxid' search modes (default: 1500).\r\n* `s/--single`: Taxonomic ID for 'single-taxid' sequence search mode (`-i` and `-i2` ignored when run with `-s` mode). 'Single-taxid' mode will fetch all target gene or protein sequences on GenBank for a specific taxonomic ID.\r\n* `--max-sequences`: Maximum number of sequences to fetch for a specific taxonomic ID (only applies when run in 'single-taxid' mode).\r\n\r\n\r\n## Examples\r\nFetch both protein and nucleotide sequences for COI with default sequence length thresholds.\r\n```\r\npython gene_fetch.py -e your.email@domain.com -k your_api_key \\\r\n                    -g cox1 -o ./output_dir -i ./samples.csv \\\r\n                    --type both\r\n```\r\n\r\nFetch rbcL nucleotide sequences using sample taxonomic information, applying a minimum nucleotide sequence length of 1000bp\r\n```\r\npython gene_fetch.py -e your.email@domain.com -k your_api_key \\\r\n                    -g rbcl -o ./output_dir -i2 ./taxonomy.csv \\\r\n                    --type nucleotide --nucleotide_size 1000\r\n```\r\n\r\nRetrieve 1000 available matK protein sequences >400aa for _Arabidopsis thaliana_ (taxid: 3702).\r\n```\r\npython gene_fetch.py -e your.email@domain.com -k your_api_key \\\r\n                    -g matk -o ./output_dir -s 3702 \\\r\n                    --type protein --protein_size 400 --max-sequences 1000\r\n```\r\n\r\n\r\n## Input\r\n**Example 'samples.csv' input file (-i/--in)**\r\n| ID | taxid |\r\n| --- | --- |\r\n| sample-1  | 177658 |\r\n| sample-2 | 177627 |\r\n| sample-3 | 3084599 |\r\n\r\n**Example 'samples_taxonomy.csv' input file (-i2/--in2)**\r\n| ID | phylum | class | order | family | genus | species |\r\n| --- | --- | --- | --- | --- | --- | --- |\r\n| sample-1  | Arthropoda | Insecta | Diptera | Acroceridae | Astomella | Astomella hispaniae |\r\n| sample-2 | Arthropoda | Insecta | Hemiptera | Cicadellidae | Psammotettix | Psammotettix sabulicola |\r\n| sample-3 | Arthropoda | Insecta | Trichoptera | Limnephilidae | Dicosmoecus | Dicosmoecus palatus |\r\n\r\n\r\n## Output\r\n### 'Normal' mode\r\n```\r\noutput_dir/\r\n\u251c\u2500\u2500 nucleotide/                 # Nucleotide sequences. Only populated if '--type nucleotide/both' utilised.\r\n\u2502   \u251c\u2500\u2500 sample-1_dna.fasta   \r\n\u2502   \u251c\u2500\u2500 sample-2_dna.fasta\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 sample-1.fasta              # Protein sequences.\r\n\u251c\u2500\u2500 sample-2.fasta\r\n\u251c\u2500\u2500 sequence_references.csv     # Sequence metadata.\r\n\u251c\u2500\u2500 failed_searches.csv         # Failed search attempts (if any).\r\n\u2514\u2500\u2500 gene_fetch.log              # Log.\r\n```\r\n\r\n**sequence_references.csv output example**\r\n| ID | taxid | protein_accession | protein_length | nucleotide_accession | nucleotide_length | matched_rank | ncbi_taxonomy | reference_name | protein_reference_path | nucleotide_reference_path |\r\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\r\n| sample-1 | 177658 | AHF21732.1 | 510 | KF756944.1 | 1530 | genus:Apatania | Eukaryota; ...; Apataniinae; Apatania | sample-1 | abs/path/to/protein_references/sample-1.fasta | abs/path/to/protein_references/sample-1_dna.fasta |\r\n| sample-2 | 2719103 | QNE85983.1 | 518 | MT410852.1 | 1557 | species:Isoptena serricornis | Eukaryota; ...; Chloroperlinae; Isoptena | sample-2 | abs/path/to/protein_references/sample-2.fasta | abs/path/to/protein_references/sample-2_dna.fasta |\r\n| sample-3 | 1876143 | YP_009526503.1 | 512 | NC_039659.1 | 1539 | genus:Triaenodes | Eukaryota; ...; Triaenodini; Triaenodes | sample-3 | abs/path/to/protein_references/sample-3.fasta | abs/path/to/protein_references/sample-3_dna.fasta |\r\n\r\n\r\n### 'Single-taxid' mode\r\n```\r\noutput_dir/\r\n\u251c\u2500\u2500 nucleotide/                      # Nucleotide sequences. Only populated if '--type nucleotide/both' utilised.\r\n\u2502   \u251c\u2500\u2500 ACCESSION1_dna.fasta   \r\n\u2502   \u251c\u2500\u2500 ACCESSION2_dna.fasta\r\n\u2502   \u2514\u2500\u2500 ...\r\n\u251c\u2500\u2500 ACCESSION1.fasta                 # Protein sequences.\r\n\u251c\u2500\u2500 ACCESSION2.fasta\r\n\u251c\u2500\u2500 fetched_nucleotide_sequences.csv # Only populated if '--type nucleotide/both' utilised. Sequence metadata.\r\n\u251c\u2500\u2500 fetched_protein_sequences.csv    # Only populated if '--type protein/both' utilised. Sequence metadata.\r\n\u251c\u2500\u2500 failed_searches.csv              # Failed search attempts (if any).\r\n\u2514\u2500\u2500 gene_fetch.log                   # Log.\r\n```\r\n\r\n**fetched_protein|nucleotide_sequences.csv output example**\r\n| ID | taxid | Description |\r\n| --- | --- | --- |\r\n| PQ645072.1 | 1501 | Ochlerotatus nigripes isolate Pool11 cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial |\r\n| PQ645071.1 | 1537 | Ochlerotatus nigripes isolate Pool10 cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial |\r\n| PQ645070.1 | 1501 | Ochlerotatus impiger isolate Pool2 cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial |\r\n| PQ645069.1 | 1518\t| Ochlerotatus impiger isolate Pool1 cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial |\r\n| PP355486.1 | 581 | Aedes scutellaris isolate NC.033 cytochrome c oxidase subunit I (COX1) gene, partial cds; mitochondrial |\r\n\r\n\r\n## Running gene_fetch on a cluster\r\n- See '1_gene_fetch.sh' for running gene_fetch.py on a HPC cluster (SLURM job schedular). \r\n- Edit 'mem' and/or 'cpus-per-task' to set memory and CPU/threads allocation.\r\n- Change paths and variables as needed.\r\n- Run '1_gene_fetch.sh' with:\r\n```\r\nsbatch 1_gene_fetch.sh\r\n```\r\n\r\n## Supported targets\r\nGene Fetch does function with other targets than those listed below, but it has hard-coded name variations and 'smarter' searching for the below targets. More targets can be added into script (see 'class config').\r\n- cox1/COI/cytochrome c oxidase subunit I\r\n- cox2/COII/cytochrome c oxidase subunit II\r\n- cox3/COIIIcytochrome c oxidase subunit III\r\n- cytb/cob/cytochrome b\r\n- nd1/NAD1/NADH dehydrogenase subunit 1\r\n- nd2/NAD2/NADH dehydrogenase subunit 2\r\n- rbcL/RuBisCO/ribulose-1,5-bisphosphate carboxylase/oxygenase large subunit\r\n- matK/maturase K/maturase type II intron splicing factor\r\n- 16S ribosomal RNA/16s\r\n- SSU/18s\r\n- LSU/28s\r\n- 12S ribosomal RNA/12s\r\n- ITS (ITS1-5.8S-ITS2)\r\n- ITS1/internal transcribed spacer 1\r\n- ITS2/internal transcribed spacer 2\r\n- tRNA-Leucine/trnL\r\n\r\n\r\n## Benchmarking\r\n| Sample Description | Run Mode | Target | Input File | Data Type | Memory | CPUs | Run Time |\r\n|--------------------|----------|--------|------------|-----------|--------|------|----------|\r\n| 570 Arthropod samples | Normal | COX1 | taxonomy.csv | Both | 10G | 18 | 02:51:06 |\r\n| 570 Arthropod samples | Normal | COX1 | samples.csv | Nucleotide | 5G | 4 | 02:04:01 |\r\n| 570 Arthropod samples | Normal | COX1 | samples.csv | Protein | 5G | 4 | 01:50:31 |\r\n| 570 Arthropod samples | Normal | 18S | samples.csv | Nucleotide | 10G | 8 | 01:38:16 |\r\n| 570 Arthropod samples | Normal | ND1 | samples.csv | Nucleotide | 10G | 4 | 01:58:35 |\r\n| All (159) _A. thaliana_ sequences >300aa | Single-taxid | rbcL | N/A | Protein | 5G | 1 | 00:02:39 |\r\n| 1000 Culicidae sequences >500bp | Single-taxid | COX1 | N/A | nucleotide | 20G | 16 | 00:30:36 |\r\n| 1000 _M. tubercolisis_ sequences | Single-taxid | 16S | N/A | nucleotide | 20G | 16 | 00:10:33 |\r\n\r\n## Future Development\r\n- Add optional alignment of retrieved sequences\r\n- Add support for direct GenBank submission format output\r\n- Enhance LRU caching for taxonomy lookups to reduce API calls\r\n- Further improve efficiency of record searching and selecting the longest sequence\r\n- Add support for additional genetic markers beyond the currently supported set\r\n\r\n\r\n## Contributions and citations\r\nGeneFetch was written by Dan Parsons & Ben Price @ NHMUK (2024).\r\n\r\nIf you use GeneFetch, please cite our publication: **XYZ()**\r\n\r\nIf you have any questions or suggested improvements, please do get in touch in the issues!\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "16S in description",
        "id": "1342",
        "keep": "Reject",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1342?version=2",
        "name": "Gene Fetch",
        "number_of_steps": 0,
        "projects": [
            "iBOL Europe Museum Skimming",
            "Biodiversity Genomics Europe (general)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bge",
            "bioinformatics"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2025-06-01",
        "versions": 2
    },
    {
        "create_time": "2025-05-24",
        "creators": [
            "Leon Bichmann"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-mhcquant_logo_dark.png\">\n    <img alt=\"nf-core/mhcquant\" src=\"docs/images/nf-core-mhcquant_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/mhcquant/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/mhcquant/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/mhcquant/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/mhcquant/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/mhcquant/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.8427707-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.8427707)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/mhcquant)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23mhcquant-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/mhcquant)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/mhcquant** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/mhcquant \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/mhcquant/usage) and the [parameter documentation](https://nf-co.re/mhcquant/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/mhcquant/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/mhcquant/output).\n\n## Credits\n\nnf-core/mhcquant was originally written by Jonas Scheid, Steffen Lemke, Leon Bichmann, Marissa Dubbelaar.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#mhcquant` channel](https://nfcore.slack.com/channels/mhcquant) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/mhcquant for your analysis, please cite it using the following doi: [10.5281/zenodo.8427707](https://doi.org/10.5281/zenodo.8427707) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "999",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/999?version=24",
        "name": "nf-core/mhcquant",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dda",
            "immunopeptidomics",
            "mass-spectrometry",
            "mhc",
            "openms",
            "peptides"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-24",
        "versions": 24
    },
    {
        "create_time": "2025-05-24",
        "creators": [
            "Friederike Hanssen"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-bamtofastq_logo_dark.png\">\n    <img alt=\"nf-core/bamtofastq\" src=\"docs/images/nf-core-bamtofastq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/bamtofastq/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/bamtofastq/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/bamtofastq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/bamtofastq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/bamtofastq/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.4710628-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.4710628)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/bamtofastq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23bamtofastq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/bamtofastq)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/bamtofastq** is a bioinformatics best-practice analysis pipeline that converts (un)mapped `.bam` or `.cram` files into `fq.gz` files. Initially, it auto-detects, whether the input file contains single-end or paired-end reads. Following this step, the reads are sorted using `samtools collate` and extracted with `samtools fastq`. Furthermore, for mapped bam/cram files it is possible to only convert reads mapping to a specific region or chromosome. The obtained FastQ files can then be used to further process with other pipelines.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources.The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/bamtofastq/results).\n\n## Pipeline summary\n\nBy default, the pipeline currently performs the following steps:\n\n1. Quality control (QC) of input (bam/cram) files ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)).\n2. Check if input files are single- or paired-end ([`Samtools`](https://www.htslib.org/)).\n3. Compute statistics on input files ([`Samtools`](https://www.htslib.org/)).\n4. Convert to fastq reads ([`Samtools`](https://www.htslib.org/)).\n5. QC of converted fastq reads ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)).\n6. Checking whether the produced fastq files are valid ([fastq_utils](https://github.com/nunofonseca/fastq_utils)).\n7. Summarize QC and statistics before and after format conversion ([`MultiQC`](http://multiqc.info/)).\n\n<p align=\"center\">\n    <img title=\"Bamtofastq Workflow\" src=\"docs/images/nf-core-bamtofastq-subway.png\" width=60%>\n</p>\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nDownload the pipeline and test it on a minimal dataset with a single command:\n\n```bash\nnextflow run nf-core/bamtofastq -profile test,<docker/singularity/.../institute> --outdir './results'\n```\n\nTo run your own analysis, start by preparing a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample_id,mapped,index,file_type\ntest,test1.bam,test1.bam.bai,bam\ntest2,test2.bam,test2.bam.bai,bam\n```\n\nEach row represents a bam/cram file with or without indices.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/bamtofastq \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/bamtofastq/usage) and the [parameter documentation](https://nf-co.re/bamtofastq/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/bamtofastq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/bamtofastq/output).\n\n## Credits\n\nnf-core/bamtofastq was originally written by Friederike Hanssen. It was ported to DSL2 by Susanne Jodoin.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Gisela Gabernet](https://github.com/ggabernet)\n- [Matilda \u00c5slin](https://github.com/matrulda)\n- [Bruno Grande](https://github.com/BrunoGrandePhd)\n\n### Resources\n\nThe individual steps of this pipeline are based of on the following tutorials and resources:\n\n1.  [Extracting paired FASTQ read data from a BAM mapping file](http://darencard.net/blog/2017-09-07-extract-fastq-bam/)\n2.  [Check if BAM is derived from pair-end or single-end reads](https://www.biostars.org/p/178730/)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#bamtofastq` channel](https://nfcore.slack.com/channels/bamtofastq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/bamtofastq for your analysis, please cite it using the following doi: [10.5281/zenodo.4710628](https://doi.org/10.5281/zenodo.4710628)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "968",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/968?version=7",
        "name": "nf-core/bamtofastq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "conversion",
            "bamtofastq",
            "cramtofastq"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-24",
        "versions": 7
    },
    {
        "create_time": "2025-05-23",
        "creators": [
            "No author provided",
            "Hadrien Gourl\u00e9",
            "Daniel Straub",
            "Sabrina Krakau"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/mag_logo_mascot_dark.png\">\n    <img alt=\"nf-core/mag\" src=\"docs/images/mag_logo_mascot_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/mag/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/mag/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/mag/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/mag/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/mag/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.3589527-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.3589527)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n[![Cite Publication](https://img.shields.io/badge/Cite%20Us!-Cite%20Publication-orange)](https://doi.org/10.1093/nargab/lqac007)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.10.0-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/mag)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23mag-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/mag)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/mag** is a bioinformatics best-practise analysis pipeline for assembly, binning and annotation of metagenomes.\n\n<p align=\"center\">\n    <img src=\"docs/images/mag_workflow.png\" alt=\"nf-core/mag workflow overview\" width=\"90%\">\n</p>\n\n## Pipeline summary\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nBy default, the pipeline currently performs the following: it supports both short and long reads, quality trims the reads and adapters with [fastp](https://github.com/OpenGene/fastp), [AdapterRemoval](https://github.com/MikkelSchubert/adapterremoval), or [trimmomatic](https://github.com/usadellab/Trimmomatic) and [Porechop](https://github.com/rrwick/Porechop), and performs basic QC with [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/), and merges multiple sequencing runs.\n\nThe pipeline then:\n\n- assigns taxonomy to reads using [Centrifuge](https://ccb.jhu.edu/software/centrifuge/) and/or [Kraken2](https://github.com/DerrickWood/kraken2/wiki)\n- performs assembly using [MEGAHIT](https://github.com/voutcn/megahit) and [SPAdes](http://cab.spbu.ru/software/spades/), and checks their quality using [Quast](http://quast.sourceforge.net/quast)\n- (optionally) performs ancient DNA assembly validation using [PyDamage](https://github.com/maxibor/pydamage) and contig consensus sequence recalling with [Freebayes](https://github.com/freebayes/freebayes) and [BCFtools](http://samtools.github.io/bcftools/bcftools.html)\n- predicts protein-coding genes for the assemblies using [Prodigal](https://github.com/hyattpd/Prodigal), and bins with [Prokka](https://github.com/tseemann/prokka) and optionally [MetaEuk](https://www.google.com/search?channel=fs&client=ubuntu-sn&q=MetaEuk)\n- performs metagenome binning using [MetaBAT2](https://bitbucket.org/berkeleylab/metabat/src/master/), [MaxBin2](https://sourceforge.net/projects/maxbin2/), and/or with [CONCOCT](https://github.com/BinPro/CONCOCT), and checks the quality of the genome bins using [Busco](https://busco.ezlab.org/), [CheckM](https://ecogenomics.github.io/CheckM/), or [CheckM2](https://github.com/chklovski/CheckM2) and optionally [GUNC](https://grp-bork.embl-community.io/gunc/).\n- Performs ancient DNA validation and repair with [pyDamage](https://github.com/maxibor/pydamage) and [freebayes](https://github.com/freebayes/freebayes)\n- optionally refines bins with [DAS Tool](https://github.com/cmks/DAS_Tool)\n- assigns taxonomy to bins using [GTDB-Tk](https://github.com/Ecogenomics/GTDBTk) and/or [CAT](https://github.com/dutilh/CAT) and optionally identifies viruses in assemblies using [geNomad](https://github.com/apcamargo/genomad), or Eukaryotes with [Tiara](https://github.com/ibe-uw/tiara)\n\nFurthermore, the pipeline creates various reports in the results directory specified, including a [MultiQC](https://multiqc.info/) report summarizing some of the findings and software versions.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n```bash\nnextflow run nf-core/mag -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input '*_R{1,2}.fastq.gz' --outdir <OUTDIR>\n```\n\nor\n\n```bash\nnextflow run nf-core/mag -profile <docker/singularity/podman/shifter/charliecloud/conda/institute> --input samplesheet.csv --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/mag/usage) and the [parameter documentation](https://nf-co.re/mag/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/mag/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/mag/output).\n\n### Group-wise co-assembly and co-abundance computation\n\nEach sample has an associated group ID (see [input specifications](https://nf-co.re/mag/usage#input_specifications)). This group information can be used for group-wise co-assembly with `MEGAHIT` or `SPAdes` and/or to compute co-abundances for the binning step with `MetaBAT2`. By default, group-wise co-assembly is disabled, while the computation of group-wise co-abundances is enabled. For more information about how this group information can be used see the documentation for the parameters [`--coassemble_group`](https://nf-co.re/mag/parameters#coassemble_group) and [`--binning_map_mode`](https://nf-co.re/mag/parameters#binning_map_mode).\n\nWhen group-wise co-assembly is enabled, `SPAdes` is run on accordingly pooled read files, since `metaSPAdes` does not yet allow the input of multiple samples or libraries. In contrast, `MEGAHIT` is run for each group while supplying lists of the individual readfiles.\n\n## Credits\n\nnf-core/mag was written by [Hadrien Gourl\u00e9](https://hadriengourle.com) at [SLU](https://slu.se), [Daniel Straub](https://github.com/d4straub) and [Sabrina Krakau](https://github.com/skrakau) at the [Quantitative Biology Center (QBiC)](http://qbic.life). [James A. Fellows Yates](https://github.com/jfy133) and [Maxime Borry](https://github.com/maxibor) at the [Max Planck Institute for Evolutionary Anthropology](https://www.eva.mpg.de) joined in version 2.2.0.\n\nOther code contributors include:\n\n- [Antonia Schuster](https://github.com/AntoniaSchuster)\n- [Alexander Ramos](https://github.com/alxndrdiaz)\n- [Carson Miller](https://github.com/CarsonJM)\n- [Daniel Lundin](https://github.com/erikrikarddaniel)\n- [Danielle Callan](https://github.com/d-callan)\n- [Gregory Sprenger](https://github.com/gregorysprenger)\n- [Jim Downie](https://github.com/prototaxites)\n- [Phil Palmer](https://github.com/PhilPalmer)\n- [@willros](https://github.com/willros)\n- [Adam Rosenbaum](https://github.com/muabnezor)\n- [Diego Alvarez](https://github.com/dialvarezs)\n- [Nikolaos Vergoulidis](https://github.com/IceGreb)\n\nLong read processing was inspired by [caspargross/HybridAssembly](https://github.com/caspargross/HybridAssembly) written by Caspar Gross [@caspargross](https://github.com/caspargross)\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Alexander Peltzer](https://github.com/apeltzer)\n- [Phil Ewels](https://github.com/ewels)\n- [Gisela Gabernet](https://github.com/ggabernet)\n- [Harshil Patel](https://github.com/drpatelh)\n- [Johannes Alneberg](https://github.com/alneberg)\n- [Maxime Garcia](https://github.com/MaxUlysse)\n- [Michael L Heuer](https://github.com/heuermh)\n- [Alex H\u00fcbner](https://github.com/alexhbnr)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#mag` channel](https://nfcore.slack.com/channels/mag) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/mag for your analysis, please cite the preprint as follows:\n\n> **nf-core/mag: a best-practice pipeline for metagenome hybrid assembly and binning**\n>\n> Sabrina Krakau, Daniel Straub, Hadrien Gourl\u00e9, Gisela Gabernet, Sven Nahnsen.\n>\n> NAR Genom Bioinform. 2022 Feb 2;4(1):lqac007. doi: [10.1093/nargab/lqac007](https://doi.org/10.1093/nargab/lqac007).\n\nAdditionally you can cite the pipeline directly with the following doi: [10.5281/zenodo.3589527](https://doi.org/10.5281/zenodo.3589527)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "995",
        "keep": "To Curate",
        "latest_version": 30,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/995?version=30",
        "name": "nf-core/mag",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "assembly",
            "metagenomics",
            "binning",
            "long-read-sequencing",
            "metagenomes",
            "nanopore",
            "nanopore-sequencing"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-23",
        "versions": 30
    },
    {
        "create_time": "2025-05-23",
        "creators": [
            "Maxime Borry"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-coproid_logo_dark.png\">\n    <img alt=\"nf-core/coproid\" src=\"docs/images/nf-core-coproid_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/coproid/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/coproid/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/coproid/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/coproid/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/coproid/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/coproid)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23coproid-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/coproid)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/coproid** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/coproid \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/coproid/usage) and the [parameter documentation](https://nf-co.re/coproid/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/coproid/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/coproid/output).\n\n## Credits\n\nnf-core/coproid was originally written by Maxime Borry & Meriam Van Os.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#coproid` channel](https://nfcore.slack.com/channels/coproid) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/coproid for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in tags",
        "id": "974",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/974?version=4",
        "name": "nf-core/coproid",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "adna",
            "ancient-dna",
            "coprolite",
            "microbiome"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-23",
        "versions": 4
    },
    {
        "create_time": "2025-05-10",
        "creators": [
            "Yongyi Luo",
            "Zhen Zhang",
            "Jiandong Shi",
            "Jingyu Hao",
            "Sheng Lian",
            "Taobo Hu",
            "Toyotaka Ishibashi",
            "Depeng Wang",
            "Shu Wang",
            "Weichuan Yu",
            "Xiaodan Fan"
        ],
        "description": "# BVSim: A Benchmarking Variation Simulator Mimicking Human Variation Spectrum\r\n\r\n[![Profile views](https://komarev.com/ghpvc/?username=YongyiLuo98&repo=BVSim&label=Profile%20views&color=0e75b6&style=flat)](https://github.com/YongyiLuo98/BVSim)\r\n## Table of Contents\r\n\r\n- [Getting Started](#getting-started)\r\n- [Installation](#installation)\r\n- [General Functions and Parameters](#parameters)\r\n  - [Shared Parameters](#shared-parameters)\r\n    - [Output Naming Conventions](#output)\r\n    - [Write the Relative Positions of Simulated Variations](#write)\r\n    - [User-defined Block Regions with No Variations](#block)\r\n  - [Uniform Mode](#uniform-mode)\r\n  - [Complex SV Mode](#complex-sv-mode)\r\n    - [Parameters for CSV Mode](#parameters-for-csv-mode)\r\n  - [Uniform Parallel Mode](#uniform-parallel-mode)\r\n    - [Parameters for Uniform parallel Mode](#parameters-for-uniform-parallel-mode)\r\n  - [Wave Mode](#wave-mode)\r\n    - [User-defined Sample(s) and Input BED File Requirements](#requirements-for-the-bed-file)\r\n    - [Generate a BED File for a Single Sample](#generating-a-bed-file-for-a-single-sample-in-wave-mode)\r\n    - [Job Submission for Single Sample (BED Format)](#job-submission-for-wave-mode-single-sample)\r\n    - [Generating BED Files for Multiple Samples](#generating-bed-files-for-multiple-samples-in-wave-mode)\r\n    - [Job Submission for Multiple Samples (BED Format)](#job-submission-for-wave-mode-multiple-samples)\r\n    - [Important Note on File Placement](#important-note-on-file-placement)\r\n    - [Parameters for Wave Mode](#parameters-for-wave-mode)\r\n  - [Wave Region Mode](#wave-region-mode)\r\n    - [Extract User-defined Regions (e.g. TR region) and Generate the BED File](#step-1-extract-tr-regions)\r\n    - [Job Submission for Single Sample (BED Format)](#job-submission-for-wave-region-mode-single-sample)\r\n    - [Parameters for Wave Region Mode](#parameters-for-wave-region-mode)\r\n  - [Human Genome](#human-genome)\r\n- [Uninstallation for Updates](#uninstallation)\r\n- [Workflow of BVSim](#workflow)\r\n- [Definitions of SVs Simulated by BVSim](#definitions)\r\n\r\n## <a name=\"getting-started\"></a>Getting Started\r\n\r\nTo get started with BVSim, follow these steps to install and run the simulator:\r\n\r\n```sh\r\n# Create an envrionment called BVSim and install the dependencies\r\nconda create -n BVSim python=3.11 numpy pandas biopython scipy seaborn psutil\r\nconda activate BVSim\r\n# Run the following to install pysam or use the latest guide\r\nconda config --add channels defaults\r\nconda config --add channels conda-forge\r\nconda config --add channels bioconda\r\nconda install pysam\r\n# Installzation\r\n## Clone the repository in your home path\r\ncd your_home_path\r\ngit clone https://github.com/YongyiLuo98/BVSim.git\r\n## Navigate to the ~/BVSim/main directory and install the package\r\npip install your_home_path/BVSim/main/.\r\n\r\n# Verify the installation in your home path\r\ncd your_home_path\r\npython -m BVSim --help\r\npython -m BVSim -h\r\n\r\n## Run a toy example with a specified reference in the cloned folder\r\nconda activate BVSim\r\npython -m BVSim -ref 'your_home_path/BVSim/empirical/sub_hg19_chr1.fasta' -seed 0 -rep 0 -write -snp 2000\r\n## If you prefer using the default reference, simply execute\r\ncd your_home_path\r\npython -m BVSim\r\n\r\n\r\n# Generate variations with specific parameters\r\ncd your_home_path\r\npython -m BVSim -seed 1 -rep 1 -snp 2000\r\n\r\n# To write out the relative positions, use the following command\r\npython your_home_path/BVSim/main/write_SV.py your_home_path/BVSim/save/ BV_1_con0_chr1_SVtable.csv BV_1_con0_chr1_tem_ins_dic.npy\r\n\r\n# Create a block intervals BED file\r\ncd your_home_path\r\necho -e \"0\\t1000\\n3000\\t4000\" > block_intervals.bed\r\n\r\n# Run the simulator with block regions\r\ncd your_home_path\r\npython -m BVSim -seed 1 -rep 1 -write -snp 2000 -block_region_bed_url block_intervals.bed\r\n```\r\n\r\n## <a name=\"Installation\"></a>Installation\r\n### Create an envrionment called BVSim and install the dependencies\r\nTo start with, you need to install the dependent packages in an environment, for example called BVSim.\r\n```bash\r\n# Create an envrionment called BVSim and install the dependencies\r\nconda create -n BVSim python=3.11 numpy pandas biopython scipy seaborn psutil\r\nconda activate BVSim\r\n# Run the following to install pysam or use the latest guide\r\nconda config --add channels defaults\r\nconda config --add channels conda-forge\r\nconda config --add channels bioconda\r\nconda install pysam\r\n```\r\n### Clone the Repository\r\nNext, you need to clone the BVSim repository to your local machine. Execute the following command in your home directory:\r\n```bash\r\ncd your_home_path\r\ngit clone https://github.com/YongyiLuo98/BVSim.git\r\n```\r\n### Navigate to the Main Directory and Install the Package\r\nNext, navigate to the .../BVSim/main/ directory to install the package:\r\n```bash\r\npip install your_home_path/BVSim/main/.\r\n```\r\n### Verify the Installation\r\nAfter installation, you can verify it from your home directory. Execute the following commands:\r\n```bash\r\ncd\r\npython -m BVSim --help\r\npython -m BVSim -h\r\n```\r\nNote: You can only call BVSim in the cloned repository directory, while the installation must take place in the BVSim/main/ directory.\r\n#### Toy Example (Uniform mode):\r\n```bash\r\nconda activate BVSim\r\npython -m BVSim -ref 'your_home_path/BVSim/empirical/sub_hg19_chr1.fasta' -seed 0 -rep 0 -write -snp 2000\r\n```\r\nor you can use the default reference to test the installation by type the following in your home path. If you do not give a saving path, the outputs will go to \"your_home_path\\BVSim\\save\\\".\r\n\r\n```bash\r\ncd your_home_path\r\npython -m BVSim \r\n```\r\n## <a name=\"parameters\"></a>Functions and Parameters\r\n\r\nFive modes: uniform, uniform parallel, csv, wave, wave_region\r\n\r\n### <a name=\"shared-parameters\"></a>Shared Parameters\r\nThe BVSim package provides several functions (modes) and parameters for simulating genetic variations. Here is a table that introduces all the functions and different parameters:\r\n\r\n| Parameter | Type | Description | Default |\r\n| --- | --- | --- | --- |\r\n| `-ref` | str | Input reference file | '.../BVSim/empirical/sub_hg19_chr1.fasta' |\r\n| `-save` | str | Saving path | .../BVSim/save/ |\r\n| `-seed` | int | Seed for random number generator | 999 |\r\n| `-times` | int | Number of times | 10 |\r\n| `-rep` | int | Replication ID | 5 |\r\n| `-sv_trans` | int | Number of trans SV | 5 |\r\n| `-sv_inver` | int | Number of inversion SV | 5 |\r\n| `-sv_dup` | int | Number of tandem duplication | 5 |\r\n| `-sv_del` | int | Number of SV deletion | 5 |\r\n| `-sv_ins` | int | Number of SV insertion | 5 |\r\n| `-snp` | float | SNV number or probability | 5 |\r\n| `-snv_del` | float | SNV deletion number or probability | 5 |\r\n| `-snv_ins` | float | SNV insertion number or probability | 5 |\r\n| `-notblockN` | bool | Do not Block N positions | False |\r\n| `-write` | bool | Write full results | False |\r\n| `-delmin` | int | Minimum deletion length | 50 |\r\n| `-delmax` | int | Maximum deletion length | 60 |\r\n| `-insmin` | int | Minimum insertion length | 50 |\r\n| `-insmax` | int | Maximum insertion length | 450 |\r\n| `-dupmin` | int | Minimum duplication length | 50 |\r\n| `-dupmax` | int | Maximum duplication length | 450 |\r\n| `-invmin` | int | Minimum inversion length | 50 |\r\n| `-invmax` | int | Maximum inversion length | 450 |\r\n| `-dupmin` | int | Minimum duplication length | 50 |\r\n| `-dupmax` | int | Maximum duplication length | 450 |\r\n| `-transmin` | int | Minimum translocation length | 50 |\r\n| `-transmax` | int | Maximum translocation length | 450 |\r\n| `-block_region_bed_url` | str | local path of the block region BED file | None |\r\n\r\n#### <a name=\"output\"></a>Output Naming Conventions\r\nWhen you run the simulation tool, the output files are named based on the sequence name you input or the parameter `rep` you set (repetition number). Below is a summary of the output files you can expect:\r\n\r\n1. **FASTA File**:  \r\n   The output FASTA file will be named as follows:\r\n```\r\nBV_<rep>_seq_<seqname>.fasta\r\n```\r\nThis file contains the simulated sequence.\r\n\r\n2. **VCF File**:  \r\nThe VCF file will be named:\r\n```\r\nBV_<rep>_seq_<seqname>.vcf\r\n```\r\nThis file stores the simulated variations.\r\n\r\n3. **SV Table**:  \r\nThe SV table will have different naming conventions depending on whether you choose to include relative positions:\r\n- If you include relative positions (by using the `-write` flag):\r\n  ```\r\n  BV_<rep>_seq_<seqname>_SVtable_full.csv\r\n  ```\r\n- If you do not include relative positions:\r\n  ```\r\n  BV_<rep>_seq_<seqname>_SVtable.csv\r\n  ```\r\n\r\n4. **Numpy File**:  \r\nThe numpy file that records all inserted segments we need to update the relative positions will be named:\r\n```\r\nBV_<rep>_seq_<seqname>_tem_ins_dic.npy\r\n```\r\n#### <a name=\"write\"></a>Write the Relative Positions of Simulated Variations\r\nIf you choose not to generate the relative positions during the initial simulation run (i.e., you do not include the `-write` flag), the columns for relative positions in the SV table will be empty. However, you can still update these relative positions later using the saved intermediate files.\r\n##### Steps to Write Relative Positions After Simulation\r\n1. **Run the Initial Simulation**:  \r\nFor example, you can execute:\r\n```bash\r\ncd your_home_path\r\npython -m BVSim -seed 1 -rep 1 -snp 2000\r\n```\r\nIn this case you generated default number of elementary SVs and micro indels, as well as 20000 SNPs saved in the default directory with `BV_1_seq_chr1_SVtable.csv`, `BV_1_seq_chr1_tem_ins_dic.npy`.\r\n\r\n2. **Update Relative Positions**:\r\nYou can then run the following command to generate a table with the relative positions:\r\n```bash\r\npython your_home_path/BVSim/main/write_SV.py your_home_path/BVSim/save/ BV_1_seq_chr1_SVtable.csv BV_1_seq_chr1_tem_ins_dic.npy\r\n```\r\nThis command will create a file called called `full_BV_1_seq_chr1_SVtable.csv` in the same directory, which will contain the relative positions for all variations with respect to the consensus sequence.\r\nBy following this naming convention and steps, you can easily manage and update your output files as needed.\r\n#### <a name=\"block\"></a>User-defined Block Regions with No Variations\r\nThe input of the '-block_region_bed_url' should be two columns of positions(start;end) without headers seperated by '\\t'. To create a bed file, you can refer to the following example. In this case, positions from 0 to 999, from 3000 to 3999 cannot have any variation, so called blocked.\r\n\r\n#### Toy Example:\r\n```bash\r\ncd your_home_path\r\necho -e \"0\\t1000\\n3000\\t4000\" > block_intervals.bed\r\n# uniform.py\r\ncd your_home_path\r\npython -m BVSim -seed 1 -rep 1 -write -snp 2000 -block_region_bed_url block_intervals.bed\r\n```\r\n\r\n### <a name=\"uniform-mode\"></a>Uniform Mode\r\nIf you do not call any of the following parameters (-csv, -cores, -len_bins, -wave), the simulation will be generated one by one uniformly.\r\n\r\n#### Toy Example (Uniform mode):\r\n```bash\r\nconda activate BVSim\r\npython -m BVSim -ref 'hg19_chr1.fasta' -seed 0 -rep 0 -write -snp 2000\r\n```\r\n### <a name=\"complex-sv-mode\"></a>Complex SV Mode\r\nAdd -csv to your command, 18 types of Complex Structure Variations can be generated.\r\n\r\n* ID1: Tandem Inverted Duplication (TanInvDup)\r\n* ID2: Dispersed Inverted Duplication (DisInvDup)\r\n* ID3: Dispersed Duplication (DisDup)\r\n* ID4: Inversion with 5\u2019 or 3\u2019 Flanking Deletion (DEL+INV/INV+DEL)\r\n* ID5: 5\u2019 Deletion and Dispersed Inverted Duplication (DEL+DisInvDup)\r\n* ID6: 5\u2019 Deletion and Dispersed Duplication (DEL+DisDup)\r\n* ID7: Tandem Duplication and 3\u2019 Deletion (TanDup+DEL)\r\n* ID8: Tandem Inverted Duplication and 3\u2019 Deletion (TanInvDup+DEL)\r\n* ID9: Tandem Duplication, Deletion and Inversion (TanDup+DEL+INV)\r\n* ID10: Tandem Inverted Duplication, Deletion and Inversion (TanInvDup+DEL+INV)\r\n* ID11: Paired-Deletion Inversion (DEL+INV+DEL)\r\n* ID12: Inversion with 5\u2019 Flanking Duplication (DUP+INV)\r\n* ID13: Inversion with 3\u2019 Flanking Duplication (INV+DUP)\r\n* ID14: Paired-Duplication Inversion (DUP+INV+DUP)\r\n* ID15: Inversion with 5\u2019 Flanking Duplication and 3\u2019 Flanking Deletion (DUP+INV+DEL)\r\n* ID16: Inversion with 5\u2019 Flanking Deletion and 3\u2019 Flanking Duplication (DEL+INV+DUP)\r\n* ID17: Inverted Duplication with Flanking Triplication (DupTripDup-INV)\r\n* ID18: Insertion with Deletion (INSdel)\r\n#### Toy Example (CSV mode):\r\n```bash\r\ncd your_home_path\r\npython -m BVSim -ref 'your_home_path/BVSim/empirical/sub_hg19_chr1.fasta' -save your_saving_url -seed 1 -rep 1 -csv -write -snp 2000\r\n```\r\n#### <a name=\"parameters-for-csv-mode\"></a>Parameters for CSV Mode\r\nThe lengths of the CSVs follow different Gaussian distributions with modifiable means (-mu) and standard deviations (-sigma).\r\n| Parameter | Type | Description | Default |\r\n| --- | --- | --- | --- |\r\n| `-csv_num` | int | Number for each type of CSV, superior to -csv_total_num | 0 |\r\n| `-csv_total_num` | int | Total number for CSV, assign number of each type by empirical weights | 0 |\r\n| `-num_ID1_csv to -num_ID18_csv` | int | Number of respective CSV types | 5 |\r\n| `-mu_ID1 to -mu_ID18` | int | Mean of Gaussian distribution of CSV length | 1000 |\r\n| `-sigma_ID1 to -sigma_ID18` | int | Standard deviation of Gaussian distribution of CSV length | 100 |\r\n\r\n### <a name=\"uniform-parallel-mode\"></a>Uniform Parallel Mode\r\nAdd -cores, -len_bins to your command, and write a .job file (task01.job) as follows (-c 5 means 5 cores, should be the same as -cores 5), parallel simulation will be allowed.\r\n\r\n#### Toy Example (Uniform-parallel mode): task01.job\r\n```bash\r\n#!/bin/bash\r\n#SBATCH -J uniform_parallel\r\n#SBATCH -N 1 -c 5\r\n#SBATCH --output=output.txt\r\n#SBATCH --error=err.txt\r\n\r\nsource /opt/share/etc/miniconda3-py39.sh\r\nconda activate BVSim\r\ncd your_home_path\r\npython -m BVSim -ref your_home_path/hg19/hg19_chr21.fasta -save your_home_path/test_data/BVSim/task03/ -cores 5 -len_bins 500000 -rep 3 -snp 200 -snv_del 200 -snv_ins 200 -write\r\nconda deactivate\r\n```\r\nSubmit the job file by:\r\n```bash\r\nsbatch task01.job\r\n```\r\n#### <a name=\"parameters-for-uniform-parallel-mode\"></a>Parameters for Uniform parallel Mode\r\n\r\n| Parameter | Type | Description | Default |\r\n| --- | --- | --- | --- |\r\n| `-cores` | int | Number of kernels for parallel processing | 1 |\r\n| `-len_bins` | int | Length of bins for parallel processing | 50000 |\r\n\r\n### <a name=\"wave-mode\"></a>Wave Mode\r\n\r\nIn Wave mode, users can provide a `.bed` file generated from an empirical `.vcf` file (for example, from HG002) or multiple BED files derived from samples of a selected population (such as the 15 Cell samples). This functionality allows you to generate non-uniform insertions and deletions with various options.\r\n\r\n#### <a name=\"requirements-for-the-bed-file\"></a>User-defined Sample(s) and Input BED File Requirements\r\n\r\nThe BED file must adhere to the following requirements:\r\n\r\n- **First Column**: Location (genomic position)\r\n- **Second Column**: DEL/INS label (indicating if the variation is a deletion or insertion)\r\n- **Third Column**: Length (absolute value of the variation)\r\n\r\nEach column should be separated by a tab character (`\\t`) and must not include headers. Additionally, each BED file should represent variations on the same sequence.\r\n\r\n#### <a name=\"generating-a-bed-file-for-a-single-sample-in-wave-mode\"></a>Generate a BED File for a Single Sample\r\n\r\nTo generate a single input BED file from the HG002 `.vcf` file of chromosome 21, you can use the following commands in your terminal:\r\n\r\n```bash\r\n# Download the VCF file and its index\r\nwget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NIST_SV_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz \r\nwget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NIST_SV_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz.tbi\r\n\r\n# Activate the bcftools environment\r\nconda activate bcftools\r\n\r\n# Generate the BED file using bcftools and awk\r\nbcftools view -H -r 21 -i 'SVTYPE=\"INS\" || SVTYPE=\"DEL\"' /home/adduser/data/test_data/TGS/hg002/HG002_SVs_Tier1_v0.6.vcf.gz | \\\r\nawk -v OFS='\\t' '{\r\n    split($8, a, \";\");\r\n    for (i in a) {\r\n        if (a[i] ~ /^SVTYPE/) {\r\n            split(a[i], b, \"=\");\r\n            svtype = b[2];\r\n        }\r\n        else if (a[i] ~ /^SVLEN/) {\r\n            split(a[i], c, \"=\");\r\n            svlen = c[2];\r\n            if (svlen < 0) svlen = -svlen;  # Extract the absolute value of SV length\r\n        }\r\n    }\r\n    print $2, svtype, svlen;  # Print the location, SV type, and absolute SV length\r\n}' > /home/adduser/data/test_data/TGS/hg002/chr21_SV_Tier1.bed\r\n```\r\n##### <a name=\"job-submission-for-wave-mode-single-sample\"></a>Job Submission for Single Sample (BED Format)\r\n\r\nTo utilize this single BED file, users should call '-indel_input_bed' in the command. Below is the example of a SLURM job script that you can use to run the Wave mode simulation with single empirical data:\r\n\r\n```bash\r\n#!/bin/bash\r\n#SBATCH -J full_chr21_parallel\r\n#SBATCH -N 1 -c 5\r\n#SBATCH --output=output_chr21_wave.txt\r\n#SBATCH --error=err_chr21_wave.txt\r\n\r\nsource /opt/share/etc/miniconda3-py39.sh\r\nconda activate BVSim\r\ncd your_home_path\r\npython -m BVSim -ref your_home_path/hg19/hg19_chr21.fasta -save your_home_path/test_data/BVSim -seed 0 -rep 2 -cores 5 -len_bins 500000 -wave -indel_input_bed your_home_path/hg002/chr21_SV_Tier1_2.bed -mode empirical -snp 2000 -snv_del 1000 -snv_ins 100 -write\r\nconda deactivate\r\n```\r\nSubmit the job file by:\r\n```bash\r\nsbatch task02_single.job\r\n```\r\n#### <a name=\"generating-bed-files-for-multiple-samples-in-wave-mode\"></a>Generating BED Files for Multiple Samples\r\n\r\nIn this section, we will outline the steps to generate `.bed` files for multiple cell samples from the original Excel spreadsheet, using the 15 Cell samples as an example.\r\n\r\n##### Step 1: Download the Original Excel File\r\n\r\nFirst, download the Excel file containing the cell samples data:\r\n\r\n```python\r\nimport os\r\n\r\n# Download the Excel file\r\nos.system('wget https://ars.els-cdn.com/content/image/1-s2.0-S0092867418316337-mmc1.xlsx')\r\n\r\n```\r\n##### Step 2: Load and View the Data\r\nNext, load the Excel file into a Pandas DataFrame and view the first few rows:\r\n```python\r\nimport pandas as pd\r\n\r\n# Read the Excel file into a DataFrame\r\nfile_path = '1-s2.0-S0092867418316337-mmc1.xlsx'\r\ndf = pd.read_excel(file_path, sheet_name=0)  # Choose the correct sheet based on the file\r\n\r\n# Display the first 5 rows of the DataFrame\r\nprint(df.head(5))\r\n```\r\n##### Step 3: Filter the Data\r\nExtract the required columns and rename the first column:\r\n```python\r\n# Extract the necessary columns\r\ncolumns_to_keep = ['#CHROM', 'POS', 'END', 'ID', 'SVTYPE', 'SVLEN', 'MERGE_SAMPLES']\r\ncell_df = df[columns_to_keep]\r\n\r\n# List of all sample strings\r\nsamples = ['CHM1', 'CHM13', 'HG00514', 'HG00733', 'NA19240', 'HG02818', 'NA19434', 'HG01352', 'HG02059', 'NA12878', 'HG04217', 'HG02106', 'HG00268', 'AK1', 'HX1']\r\n# selected population: the African population\r\nAFR_samples = ['NA19240', 'HG02818', 'NA19434']\r\n\r\n# Specify the columns to save in the BED file\r\ncolumns_to_save = ['POS', 'SVTYPE', 'SVLEN']\r\n\r\n# Extract rows where CHROM equals 'chr21'\r\nchr21_df = cell_df[cell_df['CHROM'] == 'chr21']\r\n\r\n# Display the first 10 rows for verification\r\nprint(chr21_df.head(10))\r\n\r\n# Generate BED files for each sample in the AFR_samples list\r\nfor sample in AFR_samples:\r\n    # Create a new DataFrame containing only rows where 'MERGE_SAMPLES' contains the current sample\r\n    sample_df = chr21_df[chr21_df['MERGE_SAMPLES'].str.contains(sample)]\r\n\r\n    # Specify the path for the new BED file\r\n    bed_file_path = f'.../BVSim/empirical/{sample}_chr21.bed'\r\n\r\n    # Save the specified columns to a BED file\r\n    sample_df[columns_to_save].to_csv(bed_file_path, sep='\\t', header=False, index=False)\r\n\r\n```\r\n#### <a name=\"job-submission-for-wave-mode-multiple-samples\"></a>Job Submission for Multiple Samples (BED Format)\r\n\r\nWe provide an example of a Job submission script using SLURM for running the Wave mode with BVSim. This script utilizes the generated multiple sample BED files. Below is the example of a SLURM job script that you can use to run the Wave mode simulation with multiple samples:\r\n\r\n```bash\r\n#!/bin/bash\r\n#SBATCH -J wave\r\n#SBATCH -N 1 -c 5\r\n#SBATCH --output=/home/project18/code/BVSim_code/wave2_out.txt\r\n#SBATCH --error=/home/project18/code/BVSim_code/wave2_err.txt\r\n\r\nsource /opt/share/etc/miniconda3-py39.sh\r\nconda activate BVSim\r\ncd /home/project18/\r\n\r\npython -m BVSim -ref your_home_path/hg38/chr21.fasta \\\r\n-save your_home_path/BVSim/task01/ -seed 0 -rep 1 -cores 5 \\\r\n-len_bins 500000 -wave -mode empirical -snp 2000 -snv_del 1000 -snv_ins 100 \\\r\n-write -file_list NA19240_chr21 HG02818_chr21 NA19434_chr21\r\n\r\nconda deactivate\r\n```\r\n#### <a name=\"important-note-on-file-placement\"></a>Important Note on File Placement\r\nEnsure that both the single sample and multiple sample BED files are placed in the .../BVSim/empirical/ directory. This organization simplifies the command structure, allowing you to specify only the base names of the files (without extensions) directly in the -file_list option, as demonstrated in the script above.\r\n\r\n#### <a name=\"parameters-for-wave-mode\"></a>Parameters for Wave Mode\r\n\r\n| Parameter | Type | Description | Default |\r\n| --- | --- | --- | --- |\r\n| `-cores` | int | Number of kernels for parallel processing | 1 |\r\n| `-len_bins` | int | Length of bins for parallel processing | 50000 |\r\n| `-wave` | bool | Run Wave.py script | False |\r\n| `-mode` | str | Mode for calculating probabilities | 'probability' |\r\n| `-sum` | bool | Total indel SV equals sum of the input bed | False |\r\n| `-indel_input_bed` | str | Input single BED file | None |\r\n| `-file_list` | str | Input list of multiple BED files | None |\r\n\r\n##### Mode and Sum Parameters\r\n\r\nThe `-mode` parameter determines how the simulation calculates probabilities for insertions and deletions. It accepts two values:\r\n\r\n- **'probability'**: In this mode, probabilities for insertions and deletions are derived from the empirical data provided in the input BED files. The total number of variations can be defined by the `-sum` parameter. If `-sum` is set to `True`, the total number of insertions or deletions will be the maximum of the calculated empirical total or the specified values in `-sv_ins` or `-sv_del`. This allows for flexibility in controlling the total number of SVs in the simulation.\r\n\r\n- **'empirical'**: When set to this mode, the simulation directly uses the empirical values from the input data without any probability calculations. The total number of variations will be the sum of the provided empirical data.\r\n\r\nThe `-sum` parameter, when enabled, alters the total number of insertions and deletions based on the specified empirical data. If disabled, the simulation uses the fixed total values defined in `-sv_ins` and `-sv_del`, regardless of the empirical input.\r\n\r\n\r\n### <a name=\"wave-region-mode\"></a>Wave Region Mode\r\n\r\nIn Wave region mode, you can specify different INDEL probabilities using a BED file defined by `region_bed_url`. For example, if you want to increase the insertion and deletion probabilities in the tandem repeat (TR) regions of hg19, you can follow these steps.\r\n\r\n#### <a name=\"step-1-extract-tr-regions\"></a>Extract User-defined Regions (e.g. TR region) and Generate the BED File\r\n\r\nFirst, extract the TR regions' positions from UCSC and create a BED file with two columns (start; end) separated by a tab character (`\\t`).\r\n\r\nYou can generate the BED file using the following commands:\r\n\r\n```bash\r\n# Download the TR regions data\r\nwget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh37/resources/hg19.simpleRepeat.bed.gz \r\nwget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh37/resources/hg19.simpleRepeat.bed.gz.tbi\r\n\r\n# Extract the relevant columns and create the BED file\r\nzcat hg19.simpleRepeat.bed.gz | awk 'BEGIN{OFS=\"\\t\"} {print $1, $2, $3}' > your_home_path/hg002/windows_TR.bed\r\n\r\n# Merge overlapping intervals and remove duplicates\r\nbedtools sort -i your_home_path/hg002/windows_TR.bed | bedtools merge -i stdin | awk 'BEGIN{OFS=\"\\t\"} {$4=\"TR\"; print}' | uniq > your_home_path/hg002/windows_TR_unique.bed\r\n\r\n# Filter for chromosome 21\r\nawk '$1 == \"chr21\"' your_home_path/hg002/windows_TR_unique.bed > your_home_path/hg002/windows_TR_unique_chr21.bed\r\n\r\n# Create a final BED file with start and end positions\r\nawk '{print $2 \"\\t\" $3}' your_home_path/hg002/windows_TR_unique_chr21.bed > your_home_path/hg002/chr21_TR_unique.bed\r\n```\r\n#### <a name=\"job-submission-for-wave-region-mode-single-sample\"></a>Job Submission for Single Sample (BED Format)\r\nIn this example, we set the seed to `0` and use a replication ID of `4`. The job is configured to utilize `5` cores for parallel processing, with a bin size of `500,000`. We will generate `10,000` SNPs, along with `100` micro deletions and `100` micro insertions. The probabilities for these insertions and deletions are specified in the input BED file (`-indel_input_bed`) using the empirical mode (`-mode`). Additionally, we have set the probabilities for insertions (`-p_ins_region`) and deletions (`-p_del_region`) to approximately `0.6` for the total located in the TR region defined by `-region_bed_url`.\r\n\r\n```bash\r\n#!/bin/bash\r\n#SBATCH -J full_chr21_parallel\r\n#SBATCH -N 1 -c 5\r\n#SBATCH --output=output_chr21_wave_region.txt\r\n#SBATCH --error=err_chr21_wave_region.txt\r\n\r\nsource /opt/share/etc/miniconda3-py39.sh\r\nconda activate BVSim\r\ncd your_home_path\r\npython -m BVSim -ref your_home_path/hg19/hg19_chr21.fasta -save your_home_path/test_data/BVSim -seed 0 -rep 4 -cores 5 -len_bins 500000 -wave_region -indel_input_bed your_home_path/hg002/chr21_SV_Tier1.bed -mode empirical -snp 10000 -snv_del 100 -snv_ins 100 -write -p_del_region 0.6 -p_ins_region 0.6 -region_bed_url your_home_path/hg002/chr21_TR_unique.bed\r\nconda deactivate\r\n```\r\n\r\nSubmit the job file using the following command:\r\n```bash\r\nsbatch task03.job\r\n```\r\n#### <a name=\"parameters-for-wave-region-mode\"></a>Parameters for Wave Region Mode\r\nThe table below summarizes the parameters available for Wave region mode:\r\n| Parameter | Type | Description | Default |\r\n| --- | --- | --- | --- |\r\n| `-cores` | int | Number of kernels for parallel processing | 1 |\r\n| `-len_bins` | int | Length of bins for parallel processing | 50000 |\r\n| `-wave` | bool | Run Wave.py script | False |\r\n| `-mode` | str | Mode for calculating probabilities | 'probability' |\r\n| `-sum` | bool | Total number of insertions and deletions equals sum of the input bed | False |\r\n| `-indel_input_bed` | str | Input single BED file | None |\r\n| `-file_list` | str | Input list of multiple BED files | None |\r\n| `-wave_region` | bool | Run Wave_TR.py script | False |\r\n| `-p_del_region` | float | Probability of SV DEL in the user-defined region for deletion | 0.5 |\r\n| `-p_ins_region` | float | Probability of SV INS in the user-defined region for insertion | 0.5 |\r\n| `-region_bed_url` | str | Path of the BED file for the user-defined region | 'your_home_path/hg002/chr21_TR_unique.bed' |\r\n\r\n### <a name=\"human-genome\"></a>Human Genome\r\nFor the human genome, we derive the length distributions of SVs from HG002 and the 15 representative samples. For SNPs, we embed a learned substitution transition matrix from the dbSNP database. With a user-specified bin size, BVSim learns the distribution of SV positions per interval. It can model the SVs per interval as a multinomial distribution parameterized by the observed frequencies in HG002 (GRCh37/hg19 as reference) or sample the SV numbers per interval from a Gaussian distribution with the mean and standard deviation computed across the 15 samples (GRCh38/hg38 as reference). Calling \u2018-hg19\u2019 or \u2018-hg38\u2019 and specifying the chromosome name can activate the above procedures automatically for the human genome.\r\n\r\nIn the following example, we use 5 cores and 500,000 as length of the intervals. The reference is chromosome 21 of hg19, so we call \"-hg19 chr21\" in the command line to utilize the default procedure. In addition, we generated 1,000 SNPs, 99 duplications, 7 inversions, 280 deletions, and 202 insertions. The ratio of deletions/insertions in the tandem repeat regions with respect to the total number is 0.810/0.828. We also set the minimum and maximum lengths of some SVs.\r\n#### Toy example (-hg19)\r\n```bash\r\n#!/bin/bash\r\n#SBATCH -J 0_hg19_chr21\r\n#SBATCH -N 1 -c 5\r\n#SBATCH --output=output.txt\r\n#SBATCH --error=err.txt\r\n\r\nsource /opt/share/etc/miniconda3-py39.sh\r\nconda activate BVSim\r\ncd your_home_path\r\npython -m BVSim -ref your_home_path/hg19/hg19_chr21.fasta -save your_home_path/test_data/BVSim/ -seed 0 -rep 0 -cores 5 -len_bins 500000 -hg19 chr21 -mode probability -snp 1000 -sv_trans 0 -dup 99 -sv_inver 7 -sv_del 280 -sv_ins 202 -snv_del 0 -snv_ins 0 -p_del_region 0.810 -p_ins_region 0.828 -region_bed_url /home/project18/data/test_data/TGS/hg002/chr21_TR_unique.bed -delmin 50 -delmax 2964912 -insmin 50 -insmax 187524\r\nconda deactivate\r\n```\r\n\r\n\r\n## <a name=\"uninstallation\"></a>Uninstallation for Updates\r\nTo update to the latest version of BVSim, you can uninstall and delete the cloned files. Then, try to clone from the new repository and install again.\r\n```bash\r\ncd your_home_path\r\npip uninstall BVSim\r\n```\r\n## <a name=\"workflow\"></a>Workflow of BVSim\r\nThe following figure illustrates the workflow of BVSim, encapsulated within a dashed box, and demonstrates how the output files interact with read simulators, the alignment tool Minimap2, Samtools, and evaluation tools.\r\n![Workflow of BVSim](flow_chart_pipline.png)\r\n## <a name=\"definitions\"></a>Definitions of SVs Simulated by BVSim\r\n![Definitions of SVs](Fig1a_BVSim.png)\r\n",
        "doi": "10.48546/workflowhub.workflow.1361.1",
        "edam_operation": [
            "Statistical modelling"
        ],
        "edam_topic": [
            "Genetic variation"
        ],
        "filtered_on": "profil.* in description",
        "id": "1361",
        "keep": "Reject",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1361?version=1",
        "name": "BVSim: A Benchmarking Variation Simulator Mimicking Human Variation Spectrum",
        "number_of_steps": 0,
        "projects": [
            "Structural Variation Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "python"
        ],
        "tools": [],
        "type": "Unrecognized workflow type",
        "update_time": "2025-05-10",
        "versions": 1
    },
    {
        "create_time": "2025-05-10",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-methylong_logo_dark.png\">\n    <img alt=\"nf-core/methylong\" src=\"docs/images/nf-core-methylong_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/methylong/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/methylong/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/methylong/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/methylong/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/methylong/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/methylong)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23methylong-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/methylong)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/methylong** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/methylong \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/methylong/usage) and the [parameter documentation](https://nf-co.re/methylong/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/methylong/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/methylong/output).\n\n## Credits\n\nnf-core/methylong was originally written by Jin Yan Khoo.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#methylong` channel](https://nfcore.slack.com/channels/methylong) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/methylong for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1360",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1360?version=1",
        "name": "nf-core/methylong",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-10",
        "versions": 1
    },
    {
        "create_time": "2025-05-10",
        "creators": [
            "Christopher Mohr",
            "Alexander Peltzer"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-epitopeprediction_logo_dark.png\">\n    <img alt=\"nf-core/epitopeprediction\" src=\"docs/images/nf-core-epitopeprediction_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/epitopeprediction/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/epitopeprediction/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/epitopeprediction/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/epitopeprediction/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/epitopeprediction/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/epitopeprediction)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23epitopeprediction-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/epitopeprediction)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/epitopeprediction** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/epitopeprediction \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/epitopeprediction/usage) and the [parameter documentation](https://nf-co.re/epitopeprediction/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/epitopeprediction/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/epitopeprediction/output).\n\n## Credits\n\nnf-core/epitopeprediction was originally written by Christopher Mohr, Jonas Scheid.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#epitopeprediction` channel](https://nfcore.slack.com/channels/epitopeprediction) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/epitopeprediction for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "984",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/984?version=9",
        "name": "nf-core/epitopeprediction",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "epitope",
            "epitope-prediction",
            "mhc-binding-prediction"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-10",
        "versions": 9
    },
    {
        "create_time": "2025-05-10",
        "creators": [
            "VGP",
            " Galaxy"
        ],
        "description": "Create Meryl Database used for the estimation of assembly parameters and quality control with Merqury. Part of the VGP pipeline.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in name",
        "id": "631",
        "keep": "Reject",
        "latest_version": 6,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/631?version=6",
        "name": "kmer-profiling-hifi-trio-VGP2/main",
        "number_of_steps": 11,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "meryl_count_kmers",
            "genomescope",
            "meryl_groups_kmers",
            "meryl_histogram_kmers",
            "pick_value",
            "meryl"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 6
    },
    {
        "create_time": "2025-05-09",
        "creators": [
            "Ignacio Tripodi",
            "Margaret Gruca"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-nascent_logo_dark.png\">\n    <img alt=\"nf-core/nascent\" src=\"docs/images/nf-core-nascent_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/nascent/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/nascent/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/nascent/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/nascent/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/nascent/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/nascent)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23nascent-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/nascent)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/nascent** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/nascent \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/nascent/usage) and the [parameter documentation](https://nf-co.re/nascent/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/nascent/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/nascent/output).\n\n## Credits\n\nnf-core/nascent was originally written by Edmund Miller, Ignacio Tripodi, Margaret Gruca.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#nascent` channel](https://nfcore.slack.com/channels/nascent) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/nascent for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1004",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1004?version=6",
        "name": "nf-core/nascent",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gro-seq",
            "nascent",
            "pro-seq",
            "rna",
            "transcription",
            "tss"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-05-09",
        "versions": 6
    },
    {
        "create_time": "2025-06-22",
        "creators": [
            "B\u00e9r\u00e9nice Batut",
            "Paul Zierep",
            "Mina Hojat Ansari",
            "Patrick B\u00fchler",
            "Santino Faack"
        ],
        "description": "This workflow constructs Metagenome-Assembled Genomes (MAGs) using SPAdes or MEGAHIT as assemblers, followed by binning with four different tools and refinement using Binette. The resulting MAGs are dereplicated across the entire input sample set, then annotated and evaluated for quality.\nYou can provide pooled reads (for co-assembly/binning), individual read sets, or a combination of both. The input samples must consist of the original reads, which are used for abundance estimation. In all cases, reads should be trimmed, adapters removed, and cleaned of host or other contaminants before processing.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "MAGs in name",
        "id": "1352",
        "keep": "Keep",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1352?version=3",
        "name": "mags-building/main",
        "number_of_steps": 36,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "__UNZIP_COLLECTION__",
            "binette",
            "megahit",
            "samtools_sort",
            "metabat2",
            "__BUILD_LIST__",
            "concoct_cut_up_fasta",
            "pick_value",
            "tp_awk_tool",
            "metabat2_jgi_summarize_bam_contig_depths",
            "__FLATTEN__",
            "bakta",
            "checkm_lineage_wf",
            "gtdbtk_classify_wf",
            "metaspades",
            "Fasta_to_Contig2Bin",
            "semibin",
            "quast",
            "multiqc",
            "concoct",
            "concoct_merge_cut_up_clustering",
            "concoct_extract_fasta_bins",
            "collection_column_join",
            "bowtie2",
            "checkm2",
            "coverm_genome",
            "concoct_coverage_table",
            "maxbin2",
            "drep_dereplicate",
            "map_param_value"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 3
    },
    {
        "create_time": "2025-04-26",
        "creators": [
            "Chelsea Sawyer",
            "Edmund Miller",
            "Matthias De Smet"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-demultiplex_logo_dark.png\">\n    <img alt=\"nf-core/demultiplex\" src=\"docs/images/nf-core-demultiplex_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/demultiplex/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/demultiplex/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/demultiplex/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/demultiplex/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/demultiplex/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7153103-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7153103)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/demultiplex)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23demultiplex-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/demultiplex)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/demultiplex** is a bioinformatics pipeline used to demultiplex the raw data produced by next generation sequencing machines. The following platforms are supported:\n\n1. Illumina (via `bcl2fastq` or `bclconvert`)\n2. Element Biosciences (via `bases2fastq`)\n3. Singular Genomics (via [`sgdemux`](https://github.com/Singular-Genomics/singular-demux))\n4. FASTQ files with user supplied read structures (via [`fqtk`](https://github.com/fulcrumgenomics/fqtk))\n5. 10x Genomics (via [`mkfastq`](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/mkfastq))\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources.The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/demultiplex/results).\n\n## Pipeline summary\n\n1. [samshee](#samshee) - Validates illumina v2 samplesheets.\n2. Demultiplexing\n\n- [bcl-convert](#bcl-convert) - converting bcl files to fastq, and demultiplexing (CONDITIONAL)\n- [bases2fastq](#bases2fastq) - converting bases files to fastq, and demultiplexing (CONDITIONAL)\n- [bcl2fastq](#bcl2fastq) - converting bcl files to fastq, and demultiplexing (CONDITIONAL)\n- [sgdemux](#sgdemux) - demultiplexing bgzipped fastq files produced by Singular Genomics (CONDITIONAL)\n- [fqtk](#fqtk) - a toolkit for working with FASTQ files, written in Rust (CONDITIONAL)\n- [mkfastq](#mkfastq) - converting bcl files to fastq, and demultiplexing for single-cell sequencing data (CONDITIONAL)\n\n3. [checkqc](#checkqc) - (optional) Check quality criteria after demultiplexing (bcl2fastq only)\n4. [fastp](#fastp) - Adapter and quality trimming\n5. [Falco](#falco) - Raw read QC\n6. [md5sum](#md5sum) - Creates an MD5 (128-bit) checksum of every fastq.\n7. [MultiQC](#multiqc) - aggregate report, describing results of the whole pipeline\n\n![subway map](docs/demultiplex.png)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\n```console\nnextflow run nf-core/demultiplex --input samplesheet.csv --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\n```\n\n```bash\nnextflow run nf-core/demultiplex \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/demultiplex/usage) and the [parameter documentation](https://nf-co.re/demultiplex/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/demultiplex/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/demultiplex/output).\n\n## Credits\n\nThe nf-core/demultiplex pipeline was written by Chelsea Sawyer from The Bioinformatics & Biostatistics Group for use at The Francis Crick Institute, London.\n\nThe pipeline was re-written in Nextflow DSL2 and is primarily maintained by Matthias De Smet([@matthdsm](https://github.com/matthdsm)) from [Center For Medical Genetics Ghent, Ghent University](https://github.com/CenterForMedicalGeneticsGhent) and Edmund Miller([@edmundmiller](https://github.com/edmundmiller)) from [Element Biosciences](https://www.elementbiosciences.com/)\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [`@ChristopherBarrington`](https://github.com/ChristopherBarrington)\n- [`@drpatelh`](https://github.com/drpatelh)\n- [`@danielecook`](https://github.com/danielecook)\n- [`@escudem`](https://github.com/escudem)\n- [`@crickbabs`](https://github.com/crickbabs)\n- [`@nh13`](https://github.com/nh13)\n- [`@sam-white04`](https://github.com/sam-white04)\n- [`@maxulysse`](https://github.com/maxulysse)\n- [`@atrigila`](https://github.com/atrigila)\n- [`@nschcolnicov`](https://github.com/nschcolnicov)\n- [`@aratz`](https://github.com/aratz)\n- [`@grst`](https://github.com/grst)\n- [`@apeltzer`](https://github.com/apeltzer)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#demultiplex` channel](https://nfcore.slack.com/channels/demultiplex) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/demultiplex for your analysis, please cite it using the following doi: [10.5281/zenodo.7153103](https://doi.org/10.5281/zenodo.7153103)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "978",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/978?version=15",
        "name": "nf-core/demultiplex",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bases2fastq",
            "bcl2fastq",
            "demultiplexing",
            "elementbiosciences",
            "illumina"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-04-26",
        "versions": 15
    },
    {
        "create_time": "2025-04-16",
        "creators": [
            "Christopher Mohr",
            "Alexander Peltzer",
            "Sven Fillinger"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-hlatyping_logo_dark.png\">\n    <img alt=\"nf-core/hlatyping\" src=\"docs/images/nf-core-hlatyping_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/hlatyping/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/hlatyping/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/hlatyping/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/hlatyping/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/hlatyping/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/hlatyping)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23hlatyping-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/hlatyping)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/hlatyping** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/hlatyping \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/hlatyping/usage) and the [parameter documentation](https://nf-co.re/hlatyping/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/hlatyping/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/hlatyping/output).\n\n## Credits\n\nnf-core/hlatyping was originally written by Christopher Mohr, Alexander Peltzer, Sven Fillinger.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#hlatyping` channel](https://nfcore.slack.com/channels/hlatyping) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/hlatyping for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "991",
        "keep": "To Curate",
        "latest_version": 11,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/991?version=11",
        "name": "nf-core/hlatyping",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "dna",
            "hla",
            "hla-typing",
            "immunology",
            "optitype",
            "personalized-medicine",
            "rna"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-04-16",
        "versions": 11
    },
    {
        "create_time": "2025-04-20",
        "creators": [
            "Jonas Kasmanas"
        ],
        "description": "## gSpreadComp: Streamlining Microbial Community Analysis for Resistance, Virulence, and Plasmid-Mediated Spread\r\n\r\n\r\n<p align=\"center\" width=\"100%\">\r\n\t<img width=\"30%\" src=\"/gspreadcomp_logo_noback.png\">\r\n</p>\r\n\r\n### Overview\r\n\r\ngSpreadComp is a UNIX-based, modular bioinformatics toolkit designed to streamline comparative genomics for analyzing microbial communities. It integrates genome annotation, gene spread calculation, plasmid-mediated horizontal gene transfer (HGT) detection and resistance-virulence ranking within the analysed microbial community to help researchers identify potential resistance-virulence hotspots in complex microbial datasets.\r\n\r\n> [!TIP]\r\n> After installation, the user may want to check a detailed tutorial with example input and output data [here](usage_tutorial.md)\r\n\r\n### Objectives and Features\r\n- **Six Integrated Modules**: Offers modules for taxonomy assignment, genome quality estimation, ARG annotation, plasmid/chromosome classification, virulence factor annotation, and in-depth downstream analysis, including target-based gene spread analysis and prokaryotic resistance-virulence ranking.\r\n- **Weighted Average Prevalence (WAP)**: Employs WAP for calculating the spread of target genes at different taxonomical levels or target groups, enabling refined analyses and interpretations of microbial communities.\r\n- **Reference Pathogen Identification**: Compares genomes to the NCBI pathogens database to create a resistance-virulence ranking within the community.\r\n- **HTML Reporting**: Culminates in a structured HTML report after the complete downstream analysis, providing users with an overview of the results.\r\n\r\n### Modular Approach and Flexibility\r\n`gSpreadComp`\u2019s modular nature enables researchers to use the tool's main analysis and report generation steps independently or to integrate only specific pieces of `gSpreadComp` into their pipelines, providing flexibility and accommodating the varying software management needs of investigators.\r\n\r\n#### Using other annotation tools with gSpreadComp\r\n> [!TIP]\r\n> Users can incorporate results from other annotation tools within gSpreadComp's workflow, provided the input is formatted according to gSpreadComp's specifications. This allows for the integration of preferred or specialized tools for specific steps (e.g., alternative ARG or plasmid detection methods) while still benefiting from gSpreadComp's downstream analysis capabilities.\r\n> \r\n> For the quality data it should look like: [Quality DataFrame Format](test_data/checkm_df_format_gSpread.csv)\r\n> \r\n> For the taxonomy data it should look like: [Taxonomy DataFrame Format](test_data/gtdb_df_format_gSpread.csv)\r\n> \r\n> For the gene annotation (e.g. ARGs) data it should look like: [Gene annotation DataFrame Format](test_data/deeparg_df_format_gSpread.csv)\r\n> \r\n> For the plasmid identification data it should look like: [Plasmid identification DataFrame Format](test_data/plasflow_combined_format_gSpread.csv)\r\n>\r\n> Metadata information data should look like: [Metadata Sample](test_data/02_metadata_gspread_sample.csv)\r\n\r\nBy the end of a successful run, you should have a report that looks like this: [Download Example Report](https://raw.githubusercontent.com/mdsufz/gSpreadComp/refs/heads/main/test_data/gSpread_example_result_report.html)\r\n\r\n### Comprehensive Workflow\r\n\r\n![ScreenShot](/test_data/01_Kasmanas_gSpread_Fig_1.png)\r\n\r\ngSpreadComp consists of the following modules:\r\n\r\n1. **Taxonomy Assignment**: Uses [GTDBtk v2](https://academic.oup.com/bioinformatics/article/38/23/5315/6758240) for taxonomic classification.\r\n2. **Genome Quality Estimation**: Employs [CheckM](https://genome.cshlp.org/content/25/7/1043) for assessing genome completeness and contamination.\r\n3. **ARG Annotation**: Utilizes [DeepARG](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-018-0401-z) for antimicrobial resistance gene prediction.\r\n4. **Plasmid Classification**: Implements [Plasflow](https://academic.oup.com/nar/article/46/6/e35/4807335) for plasmid sequence identification.\r\n5. **Virulence Factor Annotation**: Annotates virulence factors using the [Victors](https://academic.oup.com/nar/article/47/D1/D693/5144967?login=false) and/or [VFDB](http://www.mgc.ac.cn/VFs/main.htm) databases.\r\n6. **Downstream Analysis**: Performs gene spread analysis, resistance-virulence ranking, and potential plasmid-mediated HGT detection.\r\n\r\n\r\n# Requirements\r\n\r\nBefore installing and running `gSpreadComp`, ensure that your system meets the following requirements:\r\n\r\n## 1. Operating System\r\n- Linux x64 system\r\n\r\n## 2. Package Managers\r\n- [Miniconda](https://docs.conda.io/en/latest/miniconda.html): Required for creating environments and managing packages.\r\n- [Mamba](https://mamba.readthedocs.io/en/latest/user_guide/mamba.html): A faster package manager used within the `gSpreadComp` installation.\r\n\r\n## 3. Storage\r\n- Approximately 15 GB for software installation.\r\n- Around 92 GB for the entire database requirements.\r\n\r\n# Installation\r\n\r\n## Database Management\r\n`gSpreadComp` includes an easy-to-use script for automatic download and configuration of the required databases, with scheduled updates every January and July.\r\n\r\n## Compatibility and Requirements\r\nDesigned to support Linux x64 systems, requiring approximately 15 GB for software installation and around 92 GB for the entire database requirements.\r\n\r\n## 1 - Install miniconda\r\n\r\nTo bypass conflicting dependencies, the gSpreadComp approach uses miniconda to create automatically orchestrated environments. [Mamba](https://mamba.readthedocs.io/en/latest/user_guide/mamba.html) is a much faster package manager than conda and is used within the gSpreadComp installation. Consequently, miniconda and mamba are required to be previously installed in your system. Below is a possible way of installing miniconda and mamba. Please, be aware that mamba works best when installed in your base environment.\r\n\r\n```console\r\n# See documentation: https://docs.conda.io/en/latest/miniconda.html\r\n\r\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\r\n$ chmod +x Miniconda3-latest-Linux-x86_64.sh\r\n$ ./Miniconda3-latest-Linux-x86_64.sh\r\n$ export PATH=~/miniconda3/bin:$PATH\r\n\r\n# Install mamba. See documentation: https://mamba.readthedocs.io/en/latest/installation.html\r\n$ conda install mamba -n base -c conda-forge\r\n```\r\n\r\n## 2 - Install gSpreadComp\r\n\r\nOnce you have miniconda and mamba installed and on your PATH, you can proceed to install gSpreadComp.\r\nThe installation script was designed to install and set up all necessary tools and packages.\r\n\r\n```console\r\n# Clone repository\r\n$ git clone https://github.com/mdsufz/gSpreadComp.git\r\n\r\n# Go to the gSpreadComp cloned repository folder\r\n$ cd gSpreadComp\r\n\r\n# Make sure you have conda ready and that you are in your base environment.\r\n$ conda activate base\r\n$ echo $CONDA_PREFIX\r\n\r\n# You should see something like the following:\r\n/path/to/miniconda3\r\n\r\n# Run the installation script as follows\r\n$ bash -i installation/install.sh\r\n\r\n# Follow the instructions on the screen:\r\n# Enter \"y\" if you want to install all modules; otherwise, enter \"n\".\r\n# If you entered \"n\", enter \"y\" for each of the modules you would like to install individually.\r\n\r\n\tThe MuDoGeR's installation will begin..\r\n\r\n\r\n\t      (  )   (   )  )\t\t\t\r\n\t       ) (   )  (  (\t\t\t\r\n\t       ( )  (    ) )\t\t\t\r\n\t       _____________\t\t\t\r\n\t      <_____________> ___\t\t\r\n\t      |             |/ _ \\\t\t\r\n\t      |               | | |\t\t\r\n\t      |               |_| |\t\t\r\n\t   ___|             |\\___/\t\t\r\n\t  /    \\___________/    \\\t\t\r\n\t  \\_____________________/\t\t\r\n\r\n\tThis might take a while. Time to grab a coffee...\r\n```\r\n\r\n## 3 - Install necessary databases\r\n\r\n**Make sure to run the database setup after gSpreadComp is installed.**\r\n\r\nSome bioinformatics tools used within gSpreadComp require specific databases to work. We developed a database download and set up tool to make our lives easier. You can choose to install only the databases you intend to use. You can use the flag `--dbs` to choose and set up the selected databases (all [default], install all databases).\r\n\r\nUse this script if you want gSpreadComp to take care of everything.\r\n\r\n```console\r\n# Make sure gSpreadComp_env is activated. It should have been created when you ran 'bash -i installation/install.sh'\r\n$ conda activate gspreadcomp_env\r\n\r\n# Go to gSpreadComp cloned directory\r\n$ cd gSpreadComp\r\n\r\n# Run the database setup script\r\n$ bash -i installation/database-setup.sh --dbs all -o /path/to/save/databases\r\n\r\n# You can also check out the database-setup help information\r\n$ bash -i installation/database-setup.sh --help\r\n\r\n        gSpreadComp database script v=1.0\r\n        Usage: bash -i database-setup.sh --dbs [module] -o output_folder_for_dbs\r\n\t\t    USE THE SAME DATABASE LOCATION OUTPUT FOLDER FOR ALL DATABASES USED WITH gSpreadComp\r\n          --dbs all\t\t\t\tdownload and install the required and optional databases [default]\"\r\n          --dbs required              \t\tdownload and install the required databases (Victors and VFDB) for gSpreadComp\r\n          --dbs optional              \t\tdownload and install all the optional (ARGs, GTDB-tk, CheckM) databases for gSpreadComp\r\n          --dbs args\t\t\t\tdownload and install the required and the ARGs databases.\r\n          -o path/folder/to/save/dbs\t\toutput folder where you want to save the downloaded databases\r\n          --help | -h\t\t\t\tshow this help message\r\n          --version | -v\t\t\tshow database install script version\r\n\r\n\r\n```\r\n\r\n## Usage\r\n\r\n### Activating the Conda Environment\r\nBefore using `gSpreadComp`, activate the appropriate conda environment using the following command:\r\n```sh\r\nconda activate gSpreadComp_env\r\n```\r\n\r\n### Command-Line Usage\r\n`gSpreadComp` provides several modules, each performing a specific task within the pipeline. The quick command-line usage is as follows:\r\n```sh\r\ngspreadcomp --help\r\n```\r\n\r\n### Modules and Their Descriptions\r\n`gSpreadComp` comprises several modules, each serving a specific purpose in the genome analysis workflow:\r\n\r\n#### 1. Taxonomy Assignment\r\n```sh\r\ngspreadcomp taxonomy [options] --genome_dir genome_folder -o output_dir\r\n```\r\n- Assigns taxonomy to genomes using [GTDBtk v2](https://academic.oup.com/bioinformatics/article/38/23/5315/6758240).\r\n- Options:\r\n  - `--genome_dir STR`: folder with the bins to be classified (in fasta format)\r\n  - `--extension STR`: fasta file extension (e.g. fa or fasta) [default: fa]\r\n  - `-o STR`: output directory\r\n  - `-t INT`: number of threads\r\n\r\n#### 2. Genome Quality Estimation\r\n```sh\r\ngspreadcomp quality [options] --genome_dir genome_folder -o output_dir\r\n```\r\n- Estimates genome completeness and contamination using [CheckM](https://genome.cshlp.org/content/25/7/1043).\r\n- Options:\r\n  - `--genome_dir STR`: folder with the genomes to estimate quality (in fasta format)\r\n  - `--extension STR`: fasta file extension (e.g. fa or fasta) [default: fa]\r\n  - `-o STR`: output directory\r\n  - `-t INT`: number of threads [default: 1]\r\n  - `-h --help`: print this message\r\n\r\n#### 3. ARG Prediction\r\n```sh\r\ngspreadcomp args [options] --genome_dir genome_folder -o output_dir\r\n```\r\n- Predicts the Antimicrobial Resistance Genes (ARGs) in a genome using [DeepARG](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-018-0401-z).\r\n- Options:\r\n  - `--genome_dir STR`: folder with the genomes to be classified (in fasta format)\r\n  - `--extension STR`: fasta file extension (e.g. fa or fasta) [default: fa]\r\n  - `--min_prob NUM`: Minimum probability cutoff for DeepARG [Default: 0.8]\r\n  - `--arg_alignment_identity NUM`: Identity cutoff for sequence alignment for DeepARG [Default: 35]\r\n  - `--arg_alignment_evalue NUM`: Evalue cutoff for DeepARG [Default: 1e-10]\r\n  - `--arg_alignment_overlap NUM`: Alignment read overlap for DeepARG [Default: 0.8]\r\n  - `--arg_num_alignments_per_entry NUM`: Diamond, minimum number of alignments per entry [Default: 1000]\r\n  - `-o STR`: output directory\r\n  - `-h --help`: print this message\r\n\r\n#### 4. Plasmid Prediction\r\n```sh\r\ngspreadcomp plasmid [options] --genome_dir genome_folder -o output_dir\r\n```\r\n- Predicts if a sequence within a fasta file is a chromosome, plasmid, or undetermined using [Plasflow](https://academic.oup.com/nar/article/46/6/e35/4807335).\r\n- Options:\r\n  - `--genome_dir STR`: folder with the genomes to be classified (in fasta format)\r\n  - `--extension STR`: fasta file extension (e.g. fa or fasta) [default: fa]\r\n  - `--threshold NUM`: threshold for probability filtering [default: 0.7]\r\n  - `-o STR`: output directory\r\n  - `-h --help`: print this message\r\n\r\n#### 5. Virulence Factor annotation\r\n```sh\r\ngspreadcomp pathogens [options] --genome_dir genome_folder -o output_dir\r\n```\r\n- Aligns provided genomes to Virulence Factors databases and formats the output.\r\n- Options:\r\n  - `--genome_dir STR`: folder with the genomes to be aligned against Virulence factors (in fasta format)\r\n  - `--extension STR`: fasta file extension (e.g. fa or fasta) [default: fa]\r\n  - `--evalue NUM`: evalue, expect value, threshold as defined by NCBI-BLAST [default: 1e-50]\r\n  - `-t INT`: number of threads\r\n  - `-o STR`: output directory\r\n  - `-h --help`: print this message\r\n\r\n#### 6. Main Analysis\r\n```sh\r\ngspreadcomp gspread [options] -o output_dir\r\n```\r\n- Runs the main `gSpreadComp` to compare spread and plasmid-mediated HGT.\r\n- Options:\r\n  - `--checkm STR`: Path to the formatted Quality estimation dataframe\r\n  - `--gene STR`: Path to the formatted target Gene dataframe to calculate the spread\r\n  - `--gtdbtk STR`: Path to the formatted Taxonomy assignment dataframe\r\n  - `--meta STR`: Path to the formatted Sample's Metadata dataframe\r\n  - `--vf STR`: Path to the formatted Virulence Factors assignment dataframe\r\n  - `--plasmid STR`: Path to the formatted Plasmid prediction dataframe\r\n  - `--nmag INT`: Minimum number of Genomes per Library accepted [default=0]\r\n  - `--spread_taxa STR`: Taxonomic level to check gene spread [default=Phylum]\r\n  - `--target_gene_col STR`: Name of the column from the gene dataset with the Gene_ids to analyse [default=Gene_id]\r\n  - `-t INT`: number of threads\r\n  - `-o STR`: output directory\r\n  - `-h --help`: print this message\r\n\r\n\r\n## Important Considerations\r\n\r\n- gSpreadComp is designed for hypothesis generation and is not a standalone risk assessment tool.\r\n- Results should be interpreted cautiously and used to guide further experimental validation.\r\n- The tool provides relative rankings within analyzed communities, not absolute risk assessments.\r\n\r\n## Citation\r\n\r\nIf you use gSpreadComp in your research, please cite:\r\n\r\n[Citation information will be added upon publication]\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.1340.3",
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Comparative genomics",
            "Data curation and archival",
            "Microbiology"
        ],
        "filtered_on": "microbiom.* in tags",
        "id": "1340",
        "keep": "Keep",
        "latest_version": 3,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1340?version=3",
        "name": "gSpreadComp",
        "number_of_steps": 0,
        "projects": [
            "Kasmanas"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "antimicrobial resistance",
            "microbiome"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2025-04-20",
        "versions": 3
    },
    {
        "create_time": "2025-04-15",
        "creators": [
            "Ra\u00fcl Sirvent",
            "Rosa M Badia"
        ],
        "description": "COMPSs Matrix Multiplication resourceUsage profiling example.\r\n\r\nMN5 MSIZE=20 BSIZE=768 7 Nodes (6 workers) (--num_nodes=7 --worker_in_master_cpus=0). \r\n\r\n* Total number of tasks: 20^3 = 8000\r\n* Maximum code parallelism: 20^2 = 400\r\n* Total cores: 112*6 = 672 \r\n* Maximum utilisation: 400 / 112 = 3,57 Nodes\r\n\r\nOverall stats from \"pycompss inspect\":\r\n\r\n```\r\n    \u2502   \u2514\u2500\u2500 overall\r\n    \u2502       \u251c\u2500\u2500 matmul_tasks\r\n    \u2502       \u2502   \u2514\u2500\u2500 multiply\r\n    \u2502       \u2502       \u251c\u2500\u2500 maxTime = 91,111 ms\r\n    \u2502       \u2502       \u251c\u2500\u2500 executions = 8,000 \r\n    \u2502       \u2502       \u251c\u2500\u2500 avgTime = 84,839 ms\r\n    \u2502       \u2502       \u2514\u2500\u2500 minTime = 79,278 ms\r\n    \u2502       \u2514\u2500\u2500 executionTime = 1,929,944 ms\r\n```\r\n\r\nThis example shows misuse of resources (from 6 workers, only 3 and a half can be exploited due to the application's inherent parallelism), which can be seen in the profiling folder plots.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in name",
        "id": "1339",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1339?version=1",
        "name": "COMPSs Matrix Multiplication resourceUsage profiling example MN5 MSIZE=20 BSIZE=768 7 Nodes total",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2025-04-15",
        "versions": 1
    },
    {
        "create_time": "2025-04-14",
        "creators": [
            "Ra\u00fcl Sirvent",
            "Nicol\u00f2 Giacomini"
        ],
        "description": "Application that perform the multiplication between matrices.\r\nIn this experiment, a new profiling visualization is available, showing the resource usage such as CPU, memory, data read and written to disk, and data sent and received over the network.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1338",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1338?version=1",
        "name": "Matrix Multiplication \u2013 resource usage visualization",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2025-04-14",
        "versions": 1
    },
    {
        "create_time": "2025-04-13",
        "creators": [
            "Nils Homer"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-fastquorum_logo_dark.png\">\n    <img alt=\"nf-core/fastquorum\" src=\"docs/images/nf-core-fastquorum_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/fastquorum/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/fastquorum/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/fastquorum/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/fastquorum/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/fastquorum/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.11267672-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.11267672)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/nf-core/fastquorum)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23fastquorum-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/fastquorum)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/fastquorum** is a bioinformatics pipeline that implements the pipeline implements the [fgbio Best Practices FASTQ to Consensus Pipeline][fgbio-best-practices-link] to produce consensus reads using unique molecular indexes/barcodes (UMIs).\n`nf-core/fastquorum` can produce consensus reads from single or multi UMI reads, and even [Duplex Sequencing][duplex-seq-link] reads.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/fastquorum/results).\n\n| Tools                                                                                                              | Description                                                                                                                   |\n| ------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------- |\n| <p align=\"center\"><img title=\"Fastquorum Workflow (Tools)\" src=\"docs/images/fastquorum_subway.png\" width=100%></p> | <p align=\"center\"><img title=\"Fastquorum Workflow (Description)\" src=\"docs/images/fastquorum_subway.desc.png\" width=100%></p> |\n\n1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\n2. Fastq to BAM, extracting UMIs ([`fgbio FastqToBam`](http://fulcrumgenomics.github.io/fgbio/tools/latest/FastqToBam.html))\n3. Align ([`bwa mem`](https://github.com/lh3/bwa)), reformat ([`fgbio ZipperBam`](http://fulcrumgenomics.github.io/fgbio/tools/latest/ZipperBam.html)), and template-coordinate sort ([`samtools sort`](http://www.htslib.org/doc/samtools.html))\n4. Group reads by UMI ([`fgbio GroupReadsByUmi`](http://fulcrumgenomics.github.io/fgbio/tools/latest/GroupReadsByUmi.html))\n5. Call consensus reads\n   1. For [Duplex-Sequencing][duplex-seq-link] data\n      1. Call duplex consensus reads ([`fgbio CallDuplexConsensusReads`](http://fulcrumgenomics.github.io/fgbio/tools/latest/CallDuplexConsensusReads.html))\n      2. Collect duplex sequencing specific metrics ([`fgbio CollectDuplexSeqMetrics`](http://fulcrumgenomics.github.io/fgbio/tools/latest/CollectDuplexSeqMetrics.html))\n   2. For non-Duplex-Sequencing data:\n      1. Call molecular consensus reads ([`fgbio CallMolecularConsensusReads`](http://fulcrumgenomics.github.io/fgbio/tools/latest/CallMolecularConsensusReads.html))\n6. Align ([`bwa mem`](https://github.com/lh3/bwa))\n7. Filter consensus reads ([`fgbio FilterConsensusReads`](http://fulcrumgenomics.github.io/fgbio/tools/latest/FilterConsensusReads.html))\n8. Present QC ([`MultiQC`](http://multiqc.info/))\n\n## Verified Vendors, Kits, and Assays\n\n> [!WARNING]\n> The following Vendors, Kits, and Assays are provided for informational purposes only.\n> _No warranty for the accuracy or completeness of the information or parameters is implied._\n\n| Verified | Assay                                                     | Company                     | Strand | Randomness | UMI Location     | Read Structure  | URL                                                                                                                                                                                 |\n| -------- | --------------------------------------------------------- | --------------------------- | ------ | ---------- | ---------------- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| No       | SureSelect XT HS                                          | Agilent Technologies        | Single | Random     |                  |                 | [link](https://www.agilent.com/en/product/next-generation-sequencing/ngs-library-prep-target-enrichment-reagents/dna-seq-reagents/sureselectxt-hs-reagent-kits-4252208)             |\n| No       | SureSelect XT HS2 (MBC)                                   | Agilent Technologies        | Dual   | Random     |                  |                 | [link](https://www.agilent.com/en/product/next-generation-sequencing/ngs-library-prep-target-enrichment-reagents/dna-seq-reagents/sureselect-xt-hs2-dna-reagent-kit-4252207)        |\n| No       | TruSight Oncology (TSO)                                   | Illumina                    | Dual   | Nonrandom  |                  |                 | [link](https://www.illumina.com/products/by-type/clinical-research-products/trusight-oncology-umi.html)                                                                             |\n| No       | xGen dual index UMI Adapters                              | Integrated DNA Technologies | Single | Random     | index1 (i7)      |                 | [link](https://www.idtdna.com/pages/products/next-generation-sequencing/workflow/xgen-ngs-library-preparation/ngs-adapters-indexing-primers/adapters-indexing-primers-for-illumina) |\n| No       | xGen Prism (xGen cfDNA & FFPE DNA Library Prep MC v2 Kit) | Integrated DNA Technologies | Dual   | Nonrandom  |                  |                 | [link](https://www.idtdna.com/pages/products/next-generation-sequencing/workflow/xgen-ngs-library-preparation/dna-library-preparation/cfdna-ffpe-prep-kit)                          |\n| No       | NEBNext                                                   | New England Biosciences     | Single | Random     | index1 (i7)      |                 | [link](https://www.neb.com/en-us/products/e7874nebnext-multiplex-oligos-for-illumina-unique-dual-index-umi-adaptors-dna-set-2)                                                      |\n| No       | AML MRD                                                   | TwinStrand Biosciences      | Dual   | Random     |                  |                 | [link](https://twinstrandbio.com/aml-assay/)                                                                                                                                        |\n| No       | Mutagenesis                                               | TwinStrand Biosciences      | Dual   | Random     |                  |                 | [link](https://twinstrandbio.com/mutagenesis-assay/)                                                                                                                                |\n| No       | UMI Adapter System                                        | Twist Biosciences           | Dual   | Random     | Inline (R1 & R2) | `5M2S+T 5M2S+T` | [link](https://www.twistbioscience.com/products/ngs/library-preparation/twist-umi-adapter-system)                                                                                   |\n\nColumn Definitions:\n\n- Assay: the name of the assay or kit\n- Company: the name of the company or vendor providing the assay or kit\n- Strand: Dual if both strands of a double-stranded source molecule are sequences (e.g. Duplex Sequencing), Single otherwise\n- Randomness: if the unique molecular identifiers (UMIs) are fully random (degenerate) or are synthesized from a fixed set\n- UMI Location: the location of UMIs within the reads.\n- Read Structure: the [`read_structure`][read-structure-link] describes how the bases in a sequencing run should be allocated into logical reads, including the unique molecular index(es)\n- URL: link(s) to vendor documentation or further information\n\nTo become \"Verified\" by `nf-core/fastquorum`, please open an issue and provide the maintainers with an example dataset that can be shared publicly.\nThe dataset or a subset will be added to [nf-core/test-datasets](https://github.com/nf-core/test-datasets/tree/fastquorum).\nPlease reach out to maintainers if additional support is needed to prepare or select such data.\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow.Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2,read_structure\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,5M2S+T 5M2S+T\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\nThe `sample` column provides a unique identifier for the given sample, while the `read_structure` describes how the bases in a sequencing run should be allocated into logical reads, including the unique molecular index(es).\n(Please see the [fgbio documentation](https://github.com/fulcrumgenomics/fgbio/wiki/Read-Structures) for detailed information on read structure syntax and formatting.)\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/fastquorum \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --genome GRCh38 \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nTwo modes of running this pipeline are supported:\n\n1. Research and Development (R&D): use `--mode rd` or `params.mode=rd`. This mode is desirable to be able to branch off from the pipeline and test e.g. multiple consensus calling or filtering parameters\n2. High Throughput (HT): use `--mode ht` or `params.mode=ht`. This mode is intended for high throughput production environments where performance and throughput take precedence over flexibility\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/fastquorum/usage) and the [parameter documentation](https://nf-co.re/fastquorum/parameters).\n\nSee also:\n\n1. The [fgbio Best Practice FASTQ -> Consensus Pipeline][fgbio-best-practices-link]\n2. [Read structures](https://github.com/fulcrumgenomics/fgbio/wiki/Read-Structures) as required in the input sample sheet.\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/fastquorum/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/fastquorum/output).\n\n## Credits\n\nnf-core/fastquorum was originally written and is primarily maintained by Nils Homer.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Nils Homer](https://github.com/nh13)\n\n## Acknowledgements\n\nWe thank [Fulcrum Genomics](https://www.fulcrumgenomics.com/) for their extensive assistance in the development of this pipeline.\n\n<p align=\"left\">\n<a href=\"https://fulcrumgenomics.com\">\n<img width=\"500\" height=\"100\" src=\"docs/images/Fulcrum.svg\" alt=\"Fulcrum Genomics\"/>\n</a>\n</p>\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#fastquorum` channel](https://nfcore.slack.com/channels/fastquorum) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/fastquorum for your analysis, please cite [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.11267672.svg)](https://doi.org/10.5281/zenodo.11267672) for this pipeline and [![DOI](https://zenodo.org/badge/53011104.svg)](https://zenodo.org/doi/10.5281/zenodo.10456900) for [`fgbio`](https://github.com/fulcrumgenomics/fgbio).\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\n[fgbio-best-practices-link]: https://github.com/fulcrumgenomics/fgbio/blob/main/docs/best-practice-consensus-pipeline.md\n[duplex-seq-link]: https://en.wikipedia.org/wiki/Duplex_sequencing\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "985",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/985?version=4",
        "name": "nf-core/fastquorum",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "consensus",
            "umi",
            "umis",
            "unique-molecular-identifier"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-04-13",
        "versions": 4
    },
    {
        "create_time": "2025-04-03",
        "creators": [
            "Phuong Doan"
        ],
        "description": "# AnnoAudit - Annotation Auditor\r\n\r\nAnnoAudit is a robust Nextflow pipeline designed to evaluate the quality of genomic annotations through a multifaceted approach.\r\n\r\n## Overview of the workflow\r\n\r\nThe workflow assess the annotation quality based on different criteria:\r\n- Protein evidence support\r\n- RNASeq evidence support\r\n- Statistics of the predictions (i.e., gene length, exon number, etc.)\r\n- Ortholog analysis (BUSCO, OMArk)\r\n\r\n### Input data\r\n\r\n- Reference genome `genome.[.fna, .fa, .fasta]`\r\n- Annotation output `annotation.gff`\r\n- RNAseq data listed in a metadata csv file. Input type can be mixed between long and short reads, with the option of single-end read. The input file should follow the format below:\r\n\r\n```\r\nsample_id,R1_path,R2_path,read_type\r\nSAM1,/path/to/R1,,long             # For long reads\r\nSAM2,/path/to/R1,/path/to/R2,short # For PE reads\r\nSAM3,/path/to/R1,,short            # For SE reads\r\n```\r\n\r\n- Protein reference data in `fasta` format for evaluation, if not given, then the `Uniprot-SwissProt` will be downloaded and used.\r\n\r\n### Pipeline steps\r\n\r\n![Pipeline](./assets/images/annoaudit-workflow.svg)\r\n\r\nThe main pipeline is divided into five different subworkflows.\r\n- `General statistics`: Calculate the statistics obtained from the GFF file.\r\n- `RNASeq analysis`: Map the RNASeq data to the genome (or with provided mapping bam file) to generate exon, intron, transcript coverage.\r\n- `Ortholog analysis`: Compare the predicted proteome to known database using BUSCO and OMArk (OMA database).\r\n- `Protein analysis`: Blast the predicted proteome to a known database (could be of relative species) to obtain best reciprocal hits (BRH), then generate statistics based on the BRH results.\r\n\r\n### Output data\r\n\r\n- Output text file contain the statistic calculated from the input `GFF` file: \r\n  - General statistics\r\n  - BUSCO\r\n  - OMArk\r\n  - PSAURON\r\n  - Best reciprocal hits\r\n  - RNASeq analysis\r\n\r\n## Prerequisites\r\n\r\nThe following programs are required to run the workflow and the listed version were tested. \r\n\r\n`nextflow v23.04.0 or higher`\r\n\r\n`singularity`\r\n\r\n`docker` (have not been tested but in theory should work fine)\r\n\r\n## Installation\r\n\r\nSimply get the code from github or workflowhub and directly use it for the analysis with `nextflow`.\r\n\r\n```\r\ngit clone https://github.com/ERGA-consortium/pipelines\r\n```\r\n\r\n## Running AnnoAudit\r\n\r\n### Before running the pipeline (IMPORTANT)\r\n\r\nOne thing with Nextflow is that it is running off a Java Virtual Machine (JVM), and it will try to use all available memory for Nextflow even though it is unnecessary (for workflow management and job control). This will cause much trouble if you run a job on an HPC cluster. Thus, to minimize the effect of it, we need to limit the maximum memory the JVM can use.\r\n\r\n```\r\nexport NFX_OPTS=\"-Xms=512m -Xmx=3g\"\r\n```\r\n\r\n`-Xms` is the lower limit, which is set as 512 MB.\r\n`-Xmx` is the upper limit, which in this case is set as 3 GB.\r\nPlease modify this according to your situation.\r\n\r\n### How to run the code\r\n\r\n```\r\nnextflow run main.nf --genome genome.fasta \\\r\n      --gff annotation.gff3 \\\r\n      --rnaseq metadata.csv [--genome_bam path/to/the/mapped/bam]\\\r\n      --outdir OUTDIR_NAME \\\r\n      --taxon_id 9606 [Optional] \\\r\n      --ncbi_query_email xxxx \\\r\n      --rm -resume\r\n```\r\n\r\n### Other parameters for running the analysis\r\n\r\n```\r\nInput parameter:\r\n--genome                  Draft genome fasta file contain the assembled contigs/scaffolds.\r\n--gff                     Annotation file that need to be evaluated.\r\n--genome_bam              BAM file contain the mapped information from the RNASeq to the genome FASTA.\r\n--rnaseq                  A metadata CSV file following the pattern: sample_id,R1_path,R2_path,read_type. Required if `genome_bam` is not provided.\r\n--taxon_id                Taxon ID for identifying BUSCO lineage and download protein data from NCBI if needed.\r\n--ncbi_query_email        Email for querying protein from NCBI database.\r\n\r\nOptional input:\r\n--protein                  Fasta file containing translated protein sequences from the GFF for running evaluation. If not specified, the workflow will automatically extract it from the\r\n `genome` and `gff`.\r\n--ref_protein              Fasta file containing the reference protein sequences to be used for evaluation. Ideally this should come from the same species and/or closely related specie\r\ns. If not provided, the workflow will download the proteome from NCBI or using Uniprot SwissProt database.\r\n--lineage                  Lineage information providing for BUSCO, if not provided, the workflow will automatically search for the closest lineage. Example: eudicots_odb10.\r\n--genetic_code             Genetic code for translation of protein.\r\n--stranding                Strandness of the RNASeq reads used for extraction of junction position using `regtools`.\r\n\r\nDatabase input:\r\n--odb_version              odb version to choose to run BUSCO, option: odb12, odb10. [default: odb12]\r\n--busco_database           Pathway to the BUSCO databse store locally. [default: null]\r\n--oma_database             Pathway to the OMA database, if not specified, the workflow will download it automatically. [default: null]\r\n--ref_protein              Pathway to the reference proteome for comparison. [default: null]\r\n--ncbi_query_count         Number of protein to extract from the NCBI database. [default: 100000]\r\n--ncbi_query_batch         Number of protein to query for each batch. [default: 1000]\r\n\r\nOutput option:\r\n--pdf                      Output PDF name. [default: AnnoAudit_Report.pdf]\r\n--outdir                   Output directory. [default: /env/export/bigtmp2/pdoan/evaluate_pipeline]\r\n--tracedir                 Pipeline information. [default: /env/export/bigtmp2/pdoan/evaluate_pipeline/pipeline_info]\r\n--publish_dir_mode         Option for nextflow to move data to the output directory. [default: copy]\r\n--tmpdir                   Database directory. [default: /env/export/bigtmp2/pdoan/evaluate_pipeline/tmpdir]\r\n\r\nConditioning options:\r\n--rnaseq_single             If specify, will run `featureCounts` in single read mode, this is necessary if the mapped RNASeq is single-ended. [default: false]\r\n--run_blast                 If specify, will use `blast` for running best reciprocal hits instead of DIAMOND. [default: false]\r\n--query_ncbi_prot           If specify, will download the reference proteome from NCBI, other wise, will use the provided proteom or Uniprot SwissProt. [default: true]\r\n--cds_only                  If specify, only extracting information from the GFF file using the CDS line. [default: \"False\"]\r\n\r\n--help                   Print help message.\r\n\r\nExecution/Engine profiles:\r\nThe pipeline supports profiles to run via different Executers and Engines e.g.: -profile local,conda\r\n\r\nExecuter (choose one):\r\n  local\r\n  slurm\r\n\r\nEngines (choose one):\r\n  docker\r\n  singularity\r\n  apptainer\r\n\r\nPer default: -profile slurm,singularity is executed.\r\n```\r\n\r\n## Example output\r\n\r\nBelow is the sample output of this workflow. The example PDF output is located in `assest` folder.\r\n\r\n```\r\n|General Statistics                 | Value           |\r\n-------------------------------------------------------\r\n|num_genes                          | 36391           |\r\n|num_genes_without_introns          | 12968 (35.64%)  |\r\n|mean_gene_length                   | 2359.57         |\r\n|median_gene_length                 | 1562.0          |\r\n|num_exons                          | 149725          |\r\n|mean_exons_per_gene                | 4.11            |\r\n|median_exons_per_gene              | 2.0             |\r\n|num_exon_3n                        | 76783 (51.28%)  |\r\n|num_exon_3n1                       | 36932 (24.67%)  |\r\n|num_exon_3n2                       | 36010 (24.05%)  |\r\n|mean_cds_length                    | 1091.4          |\r\n|median_cds_length                  | 873.0           |\r\n|total_cds_length                   | 39717145        |\r\n|percentage_cds_coverage            | 10.64%          |\r\n|num_introns                        | 113334          |\r\n|mean_intron_length                 | 407.2           |\r\n|median_intron_length               | 149.0           |\r\n|short_intron_<120_3n0_without_stop | 4324 (3.82)%    |\r\n|long_intron_>120_3n0_without_stop  | 1185 (1.05)%    |\r\n|short_intron_<120_3n1_without_stop | 4205 (3.71)%    |\r\n|long_intron_>120_3n1_without_stop  | 1291 (1.14)%    |\r\n|short_intron_<120_3n2_without_stop | 4319 (3.81)%    |\r\n|long_intron_>120_3n2_without_stop  | 1249 (1.10)%    |\r\n|short_intron_<120_3n0_with_stop    | 12073 (10.65)%  |\r\n|long_intron_>120_3n0_with_stop     | 20332 (17.94)%  |\r\n|short_intron_<120_3n1_with_stop    | 11652 (10.28)%  |\r\n|long_intron_>120_3n1_with_stop     | 20486 (18.08)%  |\r\n|short_intron_<120_3n2_with_stop    | 11733 (10.35)%  |\r\n|long_intron_>120_3n2_with_stop     | 20485 (18.07)%  |\r\n\r\n|BUSCO                              | Value           |\r\n-------------------------------------------------------\r\n|lineage_dataset                    | poales_odb10    |\r\n|complete                           | 97.6%           |\r\n|single_copy                        | 95.8%           |\r\n|multi_copy                         | 1.8%            |\r\n|fragmented                         | 0.2%            |\r\n|missing                            | 2.2%            |\r\n|num_markers                        | 4896            |\r\n|domain                             | eukaryota       |\r\n\r\n\r\n|OMARK                              | Value           |\r\n-------------------------------------------------------\r\n|OMA_clade                          | Oryza           |\r\n|num_conserved_hogs                 | 15087           |\r\n|single                             | 13316 (88.26%)  |\r\n|duplicated                         | 1353 (8.97%)    |\r\n|duplicated_unexpected              | 1101 (7.30%)    |\r\n|duplicated_expected                | 252 (1.67%)     |\r\n|missing                            | 418 (2.77%)     |\r\n|num_proteins_in_proteome           | 36387           |\r\n|total_consistent                   | 30365 (83.45%)  |\r\n|consistent_partial_hits            | 1803 (4.96%)    |\r\n|consistent_fragmented              | 1625 (4.47%)    |\r\n|total_inconsistent                 | 2283 (6.27%)    |\r\n|inconsistent_partial_hits          | 517 (1.42%)     |\r\n|inconsistent_fragmented            | 1444 (3.97%)    |\r\n|total_contaminants                 | 0 (0.00%)       |\r\n|contaminants_partial_hits          | 0 (0.00%)       |\r\n|contaminants_fragmented            | 0 (0.00%)       |\r\n|total_unknown                      | 3739 (10.28%)   |\r\n\r\n|PSAURON                            | Value           |\r\n-------------------------------------------------------\r\n|psauron_score                      | 83.8            |\r\n|true_count                         | 30494           |\r\n|false_count                        | 5893            |\r\n|median_score                       | 0.98278         |\r\n|max_score                          | 1.0             |\r\n|min_score                          | 0.00022         |\r\n\r\n\r\n|Best Reciprocal Hits               | Value           |\r\n-------------------------------------------------------\r\n|num_best_reciprocal_hits           | 29185           |\r\n|num_splitting_genes_08             | 932 (3.19%)     |\r\n|num_splitting_genes_05             | 0 (0.0%)        |\r\n|num_fusion_genes_12                | 437 (1.5%)      |\r\n|num_fusion_genes_15                | 482 (1.65%)     |\r\n|KL_divergence_normalized           | 0.0105          |\r\n|JS_divergence_normalized           | 0.0023          |\r\n|Wasserstein_distance               | 2.480915        |\r\n\r\n\r\n|RNASeq                             | Value           |\r\n-------------------------------------------------------\r\n|mapping_rate                       | 96.27%          |\r\n|primary_mapping_rate               | 95.83%          |\r\n|properly_paired                    | 92.47%          |\r\n|num_gene_unsupported               | 9445 (25.95%)   |\r\n|num_exon_unsupported               | 20232 (13.51%)  |\r\n|num_intron_supported               | 107202          |\r\n|num_intron_supported_canonical     | 107131 (99.93%) |\r\n|num_intron_supported_non_canonical | 71 (0.07%)      |\r\n```\r\n\r\n## Performance of the workflow on assessing annotation\r\n\r\nTo be added\r\n\r\n## Future work\r\n\r\n- Adding other plots for easier evaluation\r\n- Perform comparative performance with different genomes",
        "doi": "10.48546/workflowhub.workflow.1330.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1330",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1330?version=1",
        "name": "AnnoAudit - Annotation Auditor",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Laboratory for Genomics and Biodiversity (LBGB)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "biodiversity",
            "bioinformatics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-04-03",
        "versions": 1
    },
    {
        "create_time": "2025-03-28",
        "creators": [],
        "description": "Lysozyme in water full COMPSs application. Added new WRROC profile: [Provenance Run Crate](https://www.researchobject.org/workflow-run-crate/profiles/provenance_run_crate/)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1326",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1326?version=1",
        "name": "Lysozyme in water full with new provenance run features",
        "number_of_steps": 0,
        "projects": [
            "Workflows and Distributed Computing"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "COMPSs",
        "update_time": "2025-03-28",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Engy Nasr",
            "B\u00e9r\u00e9nice Batut",
            "Paul Zierep"
        ],
        "description": "Microbiome - Variant calling and Consensus Building",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in description",
        "id": "1063",
        "keep": "Keep",
        "latest_version": 5,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1063?version=5",
        "name": "allele-based-pathogen-identification/main",
        "number_of_steps": 23,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "snpSift_filter",
            "regexColumn1",
            "Count1",
            "minimap2",
            "Cut1",
            "CONVERTER_gz_to_uncompressed",
            "tp_head_tool",
            "bcftools_norm",
            "samtools_depth",
            "samtools_coverage",
            "Remove beginning1",
            "table_compute",
            "collapse_dataset",
            "Paste1",
            "clair3",
            "tp_cut_tool",
            "bcftools_consensus",
            "snpSift_extractFields"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 5
    },
    {
        "create_time": "2025-10-08",
        "creators": [
            "Delphine Lariviere"
        ],
        "description": "# Contiging Solo w/HiC:\n\nGenerate phased assembly based on PacBio Hifi Reads using HiC data from the same individual for phasing.\n\n## Inputs\n\n1. Hifi long reads [fastq]\n2. HiC forward reads (if multiple input files, concatenated in same order as reverse reads) [fastq]\n3. HiC reverse reads (if multiple input files, concatenated in same order as forward reads) [fastq]\n4. K-mer database [meryldb]\n5. Genome profile summary generated by Genomescope [txt]\n6. Name of first assembly\n7. Name of second assembly\n\n## Outputs\n\n1. Haplotype 1 assembly ([fasta] and [gfa])\n2. Haplotype 2 assembly ([fasta] and [gfa])\n3. QC: BUSCO report for both assemblies\n4. QC: Merqury report for both assemblies\n5. QC: Assembly statistics for both assemblies\n6. QC: Nx plot for both assemblies\n7. QC: Size plot for both assemblies",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "641",
        "keep": "Reject",
        "latest_version": 31,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/641?version=31",
        "name": "Assembly-Hifi-HiC-phasing-VGP4/main",
        "number_of_steps": 32,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "reviewed",
            "vgp"
        ],
        "tools": [
            "",
            "cutadapt",
            "tp_replace_in_line",
            "tp_awk_tool",
            "Add_a_column1",
            "tp_grep_tool",
            "Convert characters1",
            "Cut1",
            "gfastats",
            "tp_sed_tool",
            "bandage_image",
            "merqury",
            "Paste1",
            "param_value_from_file",
            "hifiasm",
            "multiqc",
            "busco"
        ],
        "type": "Galaxy",
        "update_time": "2025-10-08",
        "versions": 31
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Peter J Bailey",
            "Bailey PJ",
            "Alexander Peltzer",
            "Botvinnik O",
            "Olga Botvinnik",
            "Marques de Almeida F",
            "Peltzer A",
            "Sturm G"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-scrnaseq_logo_dark.png\">\n    <img alt=\"nf-core/scrnaseq\" src=\"docs/images/nf-core-scrnaseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/scrnaseq/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/scrnaseq/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/scrnaseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/scrnaseq/actions/workflows/linting.yml)\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/scrnaseq/results)\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.3568187-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.3568187)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/scrnaseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23scrnaseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/scrnaseq)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/scrnaseq** is a bioinformatics best-practice analysis pipeline for processing 10x Genomics single-cell RNA-seq data.\n\nThis is a community effort in building a pipeline capable to support:\n\n- SimpleAF(Alevin-Fry) + AlevinQC\n- STARSolo\n- Kallisto + BUStools\n- Cellranger\n\n> [!IMPORTANT]\n> Cellranger is a commercial tool from 10X Genomics Inc. and falls under the EULA from 10X Genomics Inc. The container provided for the CellRanger functionality in this pipeline has been built by the nf-core community and is therefore _not supported by 10X genomics_ directly. We are in discussions with 10X on how to improve the user experience and licence situation for both us as a community as well as 10X and end users and will update this statement here accordingly.\n\n## Documentation\n\nThe nf-core/scrnaseq pipeline comes with documentation about the pipeline [usage](https://nf-co.re/scrnaseq/usage), [parameters](https://nf-co.re/scrnaseq/parameters) and [output](https://nf-co.re/scrnaseq/output).\n\n![scrnaseq workflow](docs/images/scrnaseq_pipeline_V3.0-metro_clean.png)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2,expected_cells\npbmc8k,pbmc8k_S1_L007_R1_001.fastq.gz,pbmc8k_S1_L007_R2_001.fastq.gz,10000\npbmc8k,pbmc8k_S1_L008_R1_001.fastq.gz,pbmc8k_S1_L008_R2_001.fastq.gz,10000\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/scrnaseq \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --fasta GRCm38.p6.genome.chr19.fa \\\n   --gtf gencode.vM19.annotation.chr19.gtf \\\n   --protocol 10XV2 \\\n   --aligner <simpleaf/kallisto/star/cellranger> \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/scrnaseq/usage) and the [parameter documentation](https://nf-co.re/scrnaseq/parameters).\n\n## Decision Tree for users\n\nThe nf-core/scrnaseq pipeline features several paths to analyze your single cell data. Future additions will also be done soon, e.g. the addition of multi-ome analysis types. To aid users in analyzing their data, we have added a decision tree to help people decide on what type of analysis they want to run and how to choose appropriate parameters for that.\n\n```mermaid\ngraph TD\n    A[sc RNA] -->|alevin-fry| B(h5ad/seurat/mtx matrices)\n    A[sc RNA] -->|CellRanger| B(h5ad/seurat/mtx matrices)\n    A[sc RNA] -->|kbpython| B(h5ad/seurat/mtx matrices)\n    A[sc RNA] -->|STARsolo| B(h5ad/seurat/mtx matrices)\n```\n\nOptions for the respective alignment method can be found [here](https://github.com/nf-core/scrnaseq/blob/dev/docs/usage.md#aligning-options) to choose between methods.\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/scrnaseq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/scrnaseq/output).\n\n## Credits\n\nnf-core/scrnaseq was originally written by Bailey PJ, Botvinnik O, Marques de Almeida F, Gabernet G, Peltzer A, Sturm G.\n\nWe thank the following people and teams for their extensive assistance in the development of this pipeline:\n\n- @heylf\n- @KevinMenden\n- @FloWuenne\n- @rob-p\n- [GHGA](https://www.ghga.de/)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#scrnaseq` channel](https://nfcore.slack.com/channels/scrnaseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/scrnaseq for your analysis, please cite it using the following doi: [10.5281/zenodo.3568187](https://doi.org/10.5281/zenodo.3568187)\n\nThe basic benchmarks that were used as motivation for incorporating the available modular workflows can be found in [this publication](https://www.biorxiv.org/content/10.1101/673285v2).\n\nWe offer all three paths for the processing of scRNAseq data so it remains up to the user to decide which pipeline workflow is chosen for a particular analysis question.\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1021",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1021?version=17",
        "name": "nf-core/scrnaseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "10x-genomics",
            "10xgenomics",
            "cellranger",
            "alevin",
            "bustools",
            "kallisto",
            "rna-seq",
            "single-cell",
            "star-solo"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-26",
        "versions": 17
    },
    {
        "create_time": "2025-03-26",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-pacvar_logo_dark.png\">\n    <img alt=\"nf-core/pacvar\" src=\"docs/images/nf-core-pacvar_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/pacvar/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/pacvar/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/pacvar/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/pacvar/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/pacvar/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/pacvar)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23pacvar-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/pacvar)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\nnf-core/pacvar is a bioinformatics pipeline that processes long-read PacBio data. Specifically, the pipeline provides two workflows: one for processing whole-genome sequencing data, and another for processing reads from the PureTarget expansion panel offered by PacBio. This second workflow characterizes tandem repeats. Because the pipeline is designed for PacBio reads, it uses PacBio\u2019s officially released tools.\n\n![nf-core/pacvar metro map](docs/images/pacvar_white_background.png)\n\nWorkflow Overview\n\n1. Demultiplex reads ([`lima`](https://lima.how))\n2. Align reads ([`pbmm2`](https://github.com/PacificBiosciences/pbmm2))\n3. Sort and index alignments ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\n\nWGS Workflow Overview\n\n1. Choice of SNP calling routes:\n   a. ([`deepvariant`](https://github.com/google/deepvariant))\n   b. ([`HaplotypeCaller`](https://gatk.broadinstitute.org/hc/en-us/articles/360037225632-HaplotypeCaller))\n2. Call SVs ([`pbsv`](https://github.com/PacificBiosciences/pbsv))\n3. Index VCF files ([`bcftools`](https://samtools.github.io/bcftools/bcftools.html))\n4. Phase SNPs, SVs and BAM files ([`hiphase`](https://github.com/PacificBiosciences/HiPhase))\n\nTandem Repeat Workflow Overview\n\n1. Genotype tandem repeats - produce spanning bams and vcf ([`TRGT`](https://github.com/PacificBiosciences/trgt))\n2. Index and Sort tandem tepeat spanning bam ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\n3. Plot repeat motif plots ([`TRGT`](https://github.com/PacificBiosciences/trgt))\n4. Sort spanning VCF ([`bcftools`](https://samtools.github.io/bcftools/bcftools.html))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,bam,pbi\nCONTROL,AEG588A1_S1_L002_R1_001.bam,AEG588A1_S1_L002_R1_001.pbi\n```\n\nNote that the `.pbi` file is not required. If you choose not to include it, your input file might look like this:\n\n```csv\nsample,bam,pbi\nCONTROL,AEG588A1_S1_L002_R1_001.bam\n```\n\nEach row represents an unaligned bam file and their associated index (optional).\n\nNow, you can run the pipeline. Below is an example\n\n```bash\nnextflow run nf-core/pacvar \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --workflow <wgs/repeat> \\\n   --barcodes barcodes.bed \\\n   --intervals intervals.bed \\\n   --genome <GENOME NAME (e.g. GATK.GRCh38)> \\\n   --outdir <OUTDIR>\n```\n\noptional paramaters include: `--skip_demultiplexing`, `--skip_snp`, `--skip_sv`, `--skip_phase`.\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/pacvar/usage) and the [parameter documentation](https://nf-co.re/pacvar/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/pacvar/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/pacvar/output).\n\n## Credits\n\nnf-core/pacvar was originally written by Tanya Sarkin Jain.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#pacvar` channel](https://nfcore.slack.com/channels/pacvar) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/pacvar for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1293",
        "keep": "To Curate",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1293?version=3",
        "name": "nf-core/pacvar",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "wgs",
            "long-read",
            "pacbio",
            "puretarget",
            "variant-calling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-26",
        "versions": 3
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "@kbestak None",
            "@kbestak None",
            "@FloWuenne None",
            "@FloWuenne None"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-molkart_logo_dark.png\">\n    <img alt=\"nf-core/molkart\" src=\"docs/images/nf-core-molkart_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/molkart/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/molkart/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/molkart/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/molkart/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/molkart/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.10650748-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.10650748)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/molkart)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23molkart-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/molkart)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/molkart** is a pipeline for processing Molecular Cartography data from Resolve Bioscience (combinatorial FISH). It takes as input a table of FISH spot positions (x,y,z,gene), a corresponding DAPI image (`TIFF` format) and optionally an additional staining image in the `TIFF` format. nf-core/molkart performs end-to-end processing of the data including image processing, QC filtering of spots, cell segmentation, spot-to-cell assignment and reports quality metrics such as the spot assignment rate, average spots per cell and segmentation mask size ranges.\n\n<p align=\"center\">\n    <img title=\"Molkart Workflow\" src=\"docs/images/molkart_workflow.png\" width=100%>\n</p>\n\nImage preprocessing\n\n- Fill the grid pattern in provided images ([`Mindagap`](https://github.com/ViriatoII/MindaGap))\n- Optionally apply contrast-limited adaptive histogram equalization\n- If a second (membrane) image is present, combine images into a multichannel stack (if required for segmentation)\n\nCell segmentation\n\n- Apply cell segmentation based on provided images, available options are: - [`Cellpose`](https://www.cellpose.org/) - [`Mesmer`](https://deepcell.readthedocs.io/en/master/API/deepcell.applications.html#mesmer) - [`ilastik`](https://www.ilastik.org/) - [`Stardist`](https://github.com/stardist/stardist)\n- Filter cells based on cell size to remove artifacts\n\nSpot processing\n\n- Find duplicated spots near grid lines ([`Mindagap`](https://github.com/ViriatoII/MindaGap))\n- Assign spots to segmented cells\n\nQuality control\n\n- Create quality-control metrics specific to this pipeline\n- provide them to ([`MultiQC`](http://multiqc.info/)) to create a report\n\n## Usage\n\n:::note\nIf you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\nto set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\nwith `-profile test` before running the workflow on actual data.\n:::\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,nuclear_image,spot_locations,membrane_image\nsample0,sample0_DAPI.tiff,sample0_spots.txt,sample0_WGA.tiff\n```\n\nEach row represents an FOV (field-of-view). Columns represent the sample ID (all must be unique), the path to the respective nuclear image, the spot table, and optionally the path to the respective membrane image (or any additional image to improve segmentation).\n\nNow, you can run the pipeline using all default values with:\n\n```bash\nnextflow run nf-core/molkart \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/molkart/usage) and the [parameter documentation](https://nf-co.re/molkart/parameters).\n\n## Pipeline output\n\nThe pipeline outputs a matched cell-by-transcript table based on deduplicated spots and segmented cells, as well as preprocessing and segmentation intermediaries.\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/molkart/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/molkart/output).\n\n## Credits\n\nnf-core/molkart was originally written by @kbestak, @FloWuenne.\n\nWe thank [Maxime U Garcia](https://github.com/maxulysse) for his assistance and support in the development of this pipeline.\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#molkart` channel](https://nfcore.slack.com/channels/molkart) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\nIf you use nf-core/molkart for your analysis, please cite it using the following doi: [10.5281/zenodo.10650749](https://doi.org/10.5281/zenodo.10650749)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1001",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1001?version=2",
        "name": "nf-core/molkart",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "segmentation",
            "transcriptomics",
            "fish",
            "image-processing",
            "imaging",
            "molecularcartography",
            "single-cell",
            "spatial"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-26",
        "versions": 2
    },
    {
        "create_time": "2025-03-12",
        "creators": [
            "Damon-Lee Pointon"
        ],
        "description": "# sanger-tol/curationpretext\r\n\r\n[![GitHub Actions CI Status](https://github.com/sanger-tol/curationpretext/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/curationpretext/actions?query=workflow%3A%22nf-core+CI%22)\r\n[![GitHub Actions Linting Status](https://github.com/sanger-tol/curationpretext/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/curationpretext/actions?query=workflow%3A%22nf-core+linting%22)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.12773958-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.12773958)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/curationpretext)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/curationpretext** is a bioinformatics pipeline typically used in conjunction with [TreeVal](https://github.com/sanger-tol/treeval) to generate pretext maps (and optionally telomeric, gap, coverage, and repeat density plots which can be ingested into pretext) for the manual curation of high quality genomes.\r\n\r\nThis is intended as a supplementary pipeline for the [treeval](https://github.com/sanger-tol/treeval) project. This pipeline can be simply used to generate pretext maps, information on how to run this pipeline can be found in the [usage documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/usage).\r\n\r\n![Workflow Diagram](./assets/CurationPretext.png)\r\n\r\n1. Generate Maps - Generates pretext maps as well as a static image.\r\n\r\n2. Accessory files - Generates the repeat density, gap, telomere, and coverage tracks.\r\n\r\n## Usage\r\n\r\n> **Note**\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\r\n> to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\r\n> with `-profile test` before running the workflow on actual data.\r\n\r\nCurrently, the pipeline uses the following flags:\r\n\r\n- `--input`\r\n\r\n  - The absolute path to the assembled genome in, e.g., `/path/to/assembly.fa`\r\n\r\n- `--reads`\r\n\r\n  - The directory of the fasta files generated from longread reads, e.g., `/path/to/fasta/`\r\n\r\n- `--read_type`\r\n\r\n  - The type of longread data you are utilising, e.g., ont, illumina, hifi.\r\n\r\n- `--aligner`\r\n\r\n  - The aligner yopu wish to use for the coverage generation, defaults to bwamem2 but minimap2 is also supported.\r\n\r\n- `--cram`\r\n\r\n  - The directory of the cram _and_ cram.crai files, e.g., `/path/to/cram/`\r\n\r\n- `--map_order`\r\n\r\n  - hic map scaffold order, input either `length` or `unsorted`\r\n\r\n- `--teloseq`\r\n\r\n  - A telomeric sequence, e.g., `TTAGGG`\r\n\r\n- `-entry`\r\n  - ALL_FILES is the default and generates all accessory files as well as pretext maps\r\n  - MAPS_ONLY generates only the pretext maps and static images\r\n\r\nNow, you can run the pipeline using:\r\n\r\n#### For ALL_FILES run\r\n\r\n```bash\r\nnextflow run sanger-tol/curationpretext \\\r\n  --input { input.fasta } \\\r\n  --cram { path/to/cram/ } \\\r\n  --reads { path/to/longread/fasta/ } \\\r\n  --read_type { default is \"hifi\" }\r\n  --sample { default is \"pretext_rerun\" } \\\r\n  --teloseq { default is \"TTAGGG\" } \\\r\n  --map_order { default is \"unsorted\" } \\\r\n  --outdir { OUTDIR } \\\r\n  -profile <docker/singularity/{institute}>\r\n\r\n```\r\n\r\n#### For MAPS_ONLY run\r\n\r\n```bash\r\nnextflow run sanger-tol/curationpretext \\\r\n  --input { input.fasta } \\\r\n  --cram { path/to/cram/ } \\\r\n  --reads { path/to/longread/fasta/ } \\\r\n  --read_type { default is \"hifi\" }\r\n  --sample { default is \"pretext_rerun\" } \\\r\n  --teloseq { default is \"TTAGGG\" } \\\r\n  --map_order { default is \"unsorted\" } \\\r\n  --outdir { OUTDIR } \\\r\n  -profile <docker/singularity/{institute}> \\\r\n  -entry MAPS_ONLY \\\r\n```\r\n\r\n> **Warning:**\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\r\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n\r\nFor more details, please refer to the [usage documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/usage) and the [parameter documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/parameters).\r\n\r\n## Pipeline output\r\n\r\nTo see the the results of a test run with a full size dataset refer to the [results](https://pipelines.tol.sanger.ac.uk/curationpretext/results) tab on the sanger-tol/curationpretext website pipeline page.\r\nFor more details about the output files and reports, please refer to the\r\n[output documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/output).\r\n\r\n## Credits\r\n\r\nsanger-tol/curationpretext was originally written by Damon-Lee B Pointon (@DLBPointon).\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- @muffato - For reviews.\r\n\r\n- @yumisims - TreeVal and Software.\r\n\r\n- @weaglesBio - TreeVal and Software.\r\n\r\n- @josieparis - Help with better docs and testing.\r\n\r\n- @mahesh-panchal - Large support with 1.2.0 in making the pipeline more robust with other HPC environments.\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#curationpretext` channel](https://nfcore.slack.com/channels/curationpretext) (you can join with [this invite](https://nf-co.re/join/slack)).\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/curationpretext for your analysis, please cite it using the following doi: [10.5281/zenodo.12773958](https://doi.org/10.5281/zenodo.12773958)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1321",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1321?version=1",
        "name": "sanger-tol/curationpretext",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Assembly",
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics"
        ],
        "tools": [
            "Nextflow",
            "PretextView"
        ],
        "type": "Nextflow",
        "update_time": "2025-03-12",
        "versions": 1
    },
    {
        "create_time": "2025-03-12",
        "creators": [
            "Damon-Lee Pointon"
        ],
        "description": "# ![sanger-tol/curationpretext](docs/images/curationpretext-light.png#gh-light-mode-only) ![sanger-tol/curationpretext](docs/images/curationpretext-dark.png#gh-dark-mode-only)\r\n\r\n[![GitHub Actions CI Status](https://github.com/sanger-tol/curationpretext/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/curationpretext/actions?query=workflow%3A%22nf-core+CI%22)\r\n[![GitHub Actions Linting Status](https://github.com/sanger-tol/curationpretext/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/curationpretext/actions?query=workflow%3A%22nf-core+linting%22)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.12773958-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.12773958)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/curationpretext)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/curationpretext** is a bioinformatics pipeline typically used in conjunction with [TreeVal](https://github.com/sanger-tol/treeval) to generate pretext maps (and optionally telomeric, gap, coverage, and repeat density plots which can be ingested into pretext) for the manual curation of high quality genomes.\r\n\r\nThis is intended as a supplementary pipeline for the [treeval](https://github.com/sanger-tol/treeval) project. This pipeline can be simply used to generate pretext maps, information on how to run this pipeline can be found in the [usage documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/usage).\r\n\r\n![Workflow Diagram](./assets/CurationPretext.png)\r\n\r\n1. Generate Maps - Generates pretext maps as well as a static image.\r\n\r\n2. Accessory files - Generates the repeat density, gap, telomere, and coverage tracks.\r\n\r\n## Usage\r\n\r\n> **Note**\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\r\n> to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\r\n> with `-profile test` before running the workflow on actual data.\r\n\r\nCurrently, the pipeline uses the following flags:\r\n\r\n- `--input`\r\n\r\n  - The absolute path to the assembled genome in, e.g., `/path/to/assembly.fa`\r\n\r\n- `--reads`\r\n\r\n  - The directory of the fasta files generated from longread reads, e.g., `/path/to/fasta/`\r\n\r\n- `--read_type`\r\n\r\n  - The type of longread data you are utilising, e.g., ont, illumina, hifi.\r\n\r\n- `--aligner`\r\n\r\n  - The aligner yopu wish to use for the coverage generation, defaults to bwamem2 but minimap2 is also supported.\r\n\r\n- `--cram`\r\n\r\n  - The directory of the cram _and_ cram.crai files, e.g., `/path/to/cram/`\r\n\r\n- `--map_order`\r\n\r\n  - hic map scaffold order, input either `length` or `unsorted`\r\n\r\n- `--teloseq`\r\n\r\n  - A telomeric sequence, e.g., `TTAGGG`\r\n\r\n- `-entry`\r\n  - ALL_FILES is the default and generates all accessory files as well as pretext maps\r\n  - MAPS_ONLY generates only the pretext maps and static images\r\n\r\nNow, you can run the pipeline using:\r\n\r\n#### For ALL_FILES run\r\n\r\n```bash\r\nnextflow run sanger-tol/curationpretext \\\r\n  --input { input.fasta } \\\r\n  --cram { path/to/cram/ } \\\r\n  --reads { path/to/longread/fasta/ } \\\r\n  --read_type { default is \"hifi\" }\r\n  --sample { default is \"pretext_rerun\" } \\\r\n  --teloseq { default is \"TTAGGG\" } \\\r\n  --map_order { default is \"unsorted\" } \\\r\n  --outdir { OUTDIR } \\\r\n  -profile <docker/singularity/{institute}>\r\n\r\n```\r\n\r\n#### For MAPS_ONLY run\r\n\r\n```bash\r\nnextflow run sanger-tol/curationpretext \\\r\n  --input { input.fasta } \\\r\n  --cram { path/to/cram/ } \\\r\n  --reads { path/to/longread/fasta/ } \\\r\n  --read_type { default is \"hifi\" }\r\n  --sample { default is \"pretext_rerun\" } \\\r\n  --teloseq { default is \"TTAGGG\" } \\\r\n  --map_order { default is \"unsorted\" } \\\r\n  --outdir { OUTDIR } \\\r\n  -profile <docker/singularity/{institute}> \\\r\n  -entry MAPS_ONLY \\\r\n```\r\n\r\n> **Warning:**\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\r\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n\r\nFor more details, please refer to the [usage documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/usage) and the [parameter documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/parameters).\r\n\r\n## Pipeline output\r\n\r\nTo see the the results of a test run with a full size dataset refer to the [results](https://pipelines.tol.sanger.ac.uk/curationpretext/results) tab on the sanger-tol/curationpretext website pipeline page.\r\nFor more details about the output files and reports, please refer to the\r\n[output documentation](https://pipelines.tol.sanger.ac.uk/curationpretext/output).\r\n\r\n## Credits\r\n\r\nsanger-tol/curationpretext was originally written by Damon-Lee B Pointon (@DLBPointon).\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- @muffato - For reviews.\r\n\r\n- @yumisims - TreeVal and Software.\r\n\r\n- @weaglesBio - TreeVal and Software.\r\n\r\n- @josieparis - Help with better docs and testing.\r\n\r\n- @mahesh-panchal - Large support with 1.2.0 in making the pipeline more robust with other HPC environments.\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#curationpretext` channel](https://nfcore.slack.com/channels/curationpretext) (you can join with [this invite](https://nf-co.re/join/slack)).\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/curationpretext for your analysis, please cite it using the following doi: [10.5281/zenodo.12773958](https://doi.org/10.5281/zenodo.12773958)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [
            "Genome visualisation"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Mapping",
            "Workflows"
        ],
        "filtered_on": "profil.* in description",
        "id": "1320",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1320?version=1",
        "name": "sanger-tol/curationpretext",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Assembly",
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-12",
        "versions": 1
    },
    {
        "create_time": "2025-02-25",
        "creators": [
            "Daniel Lundin"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-phyloplace_logo_dark.png\">\n    <img alt=\"nf-core/phyloplace\" src=\"docs/images/nf-core-phyloplace_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/phyloplace/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/phyloplace/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/phyloplace/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/phyloplace/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/phyloplace/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/phyloplace)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23phyloplace-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/phyloplace)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/phyloplace** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/phyloplace \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/phyloplace/usage) and the [parameter documentation](https://nf-co.re/phyloplace/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/phyloplace/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/phyloplace/output).\n\n## Credits\n\nnf-core/phyloplace was originally written by Daniel Lundin.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#phyloplace` channel](https://nfcore.slack.com/channels/phyloplace) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/phyloplace for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1009",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1009?version=2",
        "name": "nf-core/phyloplace",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "evolution",
            "evolutionary-tree",
            "phylogenetic-placement",
            "phylogenetics",
            "sequence-classification",
            "taxonomy-assignment"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-02-25",
        "versions": 2
    },
    {
        "create_time": "2025-02-10",
        "creators": [
            "Simon Heumos",
            "Michael L Heuer"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-pangenome_logo_dark.png\">\n    <img alt=\"nf-core/pangenome\" src=\"docs/images/nf-core-pangenome_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/pangenome/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/pangenome/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/pangenome/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/pangenome/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/pangenome/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.8202636-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.8202636)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/pangenome)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23pangenome-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/pangenome)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/pangenome** is a bioinformatics best-practice analysis pipeline for pangenome graph construction. The pipeline renders a collection of sequences into a pangenome graph. Its goal is to build a graph that is locally directed and acyclic while preserving large-scale variation. Maintaining local linearity is important for interpretation, visualization, mapping, comparative genomics, and reuse of pangenome graphs.\n\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\n\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/pangenome/results).\n\n<p align=\"center\">\n    <img title=\"Pangenome Workflow\" src=\"docs/images/pangenome_workflow.png\" width=100%>\n</p>\n\n## Pipeline summary\n\n- All versus all alignment (`WFMASH`)\n- Graph induction (`SEQWISH`)\n- Graph normalization (`SMOOTHXG`)\n- Remove redundancy (`GFAFFIX`)\n- Graph statistics and qualitative visualizations (`ODGI`)\n- Combine diagnostic information into a report (`MULTIQC`)\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/pangenome -r dev --input <BGZIPPED_FASTA> --n_haplotypes <NUM_HAPS_IN_FASTA> --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/pangenome/usage) and the [parameter documentation](https://nf-co.re/pangenome/parameters).\n\n## Advantages over [`PGGB`](https://github.com/pangenome/pggb)\n\nThis Nextflow pipeline version's major advantage is that it can distribute the usually computationally heavy all versus all alignment step across a whole cluster. It is capable of splitting the initial approximate alignments into problems of equal size. The base-level alignments are then distributed across several processes. Assuming you have a cluster with 10 nodes and you are the only one using it, we would recommend to set `--wfmash_chunks 10`.\nIf you have a cluster with 20 nodes, but you have to share it with others, maybe setting it to `--wfmash_chunks 10` could be a good fit, because then you don't have to wait too long for your jobs to finish.\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/pangenome/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/pangenome/output).\n\n## Credits\n\nnf-core/pangenome was originally adapted from [PGGB](https://github.com/pangenome/pggb) by [Simon Heumos](https://github.com/subwaystation), [Michael Heuer](https://github.com/heuermh).\n\n> [Simon Heumos](https://github.com/subwaystation) is currently the sole developer.\n\nMany thanks to all who have helped out and contributed along the way, including (but not limited to)\\*:\n\n| Name                                                       | Affiliation                                                                                                                                                                                                                                                                                                                                                                       |\n| ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [Philipp Ehmele](https://github.com/imipenem)              | [Institute of Computational Biology, Helmholtz Zentrum M\u00fcnchen, Munich, Germany](https://www.helmholtz-muenchen.de/icb/index.html)                                                                                                                                                                                                                                                |\n| [Gisela Gabernet](https://github.com/ggabernet)            | [Quantitative Biology Center (QBiC) T\u00fcbingen, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/research/research-infrastructure/quantitative-biology-center-qbic/) <br> [Department of Pathology, Yale School of Medicine, New Haven, USA](https://medicine.yale.edu/pathology/)                                                                                      |\n| [Erik Garrison](https://github.com/ekg)                    | [University of Tennessee Health Science Center, Memphis, Tennessee, TN, USA](https://uthsc.edu/)                                                                                                                                                                                                                                                                                  |\n| [Andrea Guarracino](https://github.com/AndreaGuarracino)   | [University of Tennessee Health Science Center, Memphis, Tennessee, TN, USA](https://uthsc.edu/)                                                                                                                                                                                                                                                                                  |\n| [Friederike Hanssen](https://github.com/FriederikeHanssen) | [Seqera](https://seqera/io)                                                                                                                                                                                                                                                                                                                                                       |\n| [Peter Heringer](https://github.com/heringerp)             | [Quantitative Biology Center (QBiC) T\u00fcbingen, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/research/research-infrastructure/quantitative-biology-center-qbic/) <br> [Biomedical Data Science, Department of Computer Science, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/department/) |\n| [Michael Heuer](https://github.com/heuermh)                | [Mammoth Biosciences, Inc., San Francisco, CA, USA](https://mammoth.bio)                                                                                                                                                                                                                                                                                                          |\n| [Lukas Heumos](https://github.com/zethson)                 | [Institute of Computational Biology, Helmholtz Zentrum M\u00fcnchen, Munich, Germany](https://www.helmholtz-muenchen.de/icb/index.html) <br> [Institute of Lung Biology and Disease and Comprehensive Pneumology Center, Helmholtz Zentrum M\u00fcnchen, Munich, Germany](https://www.helmholtz-muenchen.de/ilbd/the-institute/cpc/index.html)                                              |\n| [Simon Heumos](https://github.com/subwaystation)           | [Quantitative Biology Center (QBiC) T\u00fcbingen, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/research/research-infrastructure/quantitative-biology-center-qbic/) <br> [Biomedical Data Science, Department of Computer Science, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/department/) |\n| [Susanne Jodoin](https://github.com/SusiJo)                | [Quantitative Biology Center (QBiC) T\u00fcbingen, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/research/research-infrastructure/quantitative-biology-center-qbic/)                                                                                                                                                                                                    |\n| [J\u00falia Mir Pedrol](https://github.com/mirpedrol)           | [Quantitative Biology Center (QBiC) T\u00fcbingen, University of T\u00fcbingen, Germany](https://uni-tuebingen.de/en/research/research-infrastructure/quantitative-biology-center-qbic/)                                                                                                                                                                                                    |\n\n> \\* Listed in alphabetical order\n\n## Acknowledgments\n\n- [QBiC](https://www.qbic.uni-tuebingen.de)\n- [deNBI](https://www.denbi.de/)\n- [Human Pangenome Reference Consortium](https://humanpangenome.org)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#pangenome` channel](https://nfcore.slack.com/channels/pangenome) (you can join with [this invite](https://nf-co.re/join/slack)), or contact me [Simon Heumos](mailto:simon.heumos@qbic.uni-tuebingen.de?subject=[GitHub]%20nf-core/pangenome).\n\n## Citations\n\nIf you use nf-core/pangenome for your analysis, please cite it using the following doi: [10.5281/zenodo.8202636](https://doi.org/10.5281/zenodo.8202636)\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\n## Changelog\n\n[CHANGELOG](CHANGELOG.md)\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1007",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1007?version=5",
        "name": "nf-core/pangenome",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "pangenome"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-14",
        "versions": 5
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi"
        ],
        "description": "This workflow creates taxonomic summary tables for a specified taxonomic rank out of MAPseq's OTU tables output collection.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "OTU in description",
        "id": "1296",
        "keep": "Keep",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1296?version=2",
        "name": "taxonomic-rank-abundance-summary-table/main",
        "number_of_steps": 8,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "tp_awk_tool",
            "filter_tabular",
            "collection_column_join",
            "Grouping1",
            "map_param_value"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-02-04",
        "creators": [
            "Jonathan Manning"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-riboseq_logo_dark.png\">\n    <img alt=\"nf-core/riboseq\" src=\"docs/images/nf-core-riboseq_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/riboseq/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/riboseq/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/riboseq/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/riboseq/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/riboseq/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/riboseq)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23riboseq-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/riboseq)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/riboseq** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/riboseq \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/riboseq/usage) and the [parameter documentation](https://nf-co.re/riboseq/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/riboseq/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/riboseq/output).\n\n## Credits\n\nnf-core/riboseq was originally written by Jonathan Manning.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#riboseq` channel](https://nfcore.slack.com/channels/riboseq) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/riboseq for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1016",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1016?version=3",
        "name": "nf-core/riboseq",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-14",
        "versions": 3
    },
    {
        "create_time": "2025-02-04",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-fastqrepair_logo_dark.png\">\n    <img alt=\"nf-core/fastqrepair\" src=\"docs/images/nf-core-fastqrepair_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/fastqrepair/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/fastqrepair/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/fastqrepair/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/fastqrepair/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/fastqrepair/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/fastqrepair)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23fastqrepair-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/fastqrepair)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/fastqrepair** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/fastqrepair \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/fastqrepair/usage) and the [parameter documentation](https://nf-co.re/fastqrepair/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/fastqrepair/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/fastqrepair/output).\n\n## Credits\n\nnf-core/fastqrepair was originally written by Tommaso Mazza.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#fastqrepair` channel](https://nfcore.slack.com/channels/fastqrepair) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/fastqrepair for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1276",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1276?version=1",
        "name": "nf-core/fastqrepair",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "fastq",
            "corruption",
            "data-cleaning",
            "data-recovery",
            "fastq-corrupted",
            "fastq-format",
            "reads-interleaving",
            "recovery-tool",
            "unpaired-reads",
            "well-formed"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-14",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Mara Besemer"
        ],
        "description": "The MAPseq to Ampvis workflow processes MAPseq OTU tables and associated metadata for analysis in Ampvis2. This workflow involves reformatting MAPseq output datasets to produce structured output files suitable for Ampvis2.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "OTU in description",
        "id": "1275",
        "keep": "Keep",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1275?version=2",
        "name": "mapseq-to-ampvis2/main",
        "number_of_steps": 9,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "tp_awk_tool",
            "query_tabular",
            "ampvis2_load",
            "collection_column_join",
            "collapse_dataset"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Paul Zierep"
        ],
        "description": "MGnify's amplicon pipeline v5.0. Including the Quality control for single-end and paired-end reads, rRNA-prediction, and ITS sub-WFs.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "1274",
        "keep": "Keep",
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1274?version=2",
        "name": "mgnify-amplicon-pipeline-v5-complete/main",
        "number_of_steps": 20,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "",
            "tp_awk_tool",
            "CONVERTER_gz_to_uncompressed",
            "CONVERTER_uncompressed_to_gz",
            "__MERGE_COLLECTION__",
            "fastq_dl"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Paul Zierep"
        ],
        "description": "Classification and visualization of ITS regions.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "1273",
        "keep": "Keep",
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1273?version=2",
        "name": "mgnify-amplicon-pipeline-v5-its/main",
        "number_of_steps": 30,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "__FILTER_EMPTY_DATASETS__",
            "",
            "tp_awk_tool",
            "biom_convert",
            "bedtools_maskfastabed",
            "collection_element_identifiers",
            "taxonomy_krona_chart",
            "mapseq",
            "__FILTER_FROM_FILE__"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Paul Zierep"
        ],
        "description": "Quality control subworkflow for paired-end reads. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "1272",
        "keep": "Keep",
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1272?version=2",
        "name": "mgnify-amplicon-pipeline-v5-quality-control-paired-end/main",
        "number_of_steps": 17,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "__UNZIP_COLLECTION__",
            "fastp",
            "fastq_filter",
            "fastqc",
            "cshl_fasta_formatter",
            "fastq_to_fasta_python",
            "prinseq",
            "tp_find_and_replace",
            "mgnify_seqprep",
            "trimmomatic",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Paul Zierep"
        ],
        "description": "Quality control subworkflow for single-end reads.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "1271",
        "keep": "Keep",
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1271?version=3",
        "name": "mgnify-amplicon-pipeline-v5-quality-control-single-end/main",
        "number_of_steps": 14,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "fastq_filter",
            "fastqc",
            "cshl_fasta_formatter",
            "fastq_to_fasta_python",
            "prinseq",
            "tp_find_and_replace",
            "trimmomatic",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 3
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi",
            "Paul Zierep"
        ],
        "description": "Classification and visualization of SSU, LSU sequences.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "1270",
        "keep": "Keep",
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1270?version=2",
        "name": "mgnify-amplicon-pipeline-v5-rrna-prediction/main",
        "number_of_steps": 47,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "__FILTER_EMPTY_DATASETS__",
            "",
            "bedtools_getfastabed",
            "tp_awk_tool",
            "query_tabular",
            "biom_convert",
            "infernal_cmsearch",
            "cshl_fasta_formatter",
            "gops_concat_1",
            "collection_element_identifiers",
            "taxonomy_krona_chart",
            "cmsearch_deoverlap",
            "mapseq",
            "__FILTER_FROM_FILE__"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Rand Zoabi"
        ],
        "description": "This workflow creates taxonomic summary tables out of the amplicon pipeline results. ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "1269",
        "keep": "Keep",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1269?version=2",
        "name": "mgnify-amplicon-taxonomic-summary-tables/main",
        "number_of_steps": 10,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "tp_awk_tool",
            "filter_tabular",
            "query_tabular",
            "collection_column_join",
            "Grouping1"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2025-01-28",
        "creators": [
            "Peltzer None",
            "Alexander & Mohr",
            "Christopher None"
        ],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-nanostring_logo_dark.png\">\n    <img alt=\"nf-core/nanostring\" src=\"docs/images/nf-core-nanostring_logo_light.png\">\n  </picture>\n</h1>\n\n[![GitHub Actions CI Status](https://github.com/nf-core/nanostring/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/nanostring/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/nanostring/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/nanostring/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/nanostring/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/nanostring)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23nanostring-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/nanostring)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/nanostring** is a bioinformatics pipeline that ...\n\n<!-- TODO nf-core:\n   Complete this sentence with a 2-3 sentence summary of what types of data the pipeline ingests, a brief overview of the\n   major pipeline sections and the types of output it produces. You're giving an overview to someone new\n   to nf-core here, in 15-20 seconds. For an example, see https://github.com/nf-core/rnaseq/blob/master/README.md#introduction\n-->\n\n<!-- TODO nf-core: Include a figure that guides the user through the major workflow steps. Many nf-core\n     workflows use the \"tube map\" design for that. See https://nf-co.re/docs/contributing/design_guidelines#examples for examples.   -->\n<!-- TODO nf-core: Fill in short bullet-pointed list of the default steps in the pipeline -->1. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))2. Present QC for raw reads ([`MultiQC`](http://multiqc.info/))\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\n<!-- TODO nf-core: Describe the minimum required steps to execute the pipeline, e.g. how to prepare samplesheets.\n     Explain what rows and columns represent. For instance (please edit as appropriate):\n\nFirst, prepare a samplesheet with your input data that looks as follows:\n\n`samplesheet.csv`:\n\n```csv\nsample,fastq_1,fastq_2\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\n```\n\nEach row represents a fastq file (single-end) or a pair of fastq files (paired end).\n\n-->\n\nNow, you can run the pipeline using:\n\n<!-- TODO nf-core: update the following command to include all required parameters for a minimal example -->\n\n```bash\nnextflow run nf-core/nanostring \\\n   -profile <docker/singularity/.../institute> \\\n   --input samplesheet.csv \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/nanostring/usage) and the [parameter documentation](https://nf-co.re/nanostring/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/nanostring/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/nanostring/output).\n\n## Credits\n\nnf-core/nanostring was originally written by Peltzer, Alexander & Mohr, Christopher.\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n<!-- TODO nf-core: If applicable, make list of people who have also contributed -->\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#nanostring` channel](https://nfcore.slack.com/channels/nanostring) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/nanostring for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1003",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1003?version=7",
        "name": "nf-core/nanostring",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "nanostring",
            "nanostringnorm"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-14",
        "versions": 7
    },
    {
        "create_time": "2025-01-21",
        "creators": [
            "Tatiana Gurbich",
            "Martin Beracochea"
        ],
        "description": "[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n\r\n# mettannotator\r\n\r\n<img align=\"right\" width=\"162\" height=\"149\" src=\"media/mettannotator-logo.png\">\r\n\r\n- [ Introduction ](#intro)\r\n- [ Workflow and tools](#wf)\r\n- [ Installation and dependencies ](#install)\r\n  - [Reference databases](#reference-databases)\r\n- [ Usage ](#usage)\r\n- [ Test ](#test)\r\n- [ Outputs ](#out)\r\n- [Preparing annotations for ENA or GenBank submission](#submission)\r\n- [ Mobilome annotation ](#mobilome)\r\n- [ Credits ](#credit)\r\n- [ Contributions and Support ](#contribute)\r\n- [ Citation ](#cite)\r\n\r\n<a name=\"intro\"></a>\r\n\r\n## Introduction\r\n\r\n**mettannotator** is a bioinformatics pipeline that generates an exhaustive annotation of prokaryotic genomes using existing tools. The output is a GFF file that integrates the results of all pipeline components. Results of each individual tool are also provided.\r\n\r\n<a name=\"wf\"></a>\r\n\r\n## Workflow and tools\r\n\r\n<img src=\"media/mettannotator-schema.png\">\r\n<br />\r\n<br />\r\n\r\nThe workflow uses the following tools and databases:\r\n\r\n| Tool/Database                                                                                    | Version                                       | Purpose                                                                                                                |\r\n| ------------------------------------------------------------------------------------------------ | --------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\r\n| [Prokka](https://github.com/tseemann/prokka)                                                     | 1.14.6                                        | CDS calling and functional annotation (default)                                                                        |\r\n| [Bakta](https://github.com/oschwengers/bakta)                                                    | 1.9.3                                         | CDS calling and functional annotation (if --bakta flag is used)                                                        |\r\n| [Bakta db](https://zenodo.org/record/10522951/)                                                  | 2024-01-19 with AMRFinderPlus DB 2024-01-31.1 | Bakta DB (when Bakta is used as the gene caller)                                                                       |\r\n| [Pseudofinder](https://github.com/filip-husnik/pseudofinder)                                     | v1.1.0                                        | Identification of possible pseudogenes                                                                                 |\r\n| [Swiss-Prot](https://www.uniprot.org/help/downloads)                                             | 2024_06                                       | Database for Pseudofinder                                                                                              |\r\n| [InterProScan](https://www.ebi.ac.uk/interpro/about/interproscan/)                               | 5.62-94.0                                     | Protein annotation (InterPro, Pfam)                                                                                    |\r\n| [eggNOG-mapper](https://github.com/eggnogdb/eggnog-mapper)                                       | 2.1.11                                        | Protein annotation (eggNOG, KEGG, COG, GO-terms)                                                                       |\r\n| [eggNOG DB](http://eggnog6.embl.de/download/)                                                    | 5.0.2                                         | Database for eggNOG-mapper                                                                                             |\r\n| [UniFIRE](https://gitlab.ebi.ac.uk/uniprot-public/unifire)                                       | 2023.4                                        | Protein annotation                                                                                                     |\r\n| [AMRFinderPlus](https://github.com/ncbi/amr)                                                     | 3.12.8                                        | Antimicrobial resistance gene annotation; virulence factors, biocide, heat, acid, and metal resistance gene annotation |\r\n| [AMRFinderPlus DB](https://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/)              | 3.12 2024-01-31.1                             | Database for AMRFinderPlus                                                                                             |\r\n| [DefenseFinder](https://github.com/mdmparis/defense-finder)                                      | 1.2.0                                         | Annotation of anti-phage systems                                                                                       |\r\n| [DefenseFinder models](https://github.com/mdmparis/defense-finder-models)                        | 1.2.3                                         | Database for DefenseFinder                                                                                             |\r\n| [GECCO](https://github.com/zellerlab/GECCO)                                                      | 0.9.8                                         | Biosynthetic gene cluster annotation                                                                                   |\r\n| [antiSMASH](https://antismash.secondarymetabolites.org/#!/download)                              | 7.1.0                                         | Biosynthetic gene cluster annotation                                                                                   |\r\n| [SanntiS](https://github.com/Finn-Lab/SanntiS)                                                   | 0.9.3.4                                       | Biosynthetic gene cluster annotation                                                                                   |\r\n| [run_dbCAN](https://github.com/linnabrown/run_dbcan)                                             | 4.1.2                                         | PUL prediction                                                                                                         |\r\n| [dbCAN DB](https://bcb.unl.edu/dbCAN2/download/Databases/)                                       | V12                                           | Database for run_dbCAN                                                                                                 |\r\n| [CRISPRCasFinder](https://github.com/dcouvin/CRISPRCasFinder)                                    | 4.3.2                                         | Annotation of CRISPR arrays                                                                                            |\r\n| [cmscan](http://eddylab.org/infernal/)                                                           | 1.1.5                                         | ncRNA predictions                                                                                                      |\r\n| [Rfam](https://rfam.org/)                                                                        | 14.9                                          | Identification of SSU/LSU rRNA and other ncRNAs                                                                        |\r\n| [tRNAscan-SE](https://github.com/UCSC-LoweLab/tRNAscan-SE)                                       | 2.0.9                                         | tRNA predictions                                                                                                       |\r\n| [pyCirclize](https://github.com/moshi4/pyCirclize)                                               | 1.4.0                                         | Visualise the merged GFF file                                                                                          |\r\n| [VIRify](https://github.com/EBI-Metagenomics/emg-viral-pipeline)                                 | 2.0.0                                         | Viral sequence annotation (runs separately)                                                                            |\r\n| [Mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline) | 2.0                                           | Mobilome annotation (runs separately)                                                                                  |\r\n\r\n<a name=\"install\"></a>\r\n\r\n## Installation and dependencies\r\n\r\nThis workflow is built using [Nextflow](https://www.nextflow.io/). It uses containers (Docker or Singularity) making installation simple and results highly reproducible.\r\n\r\n- Install [Nextflow version >=21.10](https://www.nextflow.io/docs/latest/getstarted.html#installation)\r\n- Install [Singularity](https://github.com/apptainer/singularity/blob/master/INSTALL.md)\r\n- Install [Docker](https://docs.docker.com/engine/install/)\r\n\r\nAlthough it's possible to run the pipeline on a personal computer, due to the compute requirements, we encourage users to run it on HPC clusters. Any HPC scheduler supported by [Nextflow](https://www.nextflow.io/) is compatible; however, our team primarily uses [Slurm](https://slurm.schedmd.com/) and [IBM LSF](https://www.ibm.com/docs/en/spectrum-lsf) for the EBI HPC cluster, so those are the profiles we ship with the pipeline.\r\n\r\n<a name=\"reference-databases\"></a>\r\n\r\n### Reference databases\r\n\r\nThe pipeline needs reference databases in order to work, they take roughly 180G.\r\n\r\n| Path                | Size |\r\n| ------------------- | ---- |\r\n| amrfinder           | 217M |\r\n| antismash           | 9.4G |\r\n| bakta               | 71G  |\r\n| dbcan               | 7.5G |\r\n| defense_finder      | 242M |\r\n| eggnog              | 48G  |\r\n| interproscan        | 45G  |\r\n| interpro_entry_list | 2.6M |\r\n| rfam_models         | 637M |\r\n| pseudofinder        | 273M |\r\n| total               | 182G |\r\n\r\n`mettannotator` has an automated mechanism to download the databases using the `--dbs <db_path>` flag. When this flag is provided, the pipeline inspects the folder to verify if the required databases are already present. If any of the databases are missing, the pipeline will automatically download them.\r\n\r\nUsers can also provide individual paths to each reference database and its version if needed. For detailed instructions, please refer to the Reference databases section in the `--help` of the pipeline.\r\n\r\nIt's important to note that users are not allowed to mix the `--dbs` flag with individual database paths and versions; they are mutually exclusive. We recommend users to run the pipeline with the `--dbs` flag for the first time in an appropriate path and to avoid downloading the individual databases separately.\r\n\r\n<a name=\"usage\"></a>\r\n\r\n## Usage\r\n\r\n### Input file\r\n\r\nFirst, prepare an input file in the CSV format that looks as follows:\r\n\r\n`assemblies_sheet.csv`:\r\n\r\n```csv\r\nprefix,assembly,taxid\r\nBU_ATCC8492VPI0062,/path/to/BU_ATCC8492VPI0062_NT5002.fa,820\r\nEC_ASM584v2,/path/to/GCF_000005845.2.fna,562\r\n...\r\n```\r\n\r\nHere,\r\n`prefix` is the prefix and the locus tag that will be assigned to output files and proteins during the annotation process;\r\nmaximum length is 24 characters;\r\n\r\n`assembly` is the path to where the assembly file in FASTA format is located;\r\n\r\n`taxid` is the NCBI TaxId (if the species-level TaxId is not known, a TaxId for a higher taxonomic level can be used). If the taxonomy is known, look up the TaxID [here](https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi).\r\n\r\n#### Finding TaxIds\r\n\r\nIf NCBI taxonomies of input genomes are not known, a tool such as [CAT/BAT](https://github.com/MGXlab/CAT_pack) can be used.\r\nFollow the [instructions](https://github.com/MGXlab/CAT_pack?tab=readme-ov-file#installation) for getting the tool and downloading the NCBI nr database for it.\r\n\r\nIf using CAT/BAT, here is the suggested process for making the `mettannotator` input file:\r\n\r\n```bash\r\n# Run BAT on each input genome, saving all results to the same folder\r\nCAT bins -b ${genome_name}.fna -d ${path_to_CAT_database} -t ${path_to_CAT_tax_folder} -o BAT_results/${genome_name}\r\n\r\n# Optional: to check what taxa were assigned, you can add names to them\r\nCAT add_names -i BAT_results/${genome_name}.bin2classification.txt -o BAT_results/${genome_name}.name.txt -t ${path_to_CAT_tax_folder}\r\n```\r\n\r\nTo generate an input file for `mettannotator`, use [generate_input_file.py](preprocessing/generate_input_file.py):\r\n\r\n```\r\npython3 preprocessing/generate_input_file.py -h\r\nusage: generate_input_file.py [-h] -i INFILE -d INPUT_DIR -b BAT_DIR -o OUTFILE [--no-prefix]\r\n\r\nThe script takes a list of genomes and the taxonomy results generated by BAT and makes a\r\nmettannotator input csv file. The user has the option to either use the genome file name\r\n(minus the extension) as the prefix for mettannotator or leave the prefix off and fill it\r\nout themselves after the script generates an input file with just the FASTA location and\r\nthe taxid. It is expected that for all genomes, BAT results are stored in the same folder\r\nand are named as {fasta_base_name}.bin2classification.txt. The script will use the lowest-\r\nlevel taxid without an asterisk as the taxid for the genome.\r\n\r\noptional arguments:\r\n  -h, --help    show this help message and exit\r\n  -i INFILE     A file containing a list of genome files to include (file name only, with file\r\n                extension, unzipped, one file per line).\r\n  -d INPUT_DIR  Full path to the directory where the input FASTA files are located.\r\n  -b BAT_DIR    Folder with BAT results. Results for all genomes should be in the same folder\r\n                and should be named {fasta_base_name}.bin2classification.txt\r\n  -o OUTFILE    Path to the file where the output will be saved to.\r\n  --no-prefix   Skip prefix generation and leave the first column of the output file empty for\r\n                the user to fill out. Default: False\r\n```\r\n\r\nFor example:\r\n\r\n```bash\r\npython3 generate_input_file.py -i list_of_genome_fasta_files.txt -d /path/to/the/fasta/files/folder/ -b BAT_results/ -o mettannotator_input.csv\r\n```\r\n\r\nIt is always best to check the outputs to ensure the results are as expected. Correct any wrongly detected taxa before starting `mettannotator`.\r\n\r\nNote, that by default the script uses FASTA file names as prefixes and truncates them to 24 characters if they exceed the limit.\r\n\r\n### Running mettannotator\r\n\r\nRunning `mettannotator` with the `--help` option will pull the repository and display the help message:\r\n\r\n> [!NOTE]\r\n> We use the `-latest` flag with the `nextflow run` command, which ensures that the latest available version of the pipeline is pulled.\r\n> If you encounter any issues with the `nextflow run` command, please refer to the [Nextflow documentation](https://www.nextflow.io/docs/latest/reference/cli.html#run).\r\n\r\n```angular2html\r\n$ nextflow run -latest ebi-metagenomics/mettannotator/main.nf --help\r\nN E X T F L O W  ~  version 23.04.3\r\nLaunching `mettannotator/main.nf` [disturbed_davinci] DSL2 - revision: f2a0e51af6\r\n\r\n\r\n------------------------------------------------------\r\n  ebi-metagenomics/mettannotator <version>\r\n------------------------------------------------------\r\nTypical pipeline command:\r\n\r\n  nextflow run ebi-metagenomics/mettannotator --input assemblies_sheet.csv -profile docker\r\n\r\nInput/output options\r\n  --input                            [string]  Path to comma-separated file containing information about the assemblies with the prefix to be used.\r\n  --outdir                           [string]  The output directory where the results will be saved. You have to use absolute paths to storage on Cloud\r\n                                               infrastructure.\r\n  --fast                             [boolean] Run the pipeline in fast mode. In this mode, InterProScan, UniFIRE, and SanntiS won't be executed, saving\r\n                                               resources and speeding up the pipeline.\r\n  --email                            [string]  Email address for completion summary.\r\n  --multiqc_title                    [string]  MultiQC report title. Printed as page header, used for filename if not otherwise specified.\r\n\r\nReference databases\r\n  --dbs                              [string]  Folder for the tools' reference databases used by the pipeline for downloading. It's important to note that\r\n                                               mixing the --dbs flag with individual database paths and versions is not allowed; they are mutually\r\n                                               exclusive.\r\n  --interproscan_db                  [string]  The InterProScan reference database, ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/\r\n  --interproscan_db_version          [string]  The InterProScan reference database version. [default: 5.62-94.0]\r\n  --interpro_entry_list              [string]  TSV file listing basic InterPro entry information - the accessions, types and names,\r\n                                               ftp://ftp.ebi.ac.uk/pub/databases/interpro/releases/94.0/entry.list\r\n  --interpro_entry_list_version      [string]  InterPro entry list version [default: 94]\r\n  --eggnog_db                        [string]  The EggNOG reference database folder,\r\n                                               https://github.com/eggnogdb/eggnog-mapper/wiki/eggNOG-mapper-v2.1.5-to-v2.1.12#requirements\r\n  --eggnog_db_version                [string]  The EggNOG reference database version. [default: 5.0.2]\r\n  --rfam_ncrna_models                [string]  Rfam ncRNA models, ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/ncrna/\r\n  --rfam_ncrna_models_rfam_version   [string]  Rfam release version where the models come from. [default: 14.9]\r\n  --amrfinder_plus_db                [string]  AMRFinderPlus reference database,\r\n                                               https://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/AMRFinderPlus/database/. Go to the following\r\n                                               documentation for the db setup https://github.com/ncbi/amr/wiki/Upgrading#database-updates.\r\n  --amrfinder_plus_db_version        [string]  The AMRFinderPlus reference database version. [default: 2023-02-23.1]\r\n  --defense_finder_db                [string]  Defense Finder reference models, https://github.com/mdmparis/defense-finder#updating-defensefinder. The\r\n                                               Microbiome Informatics team provides a pre-indexed version of the models for version 1.2.3 on this ftp location:\r\n                                               ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipelines/tool-dbs/defense-finder/defense-finder-models_1.2.3.tar.gz.\r\n  --defense_finder_db_version        [string]  The Defense Finder models version. [default: 1.2.3]\r\n  --antismash_db                     [string]  antiSMASH reference database, go to this documentation to do the database setup\r\n                                               https://docs.antismash.secondarymetabolites.org/install/#installing-the-latest-antismash-release.\r\n  --antismash_db_version             [string]  The antiSMASH reference database version. [default: 7.1.0]\r\n  --dbcan_db                         [string]  dbCAN indexed reference database, please go to the documentation for the setup\r\n                                               https://dbcan.readthedocs.io/en/latest/. The Microbiome Informatics team provides a pre-indexed version of the\r\n                                               database for version 4.0 on this ftp location:\r\n                                               ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipelines/tool-dbs/dbcan/dbcan_4.0.tar.gz\r\n  --dbcan_db_version                 [string]  The dbCAN reference database version. [default: 4.1.3_V12]\r\n  --pseudofinder_db                  [string]  Pseudofinder reference database. Mettannotator uses SwissProt as the database for Pseudofinder.\r\n  --pseudofinder_db_version          [string]  SwissProt version. [default: 2024_06]\r\n\r\nGeneric options\r\n  --multiqc_methods_description      [string]  Custom MultiQC yaml file containing HTML including a methods description.\r\n\r\nOther parameters\r\n  --bakta                            [boolean] Use Bakta instead of Prokka for CDS annotation. Prokka will still be used for archaeal genomes.\r\n\r\n !! Hiding 17 params, use --validationShowHiddenParams to show them !!\r\n------------------------------------------------------\r\nIf you use ebi-metagenomics/mettannotator for your analysis please cite:\r\n\r\n* The nf-core framework\r\n  https://doi.org/10.1038/s41587-020-0439-x\r\n\r\n* Software dependencies\r\n  https://github.com/ebi-metagenomics/mettannotator/blob/master/CITATIONS.md\r\n------------------------------------------------------\r\n\r\n```\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\nnextflow run ebi-metagenomics/mettannotator \\\r\n   -profile <docker/singularity/...> \\\r\n   --input assemblies_sheet.csv \\\r\n   --outdir <OUTDIR> \\\r\n   --dbs <PATH/TO/WHERE/DBS/WILL/BE/SAVED>\r\n```\r\n\r\n> [!WARNING]\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\r\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\n#### Running the pipeline from the source code\r\n\r\nIf the Nextflow integration with Git does not work, users can download the tarball from the releases page. After extracting the tarball, the pipeline can be run directly by executing the following command:\r\n\r\n```bash\r\n$ nextflow run path-to-source-code/main.nf --help\r\n```\r\n\r\n#### Local execution\r\n\r\nThe pipeline can be run on a desktop or laptop, with the caveat that it will take a few hours to complete depending on the resources. There is a local profile in the Nextflow config that limits the total resources the pipeline can use to 8 cores and 12 GB of RAM. In order to run it (Docker or Singularity are still required):\r\n\r\n```bash\r\nnextflow run -latest ebi-metagenomics/mettannotator \\\r\n   -profile local,<docker or singulairty> \\\r\n   --input assemblies_sheet.csv \\\r\n   --outdir <OUTDIR> \\\r\n   --dbs <PATH/TO/WHERE/DBS/WILL/BE/SAVED>\r\n```\r\n\r\n### Gene caller choice\r\n\r\nBy default, `mettannotator` uses Prokka to identify protein-coding genes. Users can choose to use Bakta instead by\r\nrunning `mettannotator` with the `--bakta` flag. `mettannotator` runs Bakta without ncRNA and CRISPR\r\nannotation as these are produced by separate tools in the pipeline. Archaeal genomes will continue to be annotated using\r\nProkka as Bakta is only intended for annotation of bacterial genomes.\r\n\r\n### Fast mode\r\n\r\nTo reduce the compute time and the amount of resources used, the pipeline can be executed with the `--fast` flag. When\r\nrun in the fast mode, `mettannotator` will skip InterProScan, UniFIRE and SanntiS. This could be a suitable option\r\nfor a first-pass of annotation or if computational resources are limited, however, we recommend running the full version\r\nof the pipeline whenever possible.\r\n\r\nWhen generating an input file for a fast mode run, it is sufficient to indicate the taxid of the superkingdom (`2` for\r\nbacteria and `2157` for Archaea) in the \"taxid\" column rather than the taxid of the lowest known taxon.\r\n\r\n<a name=\"test\"></a>\r\n\r\n## Test\r\n\r\nTo run the pipeline using a test dataset, execute the following command:\r\n\r\n```bash\r\nwget https://raw.githubusercontent.com/EBI-Metagenomics/mettannotator/master/tests/test.csv\r\n\r\nnextflow run -latest ebi-metagenomics/mettannotator \\\r\n   -profile <docker/singularity/...> \\\r\n   --input test.csv \\\r\n   --outdir <OUTDIR> \\\r\n   --dbs <PATH/TO/WHERE/DBS/WILL/BE/SAVED>\r\n```\r\n\r\n<a name=\"out\"></a>\r\n\r\n## Outputs\r\n\r\nThe output folder structure will look as follows:\r\n\r\n```\r\n\u2514\u2500<PREFIX>\r\n   \u251c\u2500antimicrobial_resistance\r\n   \u2502  \u2514\u2500amrfinder_plus\r\n   \u251c\u2500antiphage_defense\r\n   \u2502  \u2514\u2500defense_finder\r\n   \u251c\u2500biosynthetic_gene_clusters\r\n   \u2502  \u251c\u2500antismash\r\n   \u2502  \u251c\u2500gecco\r\n   \u2502  \u2514\u2500sanntis\r\n   \u251c\u2500functional_annotation\r\n   \u2502  \u251c\u2500dbcan\r\n   \u2502  \u251c\u2500eggnog_mapper\r\n   \u2502  \u251c\u2500interproscan\r\n   \u2502  \u251c\u2500merged_gff\r\n   \u2502  \u251c\u2500prokka\r\n   \u2502  \u251c\u2500pseudofinder\r\n   \u2502  \u2514\u2500unifire\r\n   \u251c\u2500mobilome\r\n   \u2502  \u2514\u2500crisprcas_finder\r\n   \u251c\u2500quast\r\n   \u2502  \u2514\u2500<PREFIX>\r\n   \u2502      \u251c\u2500basic_stats\r\n   \u2502      \u2514\u2500icarus_viewers\r\n   \u251c\u2500rnas\r\n   \u2502  \u251c\u2500ncrna\r\n   \u2502  \u2514\u2500trna\r\n   \u251c\u2500multiqc\r\n   \u2502  \u251c\u2500multiqc_data\r\n   \u2502  \u2514\u2500multiqc_plots\r\n   \u2502      \u251c\u2500pdf\r\n   \u2502      \u251c\u2500png\r\n   \u2502      \u2514\u2500svg\r\n   \u251c\u2500pipeline_info\r\n   \u2502  \u251c\u2500software_versions.yml\r\n   \u2502  \u251c\u2500execution_report_<timestamp>.txt\r\n   \u2502  \u251c\u2500execution_report_<timestamp>.html\r\n   \u2502  \u251c\u2500execution_timeline_<timestamp>.txt\r\n   \u2502  \u251c\u2500execution_timeline_<timestamp>.html\r\n   \u2502  \u251c\u2500execution_trace_<timestamp>.txt\r\n   \u2502  \u251c\u2500execution_trace_<timestamp>.html\r\n   \u2502  \u2514\u2500pipeline_dag_<timestamp>.html\r\n\r\n```\r\n\r\n### Merged GFF\r\n\r\nThe two main output files for each genome are located in `<OUTDIR>/<PREFIX>/functional_annotation/merged_gff/`:\r\n\r\n- `<PREFIX>_annotations.gff`: annotations produced by all tools merged into a single file\r\n\r\n- `<PREFIX>_annotations_with_descriptions.gff`: a version of the GFF file above that includes descriptions of all InterPro terms to make the annotations human-readable. Not generated if `--fast` flag was used.\r\n\r\nBoth files include the genome sequence in the FASTA format at the bottom of the file.\r\n\r\nAdditionally, for genomes with no more than 50 annotated contigs, a Circos plot of the `<PREFIX>_annotations.gff` file is generated and included in the same folder. An example of such plot is shown below:\r\n\r\n<img src=\"media/circos-plot-example.png\">\r\n\r\n#### Data sources\r\n\r\nBelow is an explanation of how each field in column 3 and 9 of the final GFF file is populated. In most cases, information is taken as is from the reporting tool's output.\r\n\r\n| Feature (column 3)    | Attribute Name (column 9)                                               | Reporting Tool  | Description                                                                                                                                                                                                 |\r\n| --------------------- | ----------------------------------------------------------------------- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\r\n| ncRNA                 | all\\*                                                                   | cmscan + Rfam   | ncRNA annotation (excluding tRNA)                                                                                                                                                                           |\r\n| tRNA                  | all\\*                                                                   | tRNAscan-SE     | tRNA annotation                                                                                                                                                                                             |\r\n| LeftFLANK, RightFLANK | all\\*                                                                   | CRISPRCasFinder | CRISPR array flanking sequence                                                                                                                                                                              |\r\n| CRISPRdr              | all\\*                                                                   | CRISPRCasFinder | Direct repeat region of a CRISPR array                                                                                                                                                                      |\r\n| CRISPRspacer          | all\\*                                                                   | CRISPRCasFinder | CRISPR spacer                                                                                                                                                                                               |\r\n| CDS                   | `ID`, `eC_number`, `Name`, `Dbxref`, `gene`, `inference`, `locus_tag`   | Prokka/Bakta    | Protein annotation                                                                                                                                                                                          |\r\n| CDS                   | `product`                                                               | mettannotator   | Product assigned as described in [ Determining the product ](#product)                                                                                                                                      |\r\n| CDS                   | `product_source`                                                        | mettannotator   | Tool that reported the product chosen by mettannotator                                                                                                                                                      |\r\n| CDS                   | `eggNOG`                                                                | eggNOG-mapper   | Seed ortholog from eggNOG                                                                                                                                                                                   |\r\n| CDS                   | `cog`                                                                   | eggNOG-mapper   | COG category                                                                                                                                                                                                |\r\n| CDS                   | `kegg`                                                                  | eggNOG-mapper   | KEGG orthology term                                                                                                                                                                                         |\r\n| CDS                   | `Ontology_term`                                                         | eggNOG-mapper   | GO associations                                                                                                                                                                                             |\r\n| CDS                   | `pfam`                                                                  | InterProScan    | Pfam accessions                                                                                                                                                                                             |\r\n| CDS                   | `interpro`                                                              | InterProScan    | InterPro accessions. In `<PREFIX>_annotations_with_descriptions.gff` each accession is followed by its description and entry type: Domain [D], Family [F], Homologous Superfamily [H], Repeat [R], Site [S] |\r\n| CDS                   | `nearest_MiBIG`                                                         | SanntiS         | MiBIG accession of the nearest BGC to the cluster in the MIBIG space                                                                                                                                        |\r\n| CDS                   | `nearest_MiBIG_class`                                                   | SanntiS         | BGC class of nearest_MiBIG                                                                                                                                                                                  |\r\n| CDS                   | `gecco_bgc_type`                                                        | GECCO           | BGC type                                                                                                                                                                                                    |\r\n| CDS                   | `antismash_bgc_function`                                                | antiSMASH       | BGC function                                                                                                                                                                                                |\r\n| CDS                   | `amrfinderplus_gene_symbol`                                             | AMRFinderPlus   | Gene symbol according to AMRFinderPlus                                                                                                                                                                      |\r\n| CDS                   | `amrfinderplus_sequence_name`                                           | AMRFinderPlus   | Product description                                                                                                                                                                                         |\r\n| CDS                   | `amrfinderplus_scope`                                                   | AMRFinderPlus   | AMRFinderPlus database (core or plus)                                                                                                                                                                       |\r\n| CDS                   | `element_type`, `element_subtype`                                       | AMRFinderPlus   | Functional category                                                                                                                                                                                         |\r\n| CDS                   | `drug_class`, `drug_subclass`                                           | AMRFinderPlus   | Class and subclass of drugs that this gene is known to contribute to resistance of                                                                                                                          |\r\n| CDS                   | `dbcan_prot_type`                                                       | run_dbCAN       | Predicted protein function: transporter (TC), transcription factor (TF), signal transduction protein (STP), CAZyme                                                                                          |\r\n| CDS                   | `dbcan_prot_family`                                                     | run_dbCAN       | Predicted protein family                                                                                                                                                                                    |\r\n| CDS                   | `substrate_dbcan-pul`                                                   | run_dbCAN       | Substrate predicted by dbCAN-PUL search                                                                                                                                                                     |\r\n| CDS                   | `substrate_dbcan-sub`                                                   | run_dbCAN       | Substrate predicted by dbCAN-subfam                                                                                                                                                                         |\r\n| CDS                   | `defense_finder_type`, `defense_finder_subtype`                         | DefenseFinder   | Type and subtype of the anti-phage system found                                                                                                                                                             |\r\n| CDS                   | `uf_prot_rec_fullname`, `uf_prot_rec_shortname`, `uf_prot_rec_ecnumber` | UniFIRE         | Protein recommended full name, short name and EC number according to UniFIRE                                                                                                                                |\r\n| CDS                   | `uf_prot_alt_fullname`, `uf_prot_alt_shortname`, `uf_prot_alt_ecnumber` | UniFIRE         | Protein alternative full name, short name and EC number according to UniFIRE                                                                                                                                |\r\n| CDS                   | `uf_chebi`                                                              | UniFIRE         | ChEBI identifiers                                                                                                                                                                                           |\r\n| CDS                   | `uf_ontology_term`                                                      | UniFIRE         | GO associations                                                                                                                                                                                             |\r\n| CDS                   | `uf_keyword`                                                            | UniFIRE         | UniFIRE keywords                                                                                                                                                                                            |\r\n| CDS                   | `uf_gene_name`, `uf_gene_name_synonym`                                  | UniFIRE         | Gene name and gene name synonym according to UniFIRE                                                                                                                                                        |\r\n| CDS                   | `uf_pirsr_cofactor`                                                     | UniFIRE         | Cofactor names from PIRSR                                                                                                                                                                                   |\r\n\r\n\\*all attributes in column 9 are populated by the tool\r\n<br>\r\n<br>\r\n\r\n<a name=\"product\"></a>\r\n\r\n#### Determining the product\r\n\r\nThe following logic is used by `mettannotator` to fill out the `product` field in the 9th column of the GFF:\r\n\r\n<img src=\"media/mettannotator-product.png\">\r\n\r\nIf the pipeline is executed with the `--fast` flag, only the output of eggNOG-mapper is used to determine the product of proteins that were labeled as hypothetical by the gene caller.\r\n\r\n#### Detection of pseudogenes and spurious ORFs\r\n\r\n`mettannotator` uses several approaches to detect pseudogenes and spurious ORFs:\r\n\r\n- If Bakta is used as the initial annotation tool, `mettannotator` will inherit the pseudogene labels assigned by Bakta.\r\n- `mettannotator` runs Pseudofinder and labels genes that Pseudofinder predicts to be pseudogenes by adding `\"pseudo=true\"` to the 9th column of the final merged GFF file. If there is a disagreement between Pseudofinder and Bakta and one of the tools calls a gene a pseudogene, it will be labeled as a pseudogene.\r\n- AntiFam, which is a part of InterPro, is used to identify potential spurious ORFs. If an ORF has an AntiFam hit, `mettannotator` will remove it from the final merged GFF file. These ORFs will still appear in the raw outputs of Bakta/Prokka and may appear in other tool outputs.\r\n\r\n`mettannotator` produces a report file which is located in the `merged_gff` folder and includes a list of CDS with AntiFam hits and pseudogenes. For each pseudogene, the report shows which tool predicted it.\r\n\r\n### Contents of the tool output folders\r\n\r\nThe output folders of each individual tool contain select output files of the third-party tools used by `mettannotator`. For file descriptions, please refer to the tool documentation. For some tools that don't output a GFF, `mettannotator` converts the output into a GFF.\r\n\r\nNote: if the pipeline completed without errors but some of the tool-specific output folders are empty, those particular tools did not generate any annotations to output.\r\n\r\n<a name=\"submission\"></a>\r\n\r\n## Preparing annotations for ENA or GenBank submission\r\n\r\n`mettannotator` produces a final annotation file in GFF3 format. To submit the annotations to data archives, it is first necessary to convert the GFF3 file into the required format, using third-party tools available. `mettannotator` outputs a specially formatted GFF3 file, named `<prefix>_submission.gff` to be used with converters.\r\n\r\n### ENA\r\n\r\nENA accepts annotations in the EMBL flat-file format.\r\nPlease use [EMBLmyGFF3](https://github.com/NBISweden/EMBLmyGFF3) to perform the conversion; the repository includes detailed instructions. The two files required for conversion are:\r\n\r\n- the genome FASTA file\r\n- `<mettannotator_results_folder>/<prefix>/functional_annotation/merged_gff/<prefix>_submission.gff`\r\n\r\nPlease note that it is necessary to register the project and locus tags in ENA prior to conversion. Follow links in the [EMBLmyGFF3](https://github.com/NBISweden/EMBLmyGFF3) repository for more details.\r\n\r\n### GenBank\r\n\r\nTo convert annotations for GenBank submission, please use [table2asn](https://www.ncbi.nlm.nih.gov/genbank/table2asn/).\r\nThree files are required:\r\n\r\n- the genome FASTA file\r\n- `<mettannotator_results_folder>/<prefix>/functional_annotation/merged_gff/<prefix>_submission.gff`\r\n- Submission template file (can be generated [here](https://submit.ncbi.nlm.nih.gov/genbank/template/submission/))\r\n\r\nMore instructions on running `table2asn` are available via [GenBank](https://www.ncbi.nlm.nih.gov/genbank/genomes_gff/).\r\n\r\n<a name=\"mobilome\"></a>\r\n\r\n## Mobilome annotation\r\n\r\nThe mobilome annotation workflow is not currently integrated into `mettannotator`. However, the outputs produced by `mettannotator` can be used to run [VIRify](https://github.com/EBI-Metagenomics/emg-viral-pipeline) and the [mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline) and the outputs of these tools can be integrated back into the GFF file produced by `mettannotator`.\r\n\r\nAfter installing both tools, follow these steps to add the mobilome annotation:\r\n\r\n1. Run the [viral annotation pipeline](https://github.com/EBI-Metagenomics/emg-viral-pipeline):\r\n\r\n```bash\r\nnextflow run \\\r\n    emg-viral-pipeline/virify.nf \\\r\n    -profile <profile> \\\r\n    --fasta <genome_fasta.fna> \\\r\n    --output <prefix>\r\n```\r\n\r\n2. Run the [mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline):\r\n\r\n```bash\r\nnextflow run mobilome-annotation-pipeline/main.nf \\\r\n    --assembly <genome_fasta.fna> \\\r\n    --user_genes true \\\r\n    --prot_gff <mettannotator_results_folder/<prefix>/functional_annotation/merged_gff/<prefix>_annotations.gff \\\r\n    --virify true # only if the next two VIRify files exist, otherwise skip this line \\\r\n    --vir_gff Virify_output_folder/08-final/gff/<prefix>_virify.gff # only if file exists, otherwise skip this line \\\r\n    --vir_checkv Virify_output_folder/07-checkv/\\*quality_summary.tsv # only if the GFF file above exists, otherwise skip this line \\\r\n    --outdir <mobilome_output_folder> \\\r\n    --skip_crispr true \\\r\n    --skip_amr true \\\r\n    -profile <profile>\"\r\n```\r\n\r\n3. Integrate the output into the `mettannotator` GFF\r\n\r\n```bash\r\n# Add mobilome to the merged GFF produced by mettannotator\r\npython3 postprocessing/add_mobilome_to_gff.py \\\r\n    -m <mobilome_output_folder>/gff_output_files/mobilome_nogenes.gff \\\r\n    -i <mettannotator_results_folder>/<prefix>/functional_annotation/merged_gff/<prefix>_annotations.gff \\\r\n    -o <prefix>_annotations_with_mobilome.gff\r\n\r\n# Add mobilome to the GFF with descriptions produced by mettannotator\r\npython3 postprocessing/add_mobilome_to_gff.py \\\r\n    -m <mobilome_output_folder>/gff_output_files/mobilome_nogenes.gff \\\r\n    -i <mettannotator_results_folder>/<prefix>/functional_annotation/merged_gff/<prefix>_annotations_with_descriptions.gff \\\r\n    -o <prefix>_annotations_with_descriptions_with_mobilome.gff\r\n```\r\n\r\n4. Optional: regenerate the Circos plot with the mobilome track added\r\n\r\n```bash\r\npip install pycirclize\r\npip install matplotlib\r\n\r\npython3 bin/circos_plot.py \\\r\n    -i <prefix>_annotations_with_mobilome.gff \\\r\n    -o plot.png \\\r\n    -p <prefix> \\\r\n    --mobilome\r\n```\r\n\r\n<a name=\"credit\"></a>\r\n\r\n## Credits\r\n\r\nebi-metagenomics/mettannotator was originally written by the Microbiome Informatics Team at [EMBL-EBI](https://www.ebi.ac.uk/about/teams/microbiome-informatics/)\r\n\r\n<a name=\"contribute\"></a>\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\n<a name=\"cite\"></a>\r\n\r\n## Citations\r\n\r\nIf you use the software, please cite:\r\n\r\nGurbich TA, Beracochea M, De Silva NH, Finn RD. mettannotator: a comprehensive and scalable Nextflow annotation pipeline for prokaryotic assemblies. bioRxiv 2024.07.11.603040; doi: https://doi.org/10.1101/2024.07.11.603040\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Genomics",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "1069",
        "keep": "To Curate",
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1069?version=3",
        "name": "mettannotator",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "metagenomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-01-21",
        "versions": 3
    },
    {
        "create_time": "2025-01-18",
        "creators": [],
        "description": "<h1>\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-rangeland_logo_dark.png\">\n    <img alt=\"nf-core/rangeland\" src=\"docs/images/nf-core-rangeland_logo_light.png\">\n  </picture>\n</h1>[![GitHub Actions CI Status](https://github.com/nf-core/rangeland/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/rangeland/actions/workflows/ci.yml)\n[![GitHub Actions Linting Status](https://github.com/nf-core/rangeland/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/rangeland/actions/workflows/linting.yml)[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/rangeland/results)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\n\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A524.04.2-23aa62.svg)](https://www.nextflow.io/)\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/nf-core/rangeland)\n\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23rangeland-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/rangeland)[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\n\n## Introduction\n\n**nf-core/rangeland** is a geographical best-practice analysis pipeline for remotely sensed imagery.\nThe pipeline processes satellite imagery alongside auxiliary data in multiple steps to arrive at a set of trend files related to land-cover changes. The main pipeline steps are:\n\n1. Read satellite imagery, digital elevation model (dem), endmember definition, water vapor database (wvdb), datacube definition and area of interest definition (aoi)\n2. Generate allow list and analysis mask to determine which pixels from the satellite data can be used\n3. Preprocess data to obtain atmospherically corrected images alongside quality assurance information (aka. level 2 analysis read data)\n4. Merge spatially and temporally overlapping preprocessed data\n5. Classify pixels by applying linear spectral unmixing\n6. Time series analyses to obtain trends in vegetation dynamics to derive level 3 data\n7. Create mosaic and pyramid visualizations of the results\n8. Version reporting with MultiQC ([`MultiQC`](http://multiqc.info/))\n\n<p align=\"center\">\n    <img title=\"nf-core/rangeland diagram\" src=\"docs/images/rangeland_diagram.png\" width=95%>\n</p>\n\n## Usage\n\n> [!NOTE]\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow.Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\n\nTo run, satellite imagery, water vapor data, a digital elevation model, endmember definitions, a datacube specification, and a area-of-interest specification are required as input data.\nPlease refer to the [usage documentation](https://nf-co.re/rangeland/usage) for details on the input structure.\n\nNow, you can run the pipeline using:\n\n```bash\nnextflow run nf-core/rangeland \\\n   -profile <docker/singularity/.../institute> \\\n   --input <SATELLITE IMAGES> \\\n   --dem <DIGITAL ELEVATION MODEL> \\\n   --wvdb <WATER VAPOR DATA> \\\n   --data_cube <DATA CUBE> \\\n   --aoi <AREA OF INTEREST> \\\n   --endmember <ENDMEMBER SPECIFICATION> \\\n   --outdir <OUTDIR>\n```\n\n> [!WARNING]\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_; see [docs](https://nf-co.re/docs/usage/getting_started/configuration#custom-configuration-files).\n\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/rangeland/usage) and the [parameter documentation](https://nf-co.re/rangeland/parameters).\n\n## Pipeline output\n\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/rangeland/results) tab on the nf-core website pipeline page.\nFor more details about the output files and reports, please refer to the\n[output documentation](https://nf-co.re/rangeland/output).\n\n## Credits\n\nThe rangeland workflow was originally written by:\n\n- [Fabian Lehmann](https://github.com/Lehmann-Fabian)\n- [David Frantz](https://github.com/davidfrantz)\n\nThe original workflow can be found on [github](https://github.com/CRC-FONDA/FORCE2NXF-Rangeland).\n\nTransformation to nf-core/rangeland was conducted by [Felix Kummer](https://github.com/Felix-Kummer).\nnf-core alignment started on the [nf-core branch of the original repository](https://github.com/CRC-FONDA/FORCE2NXF-Rangeland/tree/nf-core).\n\nWe thank the following people for their extensive assistance in the development of this pipeline:\n\n- [Fabian Lehmann](https://github.com/Lehmann-Fabian)\n- [Katarzyna Ewa Lewinska](https://github.com/kelewinska).\n\n## Acknowledgements\n\nThis pipeline was developed and aligned with nf-core as part of the [Foundations of Workflows for Large-Scale Scientific Data Analysis (FONDA)](https://fonda.hu-berlin.de/) initiative.\n\n[![FONDA](docs/images/fonda_logo2_cropped.png)](https://fonda.hu-berlin.de/)\n\nFONDA can be cited as follows:\n\n> **The Collaborative Research Center FONDA.**\n>\n> Ulf Leser, Marcus Hilbrich, Claudia Draxl, Peter Eisert, Lars Grunske, Patrick Hostert, Dagmar Kainm\u00fcller, Odej Kao, Birte Kehr, Timo Kehrer, Christoph Koch, Volker Markl, Henning Meyerhenke, Tilmann Rabl, Alexander Reinefeld, Knut Reinert, Kerstin Ritter, Bj\u00f6rn Scheuermann, Florian Schintke, Nicole Schweikardt, Matthias Weidlich.\n>\n> _Datenbank Spektrum_ 2021 doi: [10.1007/s13222-021-00397-5](https://doi.org/10.1007/s13222-021-00397-5)\n\n## Contributions and Support\n\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\n\nFor further information or help, don't hesitate to get in touch on the [Slack `#rangeland` channel](https://nfcore.slack.com/channels/rangeland) (you can join with [this invite](https://nf-co.re/join/slack)).\n\n## Citations\n\n<!-- TODO nf-core: Add citation for pipeline after first release. Uncomment lines below and update Zenodo doi and badge at the top of this file. -->\n<!-- If you use nf-core/rangeland for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX) -->\n\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\n\nYou can cite the `nf-core` publication as follows:\n\n> **The nf-core framework for community-curated bioinformatics pipelines.**\n>\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\n>\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\n\nThis pipeline is based one the publication listed below.\nThe publication can be cited as follows:\n\n> **FORCE on Nextflow: Scalable Analysis of Earth Observation Data on Commodity Clusters**\n>\n> [Lehmann, F., Frantz, D., Becker, S., Leser, U., Hostert, P. (2021). FORCE on Nextflow: Scalable Analysis of Earth Observation Data on Commodity Clusters. In CIKM Workshops.](https://www.informatik.hu-berlin.de/de/forschung/gebiete/wbi/research/publications/2021/force_nextflow.pdf/@@download/file/force_nextflow.pdf)\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1250",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1250?version=1",
        "name": "nf-core/rangeland",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "earth-observation",
            "landsat",
            "satellite-imagery",
            "spectral-unmixing",
            "trend-analysis",
            "trends"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-14",
        "versions": 1
    },
    {
        "create_time": "2024-12-18",
        "creators": [
            "Marie-Emilie  Gauthier",
            "Craig Windell",
            "Magdalena Antczak",
            "Roberto Barrero"
        ],
        "description": "# ONTViSc (ONT-based Viral Screening for Biosecurity)\r\n\r\n## Introduction\r\neresearchqut/ontvisc is a Nextflow-based bioinformatics pipeline designed to help diagnostics of viruses and viroid pathogens for biosecurity. It takes fastq files generated from either amplicon or whole-genome sequencing using Oxford Nanopore Technologies as input.\r\n\r\nThe pipeline can either: 1) perform a direct search on the sequenced reads, 2) generate clusters, 3) assemble the reads to generate longer contigs or 4) directly map reads to a known reference. \r\n\r\nThe reads can optionally be filtered from a plant host before performing downstream analysis.\r\n\r\n## Pipeline overview\r\n- Data quality check (QC) and preprocessing\r\n  - Merge fastq files (Fascat, optional)\r\n  - Raw fastq file QC (Nanoplot)\r\n  - Trim adaptors (PoreChop ABI - optional)\r\n  - Filter reads based on length and/or quality (Chopper - optional)\r\n  - Reformat fastq files so read names are trimmed after the first whitespace (bbmap)\r\n  - Processed fastq file QC (if PoreChop and/or Chopper is run) (Nanoplot)\r\n- Host read filtering\r\n  - Align reads to host reference provided (Minimap2)\r\n  - Extract reads that do not align for downstream analysis (seqtk)\r\n- QC report\r\n  - Derive read counts recovered pre and post data processing and post host filtering\r\n- Read classification analysis mode\r\n- Clustering mode\r\n  - Read clustering (Rattle)\r\n  - Convert fastq to fasta format (seqtk)\r\n  - Cluster scaffolding (Cap3)\r\n  - Megablast homology search against ncbi or custom database (blast)\r\n  - Derive top candidate viral hits\r\n  - Align reads back to top reference and derive coverage statistics (mosdepth and coverM)\r\n- De novo assembly mode\r\n  - De novo assembly (Canu or Flye)\r\n  - Megablast homology search against ncbi or custom database or reference (blast)\r\n  - Derive top candidate viral hits\r\n  - Align reads back to top reference and derive coverage statistics (mosdepth and coverM)\r\n- Read classification mode\r\n  - Option 1 Nucleotide-based taxonomic classification of reads (Kraken2, Braken)\r\n  - Option 2 Protein-based taxonomic classification of reads (Kaiju, Krona)\r\n  - Option 3 Convert fastq to fasta format (seqtk) and perform direct homology search using megablast (blast)\r\n- Map to reference mode\r\n  - Align reads to reference fasta file (Minimap2) and derive bam file and alignment statistics (Samtools)\r\n\r\nCode and detailed instructions can be found [here](https://github.com/eresearchqut/ontvisc). A comprehensive, step-by-step guide on setting up and executing the ONTViSc pipeline across three high-performance computing systems hosted by Australian research and computing facilities - Lyra (Queensland University of Technology), Gadi (National Computational Infrastructure), and Setonix (Pawsey) - utilising the Australian Nextflow Seqera Service, can be found [here](https://mantczakaus.github.io/ontvisc_hpc_seqera_service_guide/).\r\n\r\n## Authors\r\nMarie-Emilie Gauthier <gauthiem@qut.edu.au>  \r\nCraig Windell <c.windell@qut.edu.au>  \r\nMagdalena Antczak <magdalena.antczak@qcif.edu.au>  \r\nRoberto Barrero <roberto.barrero@qut.edu.au>  \r\n",
        "doi": "10.48546/workflowhub.workflow.683.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in description",
        "id": "683",
        "keep": "To Curate",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/683?version=3",
        "name": "ONTViSc (ONT-based Viral Screening for Biosecurity)",
        "number_of_steps": 0,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "bioinformatics",
            "nextflow",
            "ont",
            "virology",
            "virus",
            "blast",
            "singularity"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-12-18",
        "versions": 3
    },
    {
        "create_time": "2024-12-17",
        "creators": [
            "Qiang Ye"
        ],
        "description": "# ONT Artificial Deletion Filter-Delter\r\nA tool to filter short artificial deletion variations by Oxford Nanopore Technologies (ONT) R9 and R10 flow cells and chemistries.\r\n## Requirements\r\nThe tool has been tested on Ubuntu 20.04 with 256GB RAM, 64 CPU cores and a NVIDIA GPU with 48GB RAM. The minimal requirements should be >= 64GB RAM and a NVIDIA GPU with >= 8GB RAM. Other operating systems like Windows or Mac were not tested.\r\n\r\nONT softwares like [Guppy](https://community.nanoporetech.com/downloads), [Tombo](https://github.com/nanoporetech/tombo), and [ont-fast5-api](https://github.com/nanoporetech/ont_fast5_api) should be pre-installed before generating Tombo-resquiggled single-read fast5 files.\r\nUsers might run following commands to preprocess R9 fast5 files in shell terminal before running our pipeline. As these steps below need GPU support and might take a long time, our pipeline doesn't contain them.\r\n```bash\r\n#===basecalling the fast5 files===\r\nont-guppy/bin/guppy_basecaller -c ont-guppy/data/dna_r9.4.1_450bps_sup.cfg -i $fast5dir/barcode${barcode} -s guppy_sup_basecalled/barcode${barcode} -r --compress_fastq -x cuda:1,2 --gpu_runners_per_device 4 --chunks_per_runner 256 --num_callers 3 --fast5_out\r\n\r\n#===preprocessing R9 fast5 files===\r\nc=$(ls *.fast5 | wc -l)\r\ndeclare -i count=$c-1\r\n#multiread fast5 to single read fast5\r\nmulti_to_single_fast5 -i guppy_sup_basecalled/barcode${barcode}/workspace -s fast5_pass_single --threads 24 \r\n#copy to new directory\r\ncd fast5_pass_single\r\nmkdir all_single_fast5s\r\nfor ((j=0;j<=$count;j=j+1))\r\ndo\r\n  echo $j\r\n  cp -r ./$j/*.fast5 all_single_fast5s\r\n  rm -rf ./$j\r\ndone\r\n#align to reference genome via tombo resquiggle\r\ntombo resquiggle guppy_sup_basecalled/barcode${barcode}/workspace/fast5_pass_single/all_single_fast5s data/Refs/$refseq --processes 24 --overwrite --num-most-common-errors 5  --failed-reads-filename tombo_resquiggle_failed_fast5.txt\r\n```\r\nThe fast5 files in the directory named **all_single_fast5s** could be employed in downstream workflow, which equals the input parameter **Tombo_dir** in shell command line or config yaml (details listed in **Configure input parameters** section). \r\n## Installation\r\nThe tool runs via Snakemake workflows. Users must install workflow dependencies including [Snakemake](https://snakemake.readthedocs.io/en/latest/tutorial/tutorial.html) (**Version >= 7.3**) and [R](https://mirrors.tuna.tsinghua.edu.cn/CRAN/) before using the pipeline. The workflow dependencies, which stored in a file named environment.yaml, are listed as below:\r\n\r\nchannels:\r\n  - conda-forge\r\n  - bioconda\r\n  - anaconda\r\n\r\ndependencies:\r\n  - snakemake-minimal >=7.3\r\n  - graphviz\r\n  - seaborn\r\n  - numpy\r\n  - pandas\r\n  - h5py\r\n  - scipy\r\n  - samtools =1.15\r\n  - r-essentials\r\n  - r-base\r\n  - bioconductor-shortread\r\n  - r-stringr\r\n  - r-dplyr\r\n  - r-vegan \r\n\r\nUsers are suggested to use [Conda](https://docs.conda.io/en/latest/) or [Mamba](https://mamba.readthedocs.io/en/latest/user_guide/mamba.html) to install these dependencies. After users download the working directory containing all the necessary files, the following shell command could install **Delter** in a **conda** environment in less than half an hour.\r\n```bash\r\ncd /path/to/Delter/working/directory\r\nconda env create --name Delter --file environment.yaml\r\n```\r\nor\r\n```bash\r\ncd /path/to/Delter/working/directory\r\nconda create --name Delter\r\nconda activate Delter\r\nconda install -c bioconda -y snakemake-minimal>=7.3\r\nconda install -c anaconda -y numpy pandas \r\nconda install -c anaconda -y h5py seaborn\r\nconda install -c conda-forge -y scipy\r\nconda install -c bioconda -y samtools=1.15\r\nconda install -c conda-forge -y r-essentials r-base\r\nconda install -c conda-forge -y r-dplyr r-vegan r-stringr\r\nconda install -c bioconda -y bioconductor-shortread\r\n```\r\n\r\n## Activate and exit the environment\r\nTo activate the environment \r\n  ```bash\r\n  conda activate Delter\r\n  ```\r\nTo exit the environment (after finishing the usage of the pipeline), just execute\r\n  ```bash\r\n  conda deactivate\r\n  ```\r\n## Run the pipeline\r\nThe whole pipeline could handle ONT R9 and R10 sequencing data. The working directory contains file named `Delter.config.yaml`, which stores key input parameters for the workflow. For handling the VCF file(s) generated by [LoFreq](https://csb5.github.io/lofreq/), the **Delter.py** script should be used. For VCF file(s) generated by [Clair3](https://github.com/HKU-BAL/Clair3), the **Delter_clair3.py** script should be used.\r\n\r\nThe Demo data could be accessed via [figshare](https://doi.org/10.6084/m9.figshare.26093869.v1). User should modify the filepaths in the Delter.config.yaml in the snakemakeexample directory.\r\n\r\n### Configure input parameters for the workflow\r\nThere are two ways to configure input parameters for this workflow.\r\n\r\n(1) Via shell command line\r\n\r\nUsers could define customized input paramaters using **--config** option in Snakemake command line.\r\n```bash\r\nUsage:\r\nsnakemake -s Delter.py --cores 8 --config Ref=refname Num=5 Vcf=path/to/VCF Refseq=path/to/refseq Outdir=path/to/outputdir Bam=path/to/sorted/bam Tombo_dir=path/to/tombo_processed/fast5 Subsample=2000 Flowcell=R9 Strategy=Direct MRPPthres=0.001 HomoQthres=23 OtherQthres=20.6\r\nRef=refname                             The value of #CHROM in vcf file, e.g., 'Ref=chr1'\r\nNum=5                                   The number of bases up- and down-stream that are centered around the variation loci, default=5\r\nVcf=path/to/VCF                         The file path to vcf file, e.g., 'Vcf=/data/res/lofreq.vcf'\r\nRefseq=path/to/refseq                   The file path to reference sequence, e.g., 'Refseq=/database/COVID-19.fa'\r\nOutdir=path/to/outputdir                The file path storing the output results and intermediate files, e.g., 'Outdir=/data/res'\r\nBam=path/to/sorted/bam                  The file path to sorted bam files, e.g., 'Bam=/data/res/sorted.bam'\r\nTombo_dir=path/to/tombo_processed/fast5 The file path to tombo-resquiggled single fats5 files, e.g., 'Tombo_dir=/data/fast5'\r\nSubsample=2000                          The number to subsample from reads covering variation loci, should be larger than 200, default=2000\r\nFlowcell=R9                             The version of flow cell, should be R9 or R10, default=R9\r\nStrategy=Direct                         The sequencing strategy, should be Amplicon or Direct, default=Direct\r\nMRPPthres=0.001                         The threshold of MRPP A, default=0.001\r\nHomoQthres=23                           The threshold of homo-dels, default=23\r\nOtherQthres=20.6                        The threshold of other-dels, default=20.6\r\n```\r\n(2) Edit config.yaml\r\n\r\nUsers could also define customized input paramaters by editing config.yaml.\r\n```yaml\r\nBam: \"/public/data1/yefq/data/fast5/20220703_WGA_twist/processed/20230426_Guppy621_comparison/Sce20_guppy_sup_aligned.softclip_trimmed.endtrim10_minimap2_align.mapped.sorted.bam\"\r\nRef: \"Zymo_Saccharomyces_cerevisiae_Seq5_ref\"\r\nNum: \"5\"\r\nTombo_dir: \"/public/data1/yefq/data/fast5/20220703_WGA_twist/processed/20230426_guppy_sup_basecalled/Sce20/workspace/fast5_pass_single/all_single_fast5s\"\r\nSubsample: \"2000\"\r\nVcf: \"/public/data1/yefq/data/fast5/20220703_WGA_twist/processed/20230426_Guppy621_comparison/Sce20_guppy_sup_aligned.test.vcf\" \r\nRefseq: \"/public/data1/yefq/data/Refs/Zymo_Saccharomyces_cerevisiae_Seq5_ref.fa\" \r\nOutdir: \"/public/data1/yefq/data/fast5/20220703_WGA_twist/processed/20230426_Guppy621_comparison/snakemake-tutorial/data/test\" \r\nFlowcell: \"R9\"\r\nStrategy: \"Direct\"\r\nMRPPthres: \"0.001\"\r\nHomoQthres: \"23\"\r\nOtherQthres: \"20.6\"\r\n```\r\nUsers should note that, **config values can be overwritten via the command line** even when it has deen defined in the config.yaml.\r\n### Start a run\r\nOnce the work directory and configuration files are set up, users can run the pipeline as easy as invoking:\r\n```bash\r\ncd /path/to/Delter/working/directory\r\nconda activate Delter\r\nsnakemake -s Delter.py --cores 8\r\n```\r\nOther Snakemake-related parameters like **--cores** and **--configfile** could be checked via \r\n```bash\r\nsnakemake -h\r\n```\r\n### Output\r\nThere are several outputs according to the indexes used. \r\n\r\n**(1) target.upstream_downstream.bases.comparison.result.txt**. When the workflow used MRPP A, the main output is **target.upstream_downstream.bases.comparison.result.txt**, which contains (1) the variation locus position, (2) group1 (plus.match or minus.match, corresponding to forward-aligned reads supporting the reference allele and reverse-aligned reads supporting the reference allele), (3) group2 (plus.del or minus.del, corresponding to forward-aligned reads supporting the non-reference allele and reverse-aligned reads supporting the non-reference allele), (4) the number of reads supporting group1 (**should always be around or higher than 400 in direct sequencing**), (5) the number of reads supporting group2 (**should always be around or higher than 400 in direct sequencing**), (6) the mean current measurements of upstream and downstream config[\"Num\"] bases centered around variation locus of group1, (7) the mean current measurements of upstream and downstream config[\"Num\"] bases centered around variation locus of group2, (8) P values between current measurements of group1 and group2, (9) MRPP P values, (10) **MRPP A statistic, users could compare this value against the pre-set threshold (WTA/Amplicon sequencing: 0.01; direct sequencing: 0.001) in our article to decide whether the variation locus is artificial**.\r\n\r\n**(2) fq.Qscore.info.txt**. For R9 or R10 data, when sequencing depth is low, Q score might be used to identify artificial deletions. The main output is **fq.Qscore.info.txt**, which contains (1) the variation locus position, (2) group1 (corresponding to forward-aligned reads supporting the non-reference allele and reverse-aligned reads supporting the non-reference allele), (3) group2 (plus.match or minus.match, corresponding to corresponding to forward-aligned reads supporting the reference allele and reverse-aligned reads supporting the reference allele), (4) the number of reads supporting group1 (**should always be \u226520**), (5) the number of reads supporting group2 (**should always be \u226520**), (6) the mean Q scores of upstream and downstream config[\"Num\"] bases centered around variation locus of group1, **users could compare this value against the pre-set threshold in our article to decide whether the variation locus is artificial**, (7) the mean Q scores of upstream and downstream config[\"Num\"] bases centered around variation locus of group2, (8) the P values between group1 and group2.\r\n\r\n**(3) variant.info.txt**. This file stores basic information of each variation output by VCF, which is used by the workflow. The 2th-10th columns are identical to VCF. Users should note that DP4 could be lower than DP, and **choosing to use MRPP A or Q score mainly depends on DP4 field**. The 12th-17th columns represent the location of deletion (homo or non-homo), the 1-based strating position, the 0-based starting position of homo or non-homo region, the 0-based ending position of homo or non-homo region, the deletion length output by Variation caller, and the length of homo or non-homo region (= 15th-14th+1). Our workflow use a strict criteria to extract reads with and without deletions. For example, if a deletion lacks 3 bases relative to the reference, then reads supporting the non-reference allele should only contain 3-base deletions. Therefore, **some deletions may be omitted due to undesirable read numbers**.     \r\n\r\n**(4) MRPP.filtered.txt** or **Qscore.filtered.txt**. Users should note that before comparing the results to pre-set thresholds, they are strongly recommended to filter the 4th and 5th columns in the **target.upstream_downstream.bases.comparison.result.txt** or/and **fq.Qscore.info.txt** according to our article, or the result may be biased due to low sequencing depth. We have provided two accessory scripts bundled in the workflow to pre-filter the results based on sequencing depth and then to compare with user-defined threshold(s). Position(s) with flags marked as \"FP\" are predicted artificial deletions. If neither of the above two files are generated, it indicates that none of potential variations could be removed due to very low sequencing depth or very low threshold(s).\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.1205.2",
        "edam_operation": [
            "Statistical calculation"
        ],
        "edam_topic": [
            "Computational biology"
        ],
        "filtered_on": "Amplicon in description",
        "id": "1205",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1205?version=2",
        "name": "ONT Artificial Deletion Filter-Delter",
        "number_of_steps": 0,
        "projects": [
            "NkuyfqLab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "indels",
            "r",
            "workflows"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-12-17",
        "versions": 2
    },
    {
        "create_time": "2024-12-10",
        "creators": [
            "Subina Mehta"
        ],
        "description": "Workflow for clinical metaproteomics database searching",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1225",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1225?version=1",
        "name": "clinicalmp-discovery/main",
        "number_of_steps": 24,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "fasta_merge_files_and_filter_unique_sequences",
            "filter_tabular",
            "search_gui",
            "query_tabular",
            "Grep1",
            "maxquant",
            "Cut1",
            "peptide_shaker",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "msconvert",
            "tp_cat",
            "ident_params",
            "fasta_cli",
            "fasta2tab",
            "dbbuilder"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2024-12-10",
        "creators": [
            "Valentine Murigneux",
            "Mike Thang"
        ],
        "description": "The aim of this workflow is to handle the routine part of shotgun metagenomics data processing. The workflow is using the tools Kraken2 and Bracken for taxonomy classification and the KrakenTools to evaluate diversity metrics. This workflow was tested on Galaxy Australia. \r\nA How-to guide for the workflow can be found at:  https://github.com/vmurigneu/kraken_howto_ga_workflows/blob/main/pages/taxonomy_kraken2_wf_guide.md  ",
        "doi": "10.48546/workflowhub.workflow.1199.2",
        "edam_operation": [],
        "edam_topic": [
            "Metagenomics",
            "Taxonomy"
        ],
        "filtered_on": "edam",
        "id": "1199",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1199?version=2",
        "name": "Taxonomy classification using Kraken2 and Bracken",
        "number_of_steps": 29,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gucfg2galaxy",
            "metagenomics",
            "name:collection",
            "shotgun"
        ],
        "tools": [
            "Prepare alpha diversity summary file (paste sample identifiers and Simpson results)\nPaste1",
            "__RELABEL_FROM_FILE__",
            "krakentools_kreport2krona",
            "add_line_to_file",
            "Prepare alpha diversity summary file\nPaste1",
            "Fisher results contains a header line we want to exclude \"Fisher's alpha...loading\"\nShow tail1",
            "Prepare alpha diversity summary file (paste Simpson and Fisher results)\nPaste1",
            "cat_multiple",
            "collection_column_join",
            "Extract column name and fraction_total_reads from Bracken report\nCut1",
            "krakentools_combine_kreports",
            "collection_element_identifiers",
            "taxonomy_krona_chart",
            "krakentools_beta_diversity",
            "regex1",
            "kraken2",
            "est_abundance",
            "krakentools_alpha_diversity"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-10",
        "versions": 2
    },
    {
        "create_time": "2024-11-04",
        "creators": [
            "Diego De Panis"
        ],
        "description": "**Assembly Evaluation for ERGA-BGE Reports**\r\n\r\n_One Assembly, Illumina WGS reads + HiC reads_\r\n\r\nThe workflow requires the following:\r\n* Species Taxonomy ID number\r\n* NCBI Genome assembly accession code\r\n* BUSCO Lineage\r\n* WGS accurate reads accession code\r\n* NCBI HiC reads accession code\r\n\r\nThe workflow will get the data and process it to generate genome profiling (genomescope, smudgeplot -optional-), assembly stats (gfastats), merqury stats (QV, completeness), BUSCO, snailplot, contamination blobplot, and HiC heatmap.\r\n\r\n**Use this workflow for ONT-based assemblies where the WGS accurate reads are Illumina PE**",
        "doi": null,
        "edam_operation": [
            "Genome assembly"
        ],
        "edam_topic": [
            "Genomics"
        ],
        "filtered_on": "profil.* in description",
        "id": "1103",
        "keep": "Reject",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1103?version=3",
        "name": "ERGA-BGE Genome Report ASM analyses (one-asm WGS Illumina PE + HiC)",
        "number_of_steps": 45,
        "projects": [
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "biodiversity",
            "genome assembly",
            "genomics",
            "qc"
        ],
        "tools": [
            "blobtoolkit",
            "CONVERTER_fasta_to_fai",
            "cooler_cload_tabix",
            "pairtools_sort",
            "Cut1",
            "gfastats",
            "sambamba_merge",
            "bg_diamond",
            "bedtools_makewindowsbed",
            "pick_value",
            "meryl",
            "lftp",
            "busco",
            "fastp",
            "tp_text_file_with_recurring_lines",
            "hicexplorer_hicplotmatrix",
            "__FLATTEN__",
            "merqury",
            "cooler_csort_tabix",
            "bwa_mem2",
            "sambamba_flagstat",
            "hicexplorer_hicmergematrixbins",
            "datasets_download_genome",
            "pairtools_parse",
            "fasterq_dump",
            "pairtools_split",
            "__EXTRACT_DATASET__",
            "genomescope",
            "smudgeplot",
            "pairtools_dedup",
            "collapse_dataset",
            "sam_merge2",
            "rseqc_bam_stat"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-05",
        "versions": 3
    },
    {
        "create_time": "2024-11-04",
        "creators": [
            "Diego De Panis"
        ],
        "description": "**Assembly Evaluation for ERGA-BGE Reports**\r\n\r\n_One Assembly, HiFi WGS reads + HiC reads_\r\n\r\nThe workflow requires the following:\r\n* Species Taxonomy ID number\r\n* NCBI Genome assembly accession code\r\n* BUSCO Lineage\r\n* WGS accurate reads accession code\r\n* NCBI HiC reads accession code\r\n\r\nThe workflow will get the data and process it to generate genome profiling (genomescope, smudgeplot -optional-), assembly stats (gfastats), merqury stats (QV, completeness), BUSCO, snailplot, contamination blobplot, and HiC heatmap.\r\n\r\n**Use this workflow for HiFi-based assemblies where the WGS accurate reads are PacBio HiFi**",
        "doi": null,
        "edam_operation": [
            "Genome assembly"
        ],
        "edam_topic": [
            "Genomics"
        ],
        "filtered_on": "profil.* in description",
        "id": "1104",
        "keep": "Reject",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1104?version=2",
        "name": "ERGA-BGE Genome Report ASM analyses (one-asm HiFi + HiC)",
        "number_of_steps": 45,
        "projects": [
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genome assembly",
            "genomics",
            "qc"
        ],
        "tools": [
            "blobtoolkit",
            "CONVERTER_fasta_to_fai",
            "cooler_cload_tabix",
            "pairtools_sort",
            "Cut1",
            "gfastats",
            "sambamba_merge",
            "bedtools_makewindowsbed",
            "pick_value",
            "meryl",
            "lftp",
            "busco",
            "fastp",
            "tp_text_file_with_recurring_lines",
            "hicexplorer_hicplotmatrix",
            "__FLATTEN__",
            "merqury",
            "cooler_csort_tabix",
            "bwa_mem2",
            "rseqc_bam_stat",
            "sambamba_flagstat",
            "hicexplorer_hicmergematrixbins",
            "datasets_download_genome",
            "minimap2",
            "pairtools_parse",
            "fasterq_dump",
            "pairtools_split",
            "__EXTRACT_DATASET__",
            "cutadapt",
            "genomescope",
            "smudgeplot",
            "pairtools_dedup",
            "collapse_dataset",
            "sam_merge2",
            "bg_diamond"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-05",
        "versions": 2
    },
    {
        "create_time": "2024-12-03",
        "creators": [
            "Ekaterina Sakharova",
            "Tatiana Gurbich",
            "Martin Beracochea"
        ],
        "description": "# MGnify genomes catalogue pipeline\r\n\r\n[MGnify](https://www.ebi.ac.uk/metagenomics/) A pipeline to perform taxonomic and functional annotation and to generate a catalogue from a set of isolate and/or metagenome-assembled genomes (MAGs) using the workflow described in the following publication:\r\n\r\nGurbich TA, Almeida A, Beracochea M, Burdett T, Burgin J, Cochrane G, Raj S, Richardson L, Rogers AB, Sakharova E, Salazar GA and Finn RD. (2023) [MGnify Genomes: A Resource for Biome-specific Microbial Genome Catalogues.](https://www.sciencedirect.com/science/article/pii/S0022283623000724) <i>J Mol Biol</i>. doi: https://doi.org/10.1016/j.jmb.2023.168016\r\n\r\nDetailed information about existing MGnify catalogues: https://docs.mgnify.org/src/docs/genome-viewer.html\r\n\r\n### Tools used in the pipeline\r\n| Tool/Database                                                                                    | Version           | Purpose                                                                                                                |\r\n|--------------------------------------------------------------------------------------------------|-------------------|------------------------------------------------------------------------------------------------------------------------|\r\n| CheckM2                                                                                          | 1.0.1             | Determining genome quality                                                                                             |\r\n| dRep                                                                                             | 3.2.2             | Genome clustering                                                                                                      |\r\n| Mash                                                                                             | 2.3               | Sketch for the catalogue; placement of genomes into clusters (update only); strain tree                                |\r\n| GUNC                                                                                             | 1.0.3             | Quality control                                                                                                        |\r\n| GUNC DB                                                                                          | 2.0.4             | Database for GUNC                                                                                                      |\r\n| GTDB-Tk                                                                                          | 2.4.0             | Assigning taxonomy; generating alignments                                                                              |\r\n| GTDB                                                                                             | r220              | Database for GTDB-Tk                                                                                                   |\r\n| Prokka                                                                                           | 1.14.6            | Protein annotation                                                                                                     |\r\n| IQ-TREE 2                                                                                        | 2.2.0.3           | Generating a phylogenetic tree                                                                                         |\r\n| Kraken 2                                                                                         | 2.1.2             | Generating a kraken database                                                                                           |\r\n| Bracken                                                                                          | 2.6.2             | Generating a bracken database                                                                                          |\r\n| MMseqs2                                                                                          | 13.45111          | Generating a protein catalogue                                                                                         |\r\n| eggNOG-mapper                                                                                    | 2.1.11            | Protein annotation (eggNOG, KEGG, COG,  CAZy)                                                                          |\r\n| eggNOG DB                                                                                        | 5.0.2             | Database for eggNOG-mapper                                                                                             |\r\n| Diamond                                                                                          | 2.0.11            | Protein annotation (eggNOG)                                                                                            |\r\n| InterProScan                                                                                     | 5.62-94.0         | Protein annotation (InterPro, Pfam)                                                                                    |\r\n| kegg-pathways-completeness tool                                                                  | 1.0.5             | Computes KEGG pathway completeness                                                                                     |\r\n| CRISPRCasFinder                                                                                  | 4.3.2             | Annotation of CRISPR arrays                                                                                            |\r\n| AMRFinderPlus                                                                                    | 3.11.4            | Antimicrobial resistance gene annotation; virulence factors, biocide, heat, acid, and metal resistance gene annotation |\r\n| AMRFinderPlus DB                                                                                 | 3.11 2023-02-23.1 | Database for AMRFinderPlus                                                                                             |\r\n| antiSMASH                                                                                        | 7.1.0             | Biosynthetic gene cluster annotation                                                                                   |\r\n| GECCO                                                                                            | 0.9.8             | Biosynthetic gene cluster annotation                                                                                   |\r\n| SanntiS                                                                                          | 0.9.3.2           | Biosynthetic gene cluster annotation                                                                                   |\r\n| DefenseFinder                                                                                    | 1.2.0             | Annotation of anti-phage systems                                                                                       |\r\n| DefenseFinder models                                                                             | 1.2.3             | Database for DefenseFinder                                                                                             |\r\n| run_dbCAN                                                                                        | 4.1.2             | Polysaccharide utilization loci prediction                                                                             |\r\n| dbCAN DB                                                                                         | V12               | Database for run_dbCAN                                                                                                 |\r\n| Infernal                                                                                         | 1.1.4             | RNA predictions                                                                                                        |\r\n| tRNAscan-SE                                                                                      | 2.0.9             | tRNA predictions                                                                                                       |\r\n| Rfam                                                                                             | 14.9              | Identification of SSU/LSU rRNA and other ncRNAs                                                                        |\r\n| Panaroo                                                                                          | 1.3.2             | Pan-genome computation                                                                                                 |\r\n| Seqtk                                                                                            | 1.3               | Generating a gene catalogue                                                                                            |\r\n| VIRify                                                                                           | 2.0.1             | Viral sequence annotation                                                                                              |\r\n| [Mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline) | 2.0.2             | Mobilome annotation                                                                                                    |\r\n| samtools                                                                                         | 1.15              | FASTA indexing                                                                                                         |\r\n\r\n## Setup\r\n\r\n### Environment\r\n\r\nThe pipeline is implemented in [Nextflow](https://www.nextflow.io/).\r\n\r\nRequirements:\r\n- [singulairty](https://sylabs.io/docs/) or [docker](https://www.docker.com/)\r\n\r\n#### Reference databases\r\n\r\nThe pipeline needs the following reference databases and configuration files (roughtly ~150G):\r\n\r\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/gunc_db_2.0.4.dmnd.gz\r\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/eggnog_db_5.0.2.tgz\r\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/rfam_14.9/\r\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/kegg_classes.tsv\r\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/continent_countries.csv\r\n- https://data.ace.uq.edu.au/public/gtdb/data/releases/release214/214.0/auxillary_files/gtdbtk_r214_data.tar.gz\r\n- ftp://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/AMRFinderPlus/database/3.11/2023-02-23.1\r\n- https://zenodo.org/records/4626519/files/uniref100.KO.v1.dmnd.gz\r\n\r\n### Containers\r\n\r\nThis pipeline requires [singularity](https://sylabs.io/docs/) or [docker](https://www.docker.com/) as the container engine to run pipeline.\r\n\r\nThe containers are hosted in [biocontainers](https://biocontainers.pro/) and [quay.io/microbiome-informatics](https://quay.io/organization/microbiome-informatics) repository.\r\n\r\nIt's possible to build the containers from scratch using the following script:\r\n\r\n```bash\r\ncd containers && bash build.sh\r\n```\r\n\r\n## Running the pipeline\r\n\r\n## Data preparation\r\n\r\n1. You need to pre-download your data to directories and make sure that genomes are uncompressed. Scripts to fetch genomes from ENA ([fetch_ena.py](https://github.com/EBI-Metagenomics/genomes-pipeline/blob/master/bin/fetch_ena.py)) and NCBI ([fetch_ncbi.py](https://github.com/EBI-Metagenomics/genomes-pipeline/blob/master/bin/fetch_ncbi.py)) are provided and need to be executed separately from the pipeline. If you have downloaded genomes from both ENA and NCBI, put them into separate folders.\r\n\r\n2. When genomes are fetched from ENA using the `fetch_ena.py` script, a CSV file with contamination and completeness statistics is also created in the same directory where genomes are saved to. If you are downloading genomes using a different approach, a CSV file needs to be created manually (each line should be genome accession, % completeness, % contamination). The ENA fetching script also pre-filters genomes to satisfy the QS50 cut-off (QS = % completeness - 5 * % contamination).\r\n\r\n3. You will need the following information to run the pipeline:\r\n - catalogue name (for example, zebrafish-faecal)\r\n - catalogue version (for example, 1.0)\r\n - catalogue biome (for example, root:Host-associated:Human:Digestive system:Large intestine:Fecal)\r\n - min and max accession number to be assigned to the genomes (only MGnify specific). Max - Min = #total number of genomes (NCBI+ENA)\r\n\r\n### Execution\r\n\r\nThe pipeline is built in [Nextflow](https://www.nextflow.io), and utilized containers to run the software (we don't support conda ATM).\r\nIn order to run the pipeline it's required that the user creates a profile that suits their needs, there is an `ebi` profile in `nexflow.config` that can be used as template.\r\n\r\nAfter downloading the databases and adjusting the config file:\r\n\r\n```bash\r\nnextflow run EBI-Metagenomics/genomes-pipeline -c <custom.config> -profile <profile> \\\r\n--genome-prefix=MGYG \\\r\n--biome=\"root:Host-associated:Fish:Digestive system\" \\\r\n--ena_genomes=<path to genomes> \\\r\n--ena_genomes_checkm=<path to genomes quality data> \\\r\n--mgyg_start=0 \\\r\n--mgyg_end=10 \\\r\n--preassigned_accessions=<path to file with preassigned accessions if using>\r\n--catalogue_name=zebrafish-faecal \\\r\n--catalogue_version=\"1.0\" \\\r\n--ftp_name=\"zebrafish-faecal\" \\\r\n--ftp_version=\"v1.0\" \\\r\n--outdir=\"<path-to-results>\"\r\n```\r\n\r\n### Development\r\n\r\nInstall development tools (including pre-commit hooks to run Black code formatting).\r\n\r\n```bash\r\npip install -r requirements-dev.txt\r\npre-commit install\r\n```\r\n\r\n#### Code style\r\n\r\nUse Black, this tool is configured if you install the pre-commit tools as above.\r\n\r\nTo manually run them: black .\r\n\r\n### Testing\r\n\r\nThis repo has 2 set of tests, python unit tests for some of the most critical python scripts and [nf-test](https://github.com/askimed/nf-test) scripts for the nextflow code.\r\n\r\nTo run the python tests\r\n\r\n```bash\r\npip install -r requirements-test.txt\r\npytest\r\n```\r\n\r\nTo run the nextflow ones the databases have to downloaded manually, we are working to improve this.\r\n\r\n```bash\r\nnf-test test tests/*\r\n```\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "462",
        "keep": "To Curate",
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/462?version=3",
        "name": "MGnify genomes catalogue pipeline",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "metagenomics",
            "nextflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-12-03",
        "versions": 3
    },
    {
        "create_time": "2024-11-26",
        "creators": [
            "Pratik Jagtap"
        ],
        "description": "In proteomics research, verifying detected peptides is essential for ensuring data accuracy and biological relevance. This tutorial continues from the clinical metaproteomics discovery workflow, focusing on verifying identified microbial peptides using the PepQuery tool.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1218",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1218?version=1",
        "name": "clinicalmp-verification/main",
        "number_of_steps": 19,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "dbbuilder",
            "uniprotxml_downloader",
            "query_tabular",
            "Cut1",
            "Filter1",
            "Remove beginning1",
            "Grouping1",
            "collapse_dataset",
            "pepquery2",
            "tp_cat",
            "fasta_merge_files_and_filter_unique_sequences"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2024-11-23",
        "creators": [
            "Sabrina Krakau",
            "Leon Kuchenbecker and Till Englert"
        ],
        "description": "From metagenomes to peptides",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in name",
        "id": "1217",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1217?version=1",
        "name": "nf-core/metapep",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-11-23",
        "versions": 1
    },
    {
        "create_time": "2024-11-14",
        "creators": [
            "Helena Rasche",
            "Dennis Doll\u00e9e",
            "Birgit Rijvers"
        ],
        "description": "This is an aggregation of the work done in [Seq4AMR](https://workflowhub.eu/projects/110) consisting of the following workflows:\r\n\r\n- [WF1: AbritAMR / AMRFinderPlus](https://workflowhub.eu/workflows/634)\r\n- [WF2: Sciensano](https://workflowhub.eu/workflows/644) (**not currently included**)\r\n- [WF3: SRST2](https://workflowhub.eu/workflows/407) \r\n- [WF4: StarAMR](https://workflowhub.eu/workflows/470)\r\n\r\n## Installation\r\n\r\n- You will need to:\r\n    - run the [RGI Database Builder](https://my.galaxy.training/?path=?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fcard%2Frgi%2Frgi_database_builder%2F1.2.0) as a Galaxy admin (if this hasn't been done already)\r\n    - [Have the en_US.UTF-8 locale installed](https://github.com/galaxyproject/tools-iuc/issues/6467) on the compute nodes executing cast/melt jobs.\r\n    - Install the requisite tools with e.g. [`shed-tools`](https://ephemeris.readthedocs.io/en/latest/commands/shed-tools.html) command from the [`ephemeris`](https://ephemeris.readthedocs.io/en/latest/) suite: `shed-tools install -g https://galaxy.example.com -a API_KEY -t tools.yaml` (tools.yaml is provided in this repository.)\r\n- Then you can import this workflow\r\n    - Navigate to `/workflows/import` of your Galaxy server\r\n    - Select \"GA4GH servers\"\r\n    - Enter `name:\"AMR-Pathfinder\"`\r\n- And run it\r\n    - You must provide a Sequencing collection (list:paired of fastq files)\r\n    - And a Genomes collection (list of fasta files) \r\n    - Both of these should use **identical** collection element identifiers\r\n\r\n## Outputs\r\n\r\nThis will produce two important tables: \"Binary Comparison\" and a \"% Identity Scored Outputs\". \r\n\r\n### Binary comparison\r\n\r\nThis file reports the discovery or absence of specific AMR genes across all tested AMR Analysis tools. You will mostly see 1s (presence) or 0s (absence) but you may occasionally see higher numbers when an AMR tool reports multiple hits for a specific gene.\r\n\r\n### % Identity Scored Outputs\r\n\r\nThis is similar to binary comparison, but using the % identity reported by each AMR tool. For cases where multiple hits were detected, we take the highest.\r\n\r\n## Known Issues\r\n\r\nThe names for identified AMR genes is highly inconsistent across AMR analysis tools. We urge the AMR community to rectify this by standardising gene names used in their tooling.",
        "doi": null,
        "edam_operation": [
            "Antimicrobial resistance prediction"
        ],
        "edam_topic": [],
        "filtered_on": "amr in tags",
        "id": "1189",
        "keep": "Keep",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1189?version=2",
        "name": "AMR-Pathfinder",
        "number_of_steps": 36,
        "projects": [
            "Seq4AMR",
            "ErasmusMC Clinical Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "amr",
            "amr-detection",
            "benchamrking"
        ],
        "tools": [
            "",
            "tp_split_on_column",
            "Cut1",
            "tp_text_file_with_recurring_lines",
            "datamash_ops",
            "cat1",
            "staramr_search",
            "addValue",
            "tp_find_and_replace",
            "hamronize_summarize",
            "cast",
            "__MERGE_COLLECTION__",
            "cat_multi_datasets",
            "hamronize_tool",
            "shovill",
            "__APPLY_RULES__",
            "abricate",
            "collapse_dataset",
            "Grep1"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-24",
        "versions": 2
    },
    {
        "create_time": "2024-11-01",
        "creators": [
            "Samuel Lambert",
            "Benjamin Wingfield",
            "Laurent Gil"
        ],
        "description": "# The Polygenic Score Catalog Calculator (`pgsc_calc`)\r\n\r\n[![Documentation Status](https://readthedocs.org/projects/pgsc-calc/badge/?version=latest)](https://pgsc-calc.readthedocs.io/en/latest/?badge=latest)\r\n[![pgscatalog/pgsc_calc CI](https://github.com/PGScatalog/pgsc_calc/actions/workflows/ci.yml/badge.svg)](https://github.com/PGScatalog/pgsc_calc/actions/workflows/ci.yml)\r\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5970794.svg)](https://doi.org/10.5281/zenodo.5970794)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-\u226523.10.0-23aa62.svg?labelColor=000000)](https://www.nextflow.io/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n\r\n## Introduction\r\n\r\n`pgsc_calc` is a bioinformatics best-practice analysis pipeline for calculating\r\npolygenic [risk] scores on samples with imputed genotypes using existing scoring\r\nfiles from the [Polygenic Score (PGS) Catalog](https://www.pgscatalog.org/)\r\nand/or user-defined PGS/PRS.\r\n\r\n## Pipeline summary\r\n\r\n> [!IMPORTANT]  \r\n> * Whole genome sequencing (WGS) data [are not currently supported by the calculator](https://pgsc-calc.readthedocs.io/en/latest/explanation/match.html#are-your-target-genomes-imputed-are-they-wgs)\r\n> * It\u2019s possible to [create compatible gVCFs from WGS data](https://github.com/PGScatalog/pgsc_calc/discussions/123#discussioncomment-6469422). We plan to improve support for WGS data in the near future.\r\n\r\n<p align=\"center\">\r\n  <img width=\"80%\" src=\"https://github.com/PGScatalog/pgsc_calc/assets/11425618/f766b28c-0f75-4344-abf3-3463946e36cc\">\r\n</p>\r\n\r\nThe workflow performs the following steps:\r\n\r\n* Downloading scoring files using the PGS Catalog API in a specified genome build (GRCh37 and GRCh38).\r\n* Reading custom scoring files (and performing a liftover if genotyping data is in a different build).\r\n* Automatically combines and creates scoring files for efficient parallel computation of multiple PGS\r\n    - Matching variants in the scoring files against variants in the target dataset (in plink bfile/pfile or VCF format)\r\n* Calculates PGS for all samples (linear sum of weights and dosages)\r\n* Creates a summary report to visualize score distributions and pipeline metadata (variant matching QC)\r\n\r\nAnd optionally:\r\n\r\n- Genetic Ancestry: calculate similarity of target samples to populations in a\r\n  reference dataset ([1000 Genomes (1000G)](http://www.nature.com/nature/journal/v526/n7571/full/nature15393.html)), using principal components analysis (PCA)\r\n- PGS Normalization: Using reference population data and/or PCA projections to report\r\n  individual-level PGS predictions (e.g. percentiles, z-scores) that account for genetic ancestry\r\n\r\nSee documentation for a list of planned [features under development](https://pgsc-calc.readthedocs.io/en/latest/index.html#Features-under-development).\r\n\r\n### PGS applications and libraries\r\n\r\n`pgsc_calc` uses applications and libraries internally developed at the PGS Catalog, which can do helpful things like:\r\n\r\n* Query the PGS Catalog to bulk download scoring files in a specific genome build\r\n* Match variants from scoring files to target variants\r\n* Adjust calculated PGS in the context of genetic ancestry\r\n\r\nIf you want to write Python code to work with PGS, [check out the `pygscatalog` repository to learn more](https://github.com/PGScatalog/pygscatalog).\r\n\r\nIf you want a simpler way of working with PGS, ignore this section and continue below to learn more about `pgsc_calc`.\r\n\r\n## Quick start\r\n\r\n1. Install\r\n[`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation)\r\n(`>=23.10.0`)\r\n\r\n2. Install [`Docker`](https://docs.docker.com/engine/installation/) or\r\n[`Singularity (v3.8.3 minimum)`](https://www.sylabs.io/guides/3.0/user-guide/)\r\n(please only use [`Conda`](https://conda.io/miniconda.html) as a last resort)\r\n\r\n3. Download the pipeline and test it on a minimal dataset with a single command:\r\n\r\n    ```console\r\n    nextflow run pgscatalog/pgsc_calc -profile test,<docker/singularity/conda>\r\n    ```\r\n\r\n4. Start running your own analysis!\r\n\r\n    ```console\r\n    nextflow run pgscatalog/pgsc_calc -profile <docker/singularity/conda> --input samplesheet.csv --pgs_id PGS001229\r\n    ```\r\n\r\nSee [getting\r\nstarted](https://pgsc-calc.readthedocs.io/en/latest/getting-started.html) for more\r\ndetails.\r\n\r\n## Documentation\r\n\r\n[Full documentation is available on Read the Docs](https://pgsc-calc.readthedocs.io/)\r\n\r\n## Credits\r\n\r\npgscatalog/pgsc_calc is developed as part of the PGS Catalog project, a\r\ncollaboration between the University of Cambridge\u2019s Department of Public Health\r\nand Primary Care (Michael Inouye, Samuel Lambert) and the European\r\nBioinformatics Institute (Helen Parkinson, Laura Harris).\r\n\r\nThe pipeline seeks to provide a standardized workflow for PGS calculation and\r\nancestry inference implemented in nextflow derived from an existing set of\r\ntools/scripts developed by Inouye lab (Rodrigo Canovas, Scott Ritchie, Jingqin\r\nWu) and PGS Catalog teams (Samuel Lambert, Laurent Gil).\r\n\r\nThe adaptation of the codebase, nextflow implementation, and PGS Catalog features\r\nare written by Benjamin Wingfield, Samuel Lambert, Laurent Gil with additional input\r\nfrom Aoife McMahon (EBI). Development of new features, testing, and code review\r\nis ongoing including Inouye lab members (Rodrigo Canovas, Scott Ritchie) and others. If \r\nyou use the tool we ask you to cite our paper describing software and updated PGS Catalog resource:\r\n\r\n- >Lambert, Wingfield _et al._ (2024) Enhancing the Polygenic Score Catalog with tools for score \r\n  calculation and ancestry normalization. Nature Genetics.\r\n  doi:[10.1038/s41588-024-01937-x](https://doi.org/10.1038/s41588-024-01937-x).\r\n\r\nThis pipeline is distrubuted under an [Apache License](LICENSE) amd uses code and \r\ninfrastructure developed and maintained by the [nf-core](https://nf-co.re) community \r\n(Ewels *et al. Nature Biotech* (2020) doi:[10.1038/s41587-020-0439-x](https://doi.org/10.1038/s41587-020-0439-x)), \r\nreused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\nAdditional references of open-source tools and data used in this pipeline are described in\r\n[`CITATIONS.md`](CITATIONS.md).\r\n\r\nThis work has received funding from EMBL-EBI core funds, the Baker Institute,\r\nthe University of Cambridge, Health Data Research UK (HDRUK), and the European\r\nUnion\u2019s Horizon 2020 research and innovation programme under grant agreement No\r\n101016775 INTERVENE.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "DNA polymorphism",
            "Genetic variation",
            "Genomics",
            "Human genetics"
        ],
        "filtered_on": "profil.* in description",
        "id": "556",
        "keep": "To Curate",
        "latest_version": 7,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/556?version=7",
        "name": "The Polygenic Score Catalog Calculator",
        "number_of_steps": 0,
        "projects": [
            "Polygenic Score Catalog"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gwas",
            "nextflow",
            "workflows",
            "genomic ancestry",
            "polygenic risk score",
            "polygenic score",
            "prediction"
        ],
        "tools": [
            "PLINK"
        ],
        "type": "Nextflow",
        "update_time": "2024-11-01",
        "versions": 7
    },
    {
        "create_time": "2024-10-21",
        "creators": [],
        "description": "# Metagenome-Atlas\r\n\r\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/metagenome-atlas/badges/latest_release_relative_date.svg)](https://anaconda.org/bioconda/metagenome-atlas)\r\n[![Bioconda](https://img.shields.io/conda/dn/bioconda/metagenome-atlas.svg?label=Bioconda )](https://anaconda.org/bioconda/metagenome-atlas)\r\n[![Documentation Status](https://readthedocs.org/projects/metagenome-atlas/badge/?version=latest)](https://metagenome-atlas.readthedocs.io/en/latest/?badge=latest)\r\n![Mastodon Follow](https://img.shields.io/mastodon/follow/109273833677404282?domain=https%3A%2F%2Fmstdn.science&style=social)\r\n<!--[![follow on twitter](https://img.shields.io/twitter/follow/SilasKieser.svg?style=social&label=Follow)](https://twitter.com/search?f=tweets&q=%40SilasKieser%20%23metagenomeAtlas&src=typd) -->\r\n\r\n\r\nMetagenome-atlas is a easy-to-use metagenomic pipeline based on snakemake. It handles all steps from QC, Assembly, Binning, to Annotation.\r\n\r\n![scheme of workflow](resources/images/atlas_list.png?raw=true)\r\n\r\nYou can start using atlas with three commands:\r\n```\r\n    mamba install -y -c bioconda -c conda-forge metagenome-atlas={latest_version}\r\n    atlas init --db-dir databases path/to/fastq/files\r\n    atlas run all\r\n```\r\nwhere `{latest_version}` should be replaced by [![Version](https://anaconda.org/bioconda/metagenome-atlas/badges/version.svg)](https://anaconda.org/bioconda/metagenome-atlas)\r\n\r\n\r\n# Webpage\r\n\r\n[metagenome-atlas.github.io](https://metagenome-atlas.github.io/)\r\n\r\n# Documentation\r\n\r\nhttps://metagenome-atlas.readthedocs.io/\r\n\r\n[Tutorial](https://github.com/metagenome-atlas/Tutorial)\r\n\r\n# Citation\r\n\r\n> ATLAS: a Snakemake workflow for assembly, annotation, and genomic binning of metagenome sequence data.  \r\n> Kieser, S., Brown, J., Zdobnov, E. M., Trajkovski, M. & McCue, L. A.   \r\n> BMC Bioinformatics 21, 257 (2020).  \r\n> doi: [10.1186/s12859-020-03585-4](https://doi.org/10.1186/s12859-020-03585-4)\r\n\r\n\r\n# Developpment/Extensions\r\n\r\nHere are some ideas I work or want to work on when I have time. If you want to contribute or have some ideas let me know via a feature request issue.\r\n\r\n- Optimized MAG recovery (e.g. [Spacegraphcats](https://github.com/spacegraphcats/spacegraphcats))\r\n- Integration of viruses/plasmid that live for now as [extensions](https://github.com/metagenome-atlas/virome_atlas)\r\n- Add statistics and visualisations as in [atlas_analyze](https://github.com/metagenome-atlas/atlas_analyze)\r\n- Implementation of most rules as snakemake wrapper\r\n- Cloud execution\r\n- Update to new Snakemake version and use cool reports.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "1183",
        "keep": "Keep",
        "latest_version": 1,
        "license": "BSD-3-Clause",
        "link": "https:/workflowhub.eu/workflows/1183?version=1",
        "name": "Metaenome-Atlas",
        "number_of_steps": 0,
        "projects": [
            "Snakemake-Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2024-10-21",
        "versions": 1
    },
    {
        "create_time": "2024-10-01",
        "creators": [
            "Jasper Koehorst",
            "Bart Nijsse"
        ],
        "description": "### Workflow for microbial (meta-)genome annotation\r\n\r\nInput is a (meta)genome sequence in fasta format.\r\n\r\n* bakta\r\n* KoFamScan (optional)\r\n* InterProScan (optional)\r\n* eggNOG mapper (optional)\r\n\r\n* To RDF conversion with SAPP (optional, default on) --> [SAPP conversion Workflow in WorkflowHub](https://workflowhub.eu/workflows/1174/)\r\n\r\ngit: [https://gitlab.com/m-unlock/cwl](https://gitlab.com/m-unlock/cwl)",
        "doi": null,
        "edam_operation": [
            "Annotation",
            "Gene functional annotation",
            "Genome annotation"
        ],
        "edam_topic": [
            "Microbiology"
        ],
        "filtered_on": "metage.* in tags",
        "id": "1170",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/1170?version=1",
        "name": "Microbial (meta-) genome annotation",
        "number_of_steps": 9,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "bioinformatics",
            "genome",
            "metagenome",
            "microbial"
        ],
        "tools": [
            "KofamScan",
            "eggNOG-mapper",
            "Move all Bakta files to a folder",
            "Bacterial genome annotation tool",
            "Compress files when compression is true",
            "Compress Bakta",
            "Gather files when compression is false",
            "InterProScan 5"
        ],
        "type": "Common Workflow Language",
        "update_time": "2025-10-03",
        "versions": 1
    },
    {
        "create_time": "2024-10-07",
        "creators": [
            "GalaxyP"
        ],
        "description": "Clinical Metaproteomics 4: Quantitation ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in description",
        "id": "1177",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1177?version=1",
        "name": "clinicalmp-quantitation/main",
        "number_of_steps": 7,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "extract peptides\nCut1",
            "maxquant",
            "extracting microbial Peptides\nGrep1",
            "extracting microbial Proteins\nGrep1",
            "Quantified-Proteins\nGrouping1",
            "extract proteins\nCut1",
            "Quantified-Peptides\nGrouping1"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Wolfgang Maier"
        ],
        "description": "COVID-19: variation analysis on ARTIC PE data\r\n---------------------------------------------\r\n\r\nThe workflow for Illumina-sequenced ampliconic data builds on the RNASeq workflow\r\nfor paired-end data using the same steps for mapping and variant calling, but\r\nadds extra logic for trimming amplicon primer sequences off reads with the ivar\r\npackage. In addition, this workflow uses ivar also to identify amplicons\r\naffected by primer-binding site mutations and, if possible, excludes reads\r\nderived from such \"tainted\" amplicons when calculating allele-frequencies\r\nof other variants.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in description",
        "id": "110",
        "keep": "Reject",
        "latest_version": 11,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/110?version=11",
        "name": "sars-cov-2-pe-illumina-artic-variant-calling/COVID-19-PE-ARTIC-ILLUMINA",
        "number_of_steps": 25,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "artic",
            "by-covid",
            "covid-19",
            "covid19.galaxyproject.org"
        ],
        "tools": [
            "snpSift_filter",
            "vcfvcfintersect",
            "qualimap_bamqc",
            "lofreq_viterbi",
            "bwa_mem",
            "fastp",
            "samtools_stats",
            "ivar_trim",
            "\n __FILTER_FAILED_DATASETS__",
            "lofreq_indelqual",
            "multiqc",
            "lofreq_filter",
            "\n __FLATTEN__",
            "compose_text_param",
            "bcftools_annotate",
            "samtools_view",
            "tp_replace_in_line",
            "snpeff_sars_cov_2",
            "lofreq_call",
            "ivar_removereads"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 11
    },
    {
        "create_time": "2022-11-24",
        "creators": [],
        "description": "With this galaxy pipeline you can use Salmonella sp. next generation sequencing results to predict bacterial AMR phenotypes and compare the results against gold standard Salmonella sp. phenotypes obtained from food.\r\n\r\nThis pipeline is based on the work of the National Food Agency of Canada.  \r\nDoi: [10.3389/fmicb.2020.00549](https://doi.org/10.3389/fmicb.2020.00549)\r\n\r\n| tool | version | license |\r\n| -- | -- | -- |\r\n| SeqSero2 | 1.2.1 | [GNU GPL v2.0](https://github.com/denglab/SeqSero2/blob/master/LICENSE) |\r\n| BBTools | 39.01 | [MIT License](https://github.com/kbaseapps/BBTools/blob/master/LICENSE) |\r\n| SRST2 | 0.2.0 | [BSD License](https://github.com/katholt/srst2/blob/master/LICENSE.txt) |\r\n| hamronize | 1.0.3 | [GNU LGPL v3.0](https://github.com/pha4ge/hAMRonization/blob/master/LICENSE.txt) |\r\n| SPAdes | v3.15.5 | [GNU GPL v2.0](https://github.com/ablab/spades/blob/main/LICENSE) |\r\n| SKESA | 3.0.0 | [Public Domain](https://github.com/ncbi/SKESA/blob/master/LICENSE) |\r\n| pilon | 1.1.0 | [GNU GPL v2.0](https://github.com/broadinstitute/pilon/blob/master/LICENSE) |\r\n| shovill | 1.0.4 | [GPL-3.0 license](https://github.com/tseemann/shovill/blob/master/LICENSE) |\r\n| sistr | 1.1.1 | [Apache-2.0 license](https://github.com/phac-nml/sistr_cmd/blob/master/LICENSE) |\r\n| MOB-Recon | 3.0.3 | [Apache-2.0 license](https://github.com/phac-nml/mob-suite/blob/master/LICENSE) |",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "antimicrobial.* in tags",
        "id": "407",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/407?version=1",
        "name": "Workflow 3: AMR - SeqSero2/SISTR",
        "number_of_steps": 14,
        "projects": [
            "Seq4AMR"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "antimicrobial resistance"
        ],
        "tools": [
            "__UNZIP_COLLECTION__",
            "kma_map",
            "srst2",
            "hamronize_tool",
            "sistr_cmd",
            "bbtools_tadpole",
            "shovill",
            "hamronize_summarize",
            "mob_recon",
            "seqsero2",
            "bbtools_bbduk"
        ],
        "type": "Galaxy",
        "update_time": "2024-09-09",
        "versions": 1
    },
    {
        "create_time": "2023-05-11",
        "creators": [],
        "description": "Correlation between Phenotypic and In Silico Detection of Antimicrobial Resistance in Salmonella enterica in Canada Using Staramr. \r\n\r\nDoi: [10.3390/microorganisms10020292](https://doi.org/10.3390/microorganisms10020292)\r\n\r\n| tool | version | license |\r\n| -- | -- | -- |\r\n| staramr | 0.8.0 | [Apache-2.0 license](https://github.com/phac-nml/staramr/blob/development/LICENSE) |\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amr in tags",
        "id": "470",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/470?version=1",
        "name": "Workflow 4: Staramr",
        "number_of_steps": 10,
        "projects": [
            "Seq4AMR"
        ],
        "source": "WorkflowHub",
        "tags": [
            "10.3390/microorganisms10020292",
            "amr",
            "amr-detection",
            "bioinformatics",
            "antimicrobial resistance"
        ],
        "tools": [
            "hamronize_tool",
            "shovill",
            "abricate",
            "tp_find_and_replace",
            "hamronize_summarize",
            "collapse_dataset",
            "staramr_search"
        ],
        "type": "Galaxy",
        "update_time": "2024-09-09",
        "versions": 1
    },
    {
        "create_time": "2024-08-23",
        "creators": [
            "Mahesh Binzer-Panchal",
            "Martin Pippel"
        ],
        "description": "# Swedish Earth Biogenome Project - Genome Assembly Workflow\r\n\r\nThe primary genome assembly workflow for the Earth Biogenome Project at NBIS.\r\n\r\n## Workflow overview\r\n\r\nGeneral aim:\r\n\r\n```mermaid\r\nflowchart LR\r\n    hifi[/ HiFi reads /] --> data_inspection\r\n    ont[/ ONT reads /] -->  data_inspection\r\n    hic[/ Hi-C reads /] --> data_inspection\r\n    data_inspection[[ Data inspection ]] --> preprocessing\r\n    preprocessing[[ Preprocessing ]] --> assemble\r\n    assemble[[ Assemble ]] --> validation\r\n    validation[[ Assembly validation ]] --> curation\r\n    curation[[ Assembly curation ]] --> validation\r\n```\r\n\r\nCurrent implementation:\r\n\r\n```mermaid\r\nflowchart TD\r\n    input[/ Input file/] --> hifi\r\n    input --> hic\r\n    input --> taxonkit[[ TaxonKit name2taxid/reformat ]]\r\n    taxonkit --> goat_taxon[[ GOAT taxon search ]]\r\n    goat_taxon --> busco\r\n    goat_taxon --> dtol[[ DToL lookup ]]\r\n    hifi --> samtools_fa[[ Samtools fasta ]]\r\n    samtools_fa --> fastk_hifi\r\n    samtools_fa --> mash_screen\r\n    hifi[/ HiFi reads /] --> fastk_hifi[[ FastK - HiFi ]]\r\n    hifi --> meryl_hifi[[ Meryl - HiFi ]]\r\n    hic[/ Hi-C reads /] --> fastk_hic[[ FastK - Hi-C ]]\r\n    hifi --> meryl_hic[[ Meryl - Hi-C ]]\r\n    assembly[/ Assembly /] --> quast[[ Quast ]]\r\n    fastk_hifi --> histex[[ Histex ]]\r\n    histex --> genescopefk[[ GeneScopeFK ]]\r\n    fastk_hifi --> ploidyplot[[ PloidyPlot ]]\r\n    fastk_hifi --> katgc[[ KatGC ]]\r\n    fastk_hifi --> merquryfk[[ MerquryFK ]]\r\n    assembly --> merquryfk\r\n    meryl_hifi --> merqury[[ Merqury ]]\r\n    assembly --> merqury\r\n    fastk_hifi --> katcomp[[ KatComp ]]\r\n    fastk_hic --> katcomp\r\n    assembly --> busco[[ Busco ]]\r\n    refseq_sketch[( RefSeq sketch )] --> mash_screen[[ Mash Screen ]]\r\n    hifi --> mash_screen\r\n    fastk_hifi --> hifiasm[[ HiFiasm ]]\r\n    hifiasm --> assembly\r\n    assembly --> purgedups[[ Purgedups ]]\r\n    input --> mitoref[[ Mitohifi - Find reference ]]\r\n    assembly --> mitohifi[[ Mitohifi ]]\r\n    assembly --> fcsgx[[ FCS GX ]]\r\n    fcs_fetchdb[( FCS fetchdb )] --> fcsgx\r\n    mitoref --> mitohifi\r\n    genescopefk --> quarto[[ Quarto ]]\r\n    goat_taxon --> multiqc[[ MultiQC ]]\r\n    quarto --> multiqc\r\n    dtol --> multiqc\r\n    katgc --> multiqc\r\n    ploidyplot --> multiqc\r\n    busco --> multiqc\r\n    quast --> multiqc\r\n```\r\n\r\n## Usage\r\n\r\n```bash\r\nnextflow run -params-file <params.yml> \\\r\n    [ -c <custom.config> ] \\\r\n    [ -profile <profile> ] \\\r\n    NBISweden/Earth-Biogenome-Project-pilot\r\n```\r\n\r\nwhere:\r\n- `params.yml` is a YAML formatted file containing workflow parameters\r\n    such as input paths to the assembly specification, and settings for tools within the workflow.\r\n\r\n    Example:\r\n\r\n    ```yml\r\n    input: 'assembly_spec.yml'\r\n    outdir: results\r\n    fastk: # Optional\r\n      kmer_size: 31 # default 31\r\n    genescopefk: # Optional\r\n      kmer_size: 31 # default 31\r\n    hifiasm: # Optional, default = no extra options: Key (e.g. 'opts01') is used in assembly build name (e.g., 'hifiasm-raw-opts01').\r\n      opts01: \"--opts A\"\r\n      opts02: \"--opts B\"\r\n    busco: # Optional, default: retrieved from GOAT\r\n      lineages: 'auto' # comma separated string of lineages or auto.\r\n    ```\r\n\r\n    Alternatively parameters can be provided on the\r\n    command-line using the `--parameter` notation (e.g., `--input <path>` ).\r\n- `<custom.config>` is a Nextflow configuration file which provides\r\n    additional configuration. This is used to customise settings other than\r\n    workflow parameters, such as cpus, time, and command-line options to tools.\r\n\r\n    Example:\r\n    ```nextflow\r\n    process {\r\n        withName: 'BUSCO' {  // Selects the process to apply settings.\r\n            cpus     = 6     // Overrides cpu settings defined in nextflow.config\r\n            time     = 4.d   // Overrides time settings defined in nextflow.config to 4 days. Use .h for hours, .m for minutes.\r\n            memory   = '20GB'  // Overrides memory settings defined in nextflow.config to 20 GB.\r\n            // ext.args supplies command-line options to the process tool\r\n            // overrides settings found in configs/modules.config\r\n            ext.args = '--long'  // Supplies these as command-line options to Busco\r\n        }\r\n    }\r\n    ```\r\n- `<profile>` is one of the preconfigured execution profiles\r\n    (`uppmax`, `singularity_local`, `docker_local`, etc: see nextflow.config). Alternatively,\r\n    you can provide a custom configuration to configure this workflow\r\n    to your execution environment. See [Nextflow Configuration](https://www.nextflow.io/docs/latest/config.html#scope-executor)\r\n    for more details.\r\n\r\n\r\n### Workflow parameter inputs\r\n\r\nMandatory:\r\n\r\n- `input`: A YAML formatted input file.\r\n    Example `assembly_spec.yml` (See also [test profile input](assets/test_hsapiens.yml) TODO:: Update test profile):\r\n\r\n    ```yml\r\n    sample:                          # Required: Meta data\r\n      name: 'Laetiporus sulphureus'  # Required: Species name. Correct spelling is important to look up species information.\r\n      ploidy: 2                      # Optional: Estimated ploidy (default: retrieved from GOAT)\r\n      genome_size: 2345              # Optional: Estimated genome size (default: retrieved from GOAT)\r\n      haploid_number: 13             # Optional: Estimated haploid chromosome count (default: retrieved from GOAT)\r\n      taxid: 5630                    # Optional: Taxon ID (default: retrieved with Taxonkit)\r\n      kingdom: Eukaryota             #\u00a0Optional: (default: retrived with Taxonkit)\r\n    assembly:                        # Optional: List of assemblies to curate and validate.\r\n      - assembler: hifiasm           # For each entry, the assembler,\r\n        stage: raw                   # stage of assembly,\r\n        id: uuid                     # unique id,\r\n        pri_fasta: /path/to/primary_asm.fasta # and paths to sequences are required.\r\n        alt_fasta: /path/to/alternate_asm.fasta\r\n        pri_gfa: /path/to/primary_asm.gfa\r\n        alt_gfa: /path/to/alternate_asm.gfa\r\n      - assembler: ipa\r\n        stage: raw\r\n        id: uuid\r\n        pri_fasta: /path/to/primary_asm.fasta\r\n        alt_fasta: /path/to/alternate_asm.fasta\r\n    hic:                             # Optional: List of hi-c reads to QC and use for scaffolding\r\n      - read1: '/path/to/raw/data/hic/LS_HIC_R001_1.fastq.gz'\r\n        read2: '/path/to/raw/data/hic/LS_HIC_R001_2.fastq.gz'\r\n    hifi:                            # Required: List of hifi-reads to QC and use for assembly/validation\r\n      - reads: '/path/to/raw/data/hifi/LS_HIFI_R001.bam'\r\n    rnaseq:                          # Optional: List of Rna-seq reads to use for validation\r\n      - read1: '/path/to/raw/data/rnaseq/LS_RNASEQ_R001_1.fastq.gz'\r\n        read2: '/path/to/raw/data/rnaseq/LS_RNASEQ_R001_2.fastq.gz'\r\n    isoseq:                          # Optional: List of Isoseq reads to use for validation\r\n      - reads: '/path/to/raw/data/isoseq/LS_ISOSEQ_R001.bam'\r\n    ```\r\n\r\n\r\nOptional:\r\n\r\n- `outdir`: The publishing path for results (default: `results`).\r\n- `publish_mode`: (values: `'symlink'` (default), `'copy'`) The file\r\npublishing method from the intermediate results folders\r\n(see [Table of publish modes](https://www.nextflow.io/docs/latest/process.html#publishdir)).\r\n- `steps`: The workflow steps to execute (default is all steps). Choose from:\r\n\r\n    - `inspect`: 01 - Read inspection\r\n    - `preprocess`: 02 - Read preprocessing\r\n    - `assemble`: 03 - Assembly\r\n    - `purge`: 04 - Duplicate purging\r\n    - `polish`: 05 - Error polishing\r\n    - `screen`: 06 - Contamination screening\r\n    - `scaffold`: 07 - Scaffolding\r\n    - `curate`: 08 - Rapid curation\r\n    - `alignRNA`: 09 - Align RNAseq data\r\n\r\nSoftware specific:\r\n\r\nTool specific settings are provided by supplying values to specific keys or supplying an array of\r\nsettings under a tool name. The input to `-params-file` would look like this:\r\n\r\n```yml\r\ninput: assembly.yml\r\noutdir: results\r\nfastk:\r\n  kmer_size: 31\r\ngenescopefk:\r\n  kmer_size: 31\r\nhifiasm:\r\n  opts01: \"--opts A\"\r\n  opts02: \"--opts B\"\r\nbusco:\r\n  lineages: 'auto'\r\n```\r\n\r\n- `multiqc_config`: Path to MultiQC configuration file (default: `configs/multiqc_conf.yaml`).\r\n\r\nUppmax and PDC cluster specific:\r\n\r\n- `project`: NAISS Compute allocation number.\r\n\r\n### Workflow outputs\r\n\r\nAll results are published to the path assigned to the workflow parameter `results`.\r\n\r\nTODO:: List folder contents in results file\r\n### Customization for Uppmax\r\n\r\nA custom profile named `uppmax` is available to run this workflow specifically\r\non UPPMAX clusters. The process `executor` is `slurm` so jobs are\r\nsubmitted to the Slurm Queue Manager. All jobs submitted to slurm\r\nmust have a project allocation. This is automatically added to the `clusterOptions`\r\nin the `uppmax` profile. All Uppmax clusters have node local disk space to do\r\ncomputations, and prevent heavy input/output over the network (which\r\nslows down the cluster for all).\r\nThe path to this disk space is provided by the `$SNIC_TMP` variable, used by\r\nthe `process.scratch` directive in the `uppmax` profile. Lastly\r\nthe profile enables the use of Singularity so that all processes must be\r\nexecuted within Singularity containers. See [nextflow.config](nextflow.config)\r\nfor the profile specification.\r\n\r\nThe profile is enabled using the `-profile` parameter to nextflow:\r\n```bash\r\nnextflow run -profile uppmax <nextflow_script>\r\n```\r\n\r\nA NAISS compute allocation should also be supplied using the `--project` parameter.\r\n\r\n### Customization for PDC\r\n\r\nA custom profile named `dardel` is available to run this workflow specifically\r\non the PDC cluster *Dardel*. The process `executor` is `slurm` so jobs are\r\nsubmitted to the Slurm Queue Manager. All jobs submitted to slurm\r\nmust have a project allocation. This is automatically added to the `clusterOptions`\r\nin the `dardel` profile. Calculations are performed in the scratch space allocated\r\nby `PDC_TMP` which is also on the lustre file system and is not node local storage.\r\nThe path to this disk space is provided by the `$PDC_TMP` variable, used by\r\nthe `process.scratch` directive in the `dardel` profile. Lastly\r\nthe profile enables the use of Singularity so that all processes must be\r\nexecuted within Singularity containers. See [nextflow.config](nextflow.config)\r\nfor the profile specification.\r\n\r\nThe profile is enabled using the `-profile` parameter to nextflow:\r\n```bash\r\nnextflow run -profile dardel <nextflow_script>\r\n```\r\n\r\nA NAISS compute allocation should also be supplied using the `--project` parameter.\r\n\r\n## Workflow organization\r\n\r\nThe workflows in this folder manage the execution of your analyses\r\nfrom beginning to end.\r\n\r\n```\r\nworkflow/\r\n | - .github/                        Github data such as actions to run\r\n | - assets/                         Workflow assets such as test samplesheets\r\n | - bin/                            Custom workflow scripts\r\n | - configs/                        Configuration files that govern workflow execution\r\n | - dockerfiles/                    Custom container definition files\r\n | - docs/                           Workflow usage and interpretation information\r\n | - modules/                        Process definitions for tools used in the workflow\r\n | - subworkflows/                   Custom workflows for different stages of the main analysis\r\n | - tests/                          Workflow tests\r\n | - main.nf                         The primary analysis script\r\n | - nextflow.config                 General Nextflow configuration\r\n \\ - modules.json                    nf-core file which tracks modules/subworkflows from nf-core\r\n```\r\n\r\n",
        "doi": null,
        "edam_operation": [
            "Contact map calculation",
            "Sequence profile generation",
            "k-mer counting"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "profil.* in description",
        "id": "1106",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/1106?version=1",
        "name": "Swedish Earth Biogenome Project Genome Assembly Workflow",
        "number_of_steps": 0,
        "projects": [
            "NBIS",
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genome_assembly"
        ],
        "tools": [
            "Hifiasm",
            "GenomeScope 2.0",
            "Merqury",
            "Meryl",
            "BUSCO",
            "QUAST",
            "purge_dups",
            "SAMtools",
            "TaxonKit",
            "MitoHiFi",
            "Mash"
        ],
        "type": "Nextflow",
        "update_time": "2024-08-26",
        "versions": 1
    },
    {
        "create_time": "2024-08-21",
        "creators": [
            "Marie Joss\u00e9"
        ],
        "description": "Secondary metabolite biosynthetic gene cluster (SMBGC) Annotation using Neural Networks Trained on Interpro Signatures ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Marine biology"
        ],
        "filtered_on": "metab.* in description",
        "id": "1105",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/1105?version=1",
        "name": "Marine Omics identifying biosynthetic gene clusters",
        "number_of_steps": 5,
        "projects": [
            "FAIR-EASE",
            "usegalaxy-eu"
        ],
        "source": "WorkflowHub",
        "tags": [
            "earth-system",
            "ecology",
            "marine omics",
            "ocean"
        ],
        "tools": [
            "Create the protein fasta file",
            "Create TSV file for Sanntis",
            "Use of Sanntis",
            "Remove useless * in the protein fasta file"
        ],
        "type": "Galaxy",
        "update_time": "2024-08-21",
        "versions": 1
    },
    {
        "create_time": "2024-08-15",
        "creators": [
            "Ann-Kathrin D\u00f6rr"
        ],
        "description": "# RiboSnake: 16S rRNA analysis workflow with QIIME2 and Snakemake\r\n\r\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22656.10-brightgreen.svg)](https://snakemake.bitbucket.io)\r\n[![Build Status](https://travis-ci.org/snakemake-workflows/16S.svg?branch=master)](https://travis-ci.org/snakemake-workflows/16S)\r\n\r\nQiime2 workflow for 16S analysis created with snakemake.\r\n\r\n## Authors\r\n\r\n* Ann-Kathrin D\u00f6rr (@AKBrueggemann)\r\n\r\n## Usage\r\n\r\nIf you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this (original) repository and, if available, its DOI (see above).\r\n\r\n### Step 1: Obtain a copy of this workflow\r\n\r\nIf you want to use the workflow, please obtain a copy of it by either:\r\n[Cloning](https://help.github.com/en/articles/cloning-a-repository) the repository to your local system, into the place where you want to perform the data analysis or\r\nDownloading a zip-file of the repository to your local machine.\r\n\r\nWhen you have the folder structure added on your local machine, please add a \"data\" folder manually.\r\n\r\n### Step 2: Configure workflow\r\n\r\nConfigure the workflow according to your needs via editing the files in the `config/` folder. Adjust `config.yaml` to configure the workflow execution, and `metadata.txt` to specify your sample setup.\r\n\r\nSome important parameters you should check and set according to your own FASTQ-files in the `config.yaml` are primers for the forward and reverse reads, the `datatype`, that should be used by QIIME2 and the `min-seq-length`. Based on the sequencing, the length of the reads can vary.\r\n\r\nThe default parameters for filtering and truncation were validated with the help of a MOCK community and fitted to retrieve all bacteria from that community.\r\n\r\nIn addition to that, you need to fit the metadata-parameters to your data. Please change the names of the used metadata-columns according to your information.\r\nTake special care of the \"remove-columns\" information. Here you can add the columns you don't want to have analyzed or the workflow can't anlyse. This can happen when\r\nall of the values in one column are unique or all the same. You should also look out for the information under \"metadata-parameters\" and \"songbird\" as well as \"ancom\".\r\nIn every case you have to specify the column names based on your own data.\r\n\r\nIf your metadata is not containing numeric values, please use the \"reduced-analysis\" option in the config file to run the workflow, as the workflow is currently not able to run only on categorical metadata for the full analysis version. We are going to fix that in the future.\r\n\r\nThe workflow is able to perform clustering and denoising either with vsearch, leading to OTU creation, or with DADA2, creating ASVs. You can decide which modus to use by setting the variable \"DADA2\" to `True` (DADA2 usage) or `False` (vsearch).\r\n\r\nPlease make sure, that the names of your FASTQ files are correctly formatted. They should look like this:\r\n\r\n    samplename_SNumber_Lane_R1/R2_001.fastq.gz\r\n\r\nIn the config file you can also set the input and output directory. You can either create a specific directory for your input data and then put that filepath in the config file, or you can put the path to an existing directory where the data is located.\r\nThe data will then be copied to the workflow's data directory. The compressed and final file holding the results will be copied to the directory you specified in \"output\". It will also stay in the local \"results\" folder together with important intermediate results.\r\nThe \"data\" folder is also not provided by the repository. It is the folder the fastq files are copied to before being used in the workflow. It is best if you create the folder inside the workflows folder structure. It must definitely be created on the machine, the workflow is running on.\r\n\r\n### Step 3: Install Snakemake\r\n\r\nCreate a snakemake environment using [mamba](https://mamba.readthedocs.io/en/latest/) via:\r\n\r\n    mamba create -c conda-forge -c bioconda -n snakemake snakemake\r\n\r\nFor installation details, see the [instructions in the Snakemake documentation](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html).\r\n\r\n### Step 4: Execute workflow\r\n\r\nActivate the conda environment:\r\n\r\n    conda activate snakemake\r\n\r\nFill up the `metadata.txt` with the information of your samples:\r\n\r\n    Please be careful to not include spaces between the commas. If there is a column, that you don't have any information about, please leave it empty and simply go on with the next column.\r\n\r\nTest your configuration by performing a dry-run via\r\n\r\n    snakemake --use-conda -n\r\n\r\nExecuting the workflow takes two steps:\r\n\r\n    Data preparation: snakemake --cores $N --use-conda data_prep\r\n    Workflow execution: snakemake --cores $N --use-conda\r\n\r\nusing `$N` cores.\r\n\r\nWhen running on snakemake > 8.0 we recommend setting the --shared-fs-usage none as well as setting the environment variable TEMP to a local directory to prevent problems with the usage of the fs-storage system.\r\nThe environment variable can be set like this:\r\n\r\n    conda activate your_environment_name\r\n    export TEMP=/path/to/local/tmp\r\n\r\nThen run the snakemake command like above with the addition of the storage flag:\r\n\r\n    snakemake --cores $N --use-conda --shared-fs-usage none\r\n\r\n### Step 5: Investigate results\r\n\r\nAfter successful execution, the workflow provides you with a compressed folder, holding all interesting results ready to decompress or to download to your local machine.\r\nThe compressed file 16S-report.tar.gz holds several qiime2-artifacts that can be inspected via qiime-view. In the zipped folder report.zip is the snakemake html\r\nreport holding graphics as well as the DAG of the executed jobs and html files leading you directly to the qiime2-results, without the need of using qiime-view.\r\n\r\nThis report can, e.g., be forwarded to your collaborators.\r\n\r\n### Step 6: Obtain updates from upstream\r\n\r\nWhenever you want to synchronize your workflow copy with new developments from upstream, do the following.\r\n\r\n1. Once, register the upstream repository in your local copy: `git remote add -f upstream git@github.com:snakemake-workflows/16S.git` or `git remote add -f upstream https://github.com/snakemake-workflows/16S.git` if you do not have setup ssh keys.\r\n2. Update the upstream version: `git fetch upstream`.\r\n3. Create a diff with the current version: `git diff HEAD upstream/master workflow > upstream-changes.diff`.\r\n4. Investigate the changes: `vim upstream-changes.diff`.\r\n5. Apply the modified diff via: `git apply upstream-changes.diff`.\r\n6. Carefully check whether you need to update the config files: `git diff HEAD upstream/master config`. If so, do it manually, and only where necessary, since you would otherwise likely overwrite your settings and samples.\r\n\r\n## Contribute back\r\n\r\nIn case you have also changed or added steps, please consider contributing them back to the original repository:\r\n\r\n### Step 1: Forking the repository\r\n\r\n[Fork](https://help.github.com/en/articles/fork-a-repo) the original repo to a personal or lab account.\r\n\r\n### Step 2: Cloning\r\n\r\n[Clone](https://help.github.com/en/articles/cloning-a-repository) the fork to your local system, to a different place than where you ran your analysis.\r\n\r\n### Step 3: Add changes\r\n\r\n1. Copy the modified files from your analysis to the clone of your fork, e.g., `cp -r workflow path/to/fork`. Make sure to **not** accidentally copy config file contents or sample sheets. Instead, manually update the example config files if necessary.\r\n2. Commit and push your changes to your fork.\r\n3. Create a [pull request](https://help.github.com/en/articles/creating-a-pull-request) against the original repository.\r\n4. If you want to add your config file and the parameters as a new default parameter sets, please do this by opening a pull request adding the file to the \"contributions\" folder.\r\n\r\n## Testing\r\n\r\nTest cases are in the subfolder `.test`. They are automatically executed via continuous integration with [Github Actions](https://github.com/features/actions).\r\nIf you want to test the RiboSnake functions yourself, you can use the same data used for the CI/CD tests. The used fastq files can be downloaded [here](https://data.qiime2.org/2022.2/tutorials/importing/casava-18-paired-end-demultiplexed.zip). They have been published by Neilson et al., mSystems, 2017.\r\n\r\n### Example\r\n\r\n1. First clone teh repository to your local machine as described above.\r\n2. Download a dataset of your liking, or the data used for testing the pipeline. The FASTQ files can be downloaded with:\r\n    curl -sL \\\r\n          \"https://data.qiime2.org/2022.2/tutorials/importing/casava-18-paired-end-demultiplexed.zip\"\r\n3. Unzip the data into a folder of your liking, it can be called \"incoming\" but it does not have to be.\r\nIf you name your folder differently, please change the \"input\" path in the config file.\r\n4. If you don't want to use the whole dataset for testing, remove some of the FASTQ files from the folder:\r\n    rm PAP*\r\n    rm YUN*\r\n    rm Rep*\r\n    rm blank*\r\n5. Use the information that can be found in [this](https://data.qiime2.org/2024.5/tutorials/atacama-soils/sample_metadata.tsv) file from the Qiime2 tutorial, to fill out your metadata.txt file for the samples starting with \"BAQ\".\r\n6. The default-parameters to be used in the config file can be found in the provided file \"PowerSoil-Illumina-soil.yaml\" in the config folder.\r\n7. With these parameters and the previous steps, you should be able to execute the workflow.\r\n\r\n## Tools\r\n\r\nA list of the tools used in this pipeline:\r\n\r\n| Tool         | Link                                              |\r\n|--------------|---------------------------------------------------|\r\n| QIIME2       | www.doi.org/10.1038/s41587-019-0209-9             |\r\n| Snakemake    | www.doi.org/10.12688/f1000research.29032.1        |\r\n| FastQC       | www.bioinformatics.babraham.ac.uk/projects/fastqc |\r\n| MultiQC      | www.doi.org/10.1093/bioinformatics/btw354         |\r\n| pandas       | pandas.pydata.org                                 |\r\n| kraken2      | www.doi.org/10.1186/s13059-019-1891-0             |\r\n| vsearch      | www.github.com/torognes/vsearch                   |\r\n| DADA2        | www.doi.org/10.1038/nmeth.3869                    |\r\n| songbird     | www.doi.org/10.1038/s41467-019-10656-5            |\r\n| bowtie2      | www.doi.org/10.1038/nmeth.1923                    |\r\n| Ancom        | www.doi.org/10.3402/mehd.v26.27663                |\r\n| cutadapt     | www.doi.org/10.14806/ej.17.1.200                  |\r\n| BLAST        | www.doi.org/10.1016/S0022-2836(05)80360-2         |\r\n| gneiss       | www.doi.org/10.1128/mSystems.00162-16             |\r\n| qurro        | www.doi.org/10.1093/nargab/lqaa023                |\r\n| Rescript     | www.doi.org/10.1371/journal.pcbi.1009581          |",
        "doi": "10.48546/workflowhub.workflow.1102.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "16S in name",
        "id": "1102",
        "keep": "Keep",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1102?version=1",
        "name": "RiboSnake: 16S rRNA analysis workflow with QIIME2 and Snakemake",
        "number_of_steps": 0,
        "projects": [
            "16S rRNA Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-08-15",
        "versions": 1
    },
    {
        "create_time": "2024-06-26",
        "creators": [
            "B\u00e9r\u00e9nice Batut",
            "Engy Nasr",
            "Paul Zierep"
        ],
        "description": "Microbiome - QC and Contamination Filtering",
        "doi": "10.48546/workflowhub.workflow.1061.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "microbiom.* in description",
        "id": "1061",
        "keep": "Keep",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1061?version=1",
        "name": "nanopore-pre-processing/main",
        "number_of_steps": 25,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "nanoplot",
            "fastp",
            "Add_a_column1",
            "regexColumn1",
            "krakentools_extract_kraken_reads",
            "fastqc",
            "minimap2",
            "porechop",
            "Cut1",
            "collection_column_join",
            "samtools_fastx",
            "kraken2",
            "collapse_dataset",
            "__FILTER_FAILED_DATASETS__",
            "bamtools_split_mapped",
            "Grep1",
            "multiqc"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2024-06-26",
        "creators": [
            "Engy Nasr",
            "B\u00e9r\u00e9nice Batut",
            "Paul Zierep"
        ],
        "description": "Microbiome - Taxonomy Profiling",
        "doi": "10.48546/workflowhub.workflow.1059.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in name",
        "id": "1059",
        "keep": "Keep",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1059?version=1",
        "name": "taxonomy-profiling-and-visualization-with-krona/main",
        "number_of_steps": 3,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "taxonomy_krona_chart",
            "kraken2",
            "krakentools_kreport2krona"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 1
    },
    {
        "create_time": "2024-06-25",
        "creators": [
            "Arnau Soler Costa",
            "Amy Curwin",
            "Jordi Rambla"
        ],
        "description": "# ![IMPaCT program](https://github.com/EGA-archive/sarek-IMPaCT-data-QC/blob/master/impact_qc/docs/png/impact_data_logo_pink_horitzontal.png)\r\n\r\n[![IMPaCT](https://img.shields.io/badge/Web%20-IMPaCT-blue)](https://impact.isciii.es/)\r\n[![IMPaCT-isciii](https://img.shields.io/badge/Web%20-IMPaCT--isciii-red)](https://www.isciii.es/QueHacemos/Financiacion/IMPaCT/Paginas/default.aspx)\r\n[![IMPaCT-Data](https://img.shields.io/badge/Web%20-IMPaCT--Data-1d355c.svg?labelColor=000000)](https://impact-data.bsc.es/)\r\n\r\n## Introduction of the project\r\n\r\nIMPaCT-Data is the IMPaCT program that aims to support the development of a common, interoperable and integrated system for the collection and analysis of clinical and molecular data by providing the knowledge and resources available in the Spanish Science and Technology System. This development will make it possible to answer research questions based on the different clinical and molecular information systems available. Fundamentally, it aims to provide researchers with a population perspective based on individual data.\r\n\r\nThe IMPaCT-Data project is divided into different work packages (WP). In the context of IMPaCT-Data WP3 (Genomics), a working group of experts worked on the generation of a specific quality control (QC) workflow for germline exome samples.\r\n\r\nTo achieve this, a set of metrics related to human genomic data was decided upon, and the toolset or software to extract these metrics was implemented in an existing variant calling workflow called Sarek, part of the nf-core community. The final outcome is a Nextflow subworkflow, called IMPaCT-QC implemented in the Sarek pipeline.\r\n\r\nBelow you can find the explanation of this workflow (raw pipeline), the link to the documentation of the IMPaCT QC subworkflow and a linked documentation associated to the QC metrics added in the mentioned workflow.\r\n\r\n- [IMPaCT-data subworkflow documentation](https://github.com/EGA-archive/sarek-IMPaCT-data-QC/tree/master/impact_qc)\r\n\r\n- [Metrics documentation](https://github.com/EGA-archive/sarek-IMPaCT-data-QC/blob/master/impact_qc/docs/QC_Sarek_supporing_documentation.pdf)\r\n\r\n<h1>\r\n  <picture>\r\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-sarek_logo_dark.png\">\r\n    <img alt=\"nf-core/sarek\" src=\"docs/images/nf-core-sarek_logo_light.png\">\r\n  </picture>\r\n</h1>\r\n\r\n[![GitHub Actions CI Status](https://github.com/nf-core/sarek/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/sarek/actions/workflows/ci.yml)\r\n[![GitHub Actions Linting Status](https://github.com/nf-core/sarek/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/sarek/actions/workflows/linting.yml)\r\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/sarek/results)\r\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\r\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.3476425-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.3476425)\r\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/nf-core/sarek)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23sarek-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/sarek)\r\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)\r\n[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)\r\n[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\r\n\r\n## Introduction\r\n\r\n**nf-core/sarek** is a workflow designed to detect variants on whole genome or targeted sequencing data. Initially designed for Human, and Mouse, it can work on any species with a reference genome. Sarek can also handle tumour / normal pairs and could include additional relapses.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/sarek/results).\r\n\r\nIt's listed on [Elixir - Tools and Data Services Registry](https://bio.tools/nf-core-sarek) and [Dockstore](https://dockstore.org/workflows/github.com/nf-core/sarek).\r\n\r\n<p align=\"center\">\r\n    <img title=\"Sarek Workflow\" src=\"docs/images/sarek_workflow.png\" width=30%>\r\n</p>\r\n\r\n## Pipeline summary\r\n\r\nDepending on the options and samples provided, the pipeline can currently perform the following:\r\n\r\n- Form consensus reads from UMI sequences (`fgbio`)\r\n- Sequencing quality control and trimming (enabled by `--trim_fastq`) (`FastQC`, `fastp`)\r\n- Map Reads to Reference (`BWA-mem`, `BWA-mem2`, `dragmap` or `Sentieon BWA-mem`)\r\n- Process BAM file (`GATK MarkDuplicates`, `GATK BaseRecalibrator` and `GATK ApplyBQSR` or `Sentieon LocusCollector` and `Sentieon Dedup`)\r\n- Summarise alignment statistics (`samtools stats`, `mosdepth`)\r\n- Variant calling (enabled by `--tools`, see [compatibility](https://nf-co.re/sarek/latest/docs/usage#which-variant-calling-tool-is-implemented-for-which-data-type)):\r\n  - `ASCAT`\r\n  - `CNVkit`\r\n  - `Control-FREEC`\r\n  - `DeepVariant`\r\n  - `freebayes`\r\n  - `GATK HaplotypeCaller`\r\n  - `Manta`\r\n  - `mpileup`\r\n  - `MSIsensor-pro`\r\n  - `Mutect2`\r\n  - `Sentieon Haplotyper`\r\n  - `Strelka2`\r\n  - `TIDDIT`\r\n- Variant filtering and annotation (`SnpEff`, `Ensembl VEP`, `BCFtools annotate`)\r\n- Summarise and represent QC (`MultiQC`)\r\n\r\n<p align=\"center\">\r\n    <img title=\"Sarek Workflow\" src=\"docs/images/sarek_subway.png\" width=60%>\r\n</p>\r\n\r\n## Usage\r\n\r\n> [!NOTE]\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\r\n\r\nFirst, prepare a samplesheet with your input data that looks as follows:\r\n\r\n`samplesheet.csv`:\r\n\r\n```csv\r\npatient,sample,lane,fastq_1,fastq_2\r\nID1,S1,L002,ID1_S1_L002_R1_001.fastq.gz,ID1_S1_L002_R2_001.fastq.gz\r\n```\r\n\r\nEach row represents a pair of fastq files (paired end).\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\nnextflow run nf-core/sarek \\\r\n   -profile <docker/singularity/.../institute> \\\r\n   --input samplesheet.csv \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\n> [!WARNING]\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/sarek/usage) and the [parameter documentation](https://nf-co.re/sarek/parameters).\r\n\r\n## Pipeline output\r\n\r\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/sarek/results) tab on the nf-core website pipeline page.\r\nFor more details about the output files and reports, please refer to the\r\n[output documentation](https://nf-co.re/sarek/output).\r\n\r\n## Benchmarking\r\n\r\nOn each release, the pipeline is run on 3 full size tests:\r\n\r\n- `test_full` runs tumor-normal data for one patient from the SEQ2C consortium\r\n- `test_full_germline` runs a WGS 30X Genome-in-a-Bottle(NA12878) dataset\r\n- `test_full_germline_ncbench_agilent` runs two WES samples with 75M and 200M reads (data available [here](https://github.com/ncbench/ncbench-workflow#contributing-callsets)). The results are uploaded to Zenodo, evaluated against a truth dataset, and results are made available via the [NCBench dashboard](https://ncbench.github.io/report/report.html#).\r\n\r\n## Credits\r\n\r\nSarek was originally written by Maxime U Garcia and Szilveszter Juhos at the [National Genomics Infastructure](https://ngisweden.scilifelab.se) and [National Bioinformatics Infastructure Sweden](https://nbis.se) which are both platforms at [SciLifeLab](https://scilifelab.se), with the support of [The Swedish Childhood Tumor Biobank (Barntum\u00f6rbanken)](https://ki.se/forskning/barntumorbanken).\r\nFriederike Hanssen and Gisela Gabernet at [QBiC](https://www.qbic.uni-tuebingen.de/) later joined and helped with further development.\r\n\r\nThe Nextflow DSL2 conversion of the pipeline was lead by Friederike Hanssen and Maxime U Garcia.\r\n\r\nMaintenance is now lead by Friederike Hanssen and Maxime U Garcia (now at [Seqera Labs](https://seqera/io))\r\n\r\nMain developers:\r\n\r\n- [Maxime U Garcia](https://github.com/maxulysse)\r\n- [Friederike Hanssen](https://github.com/FriederikeHanssen)\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- [Abhinav Sharma](https://github.com/abhi18av)\r\n- [Adam Talbot](https://github.com/adamrtalbot)\r\n- [Adrian L\u00e4rkeryd](https://github.com/adrlar)\r\n- [Alexander Peltzer](https://github.com/apeltzer)\r\n- [Alison Meynert](https://github.com/ameynert)\r\n- [Anders Sune Pedersen](https://github.com/asp8200)\r\n- [arontommi](https://github.com/arontommi)\r\n- [BarryDigby](https://github.com/BarryDigby)\r\n- [Bekir Erg\u00fcner](https://github.com/berguner)\r\n- [bjornnystedt](https://github.com/bjornnystedt)\r\n- [cgpu](https://github.com/cgpu)\r\n- [Chela James](https://github.com/chelauk)\r\n- [David Mas-Ponte](https://github.com/davidmasp)\r\n- [Edmund Miller](https://github.com/edmundmiller)\r\n- [Francesco Lescai](https://github.com/lescai)\r\n- [Gavin Mackenzie](https://github.com/GCJMackenzie)\r\n- [Gisela Gabernet](https://github.com/ggabernet)\r\n- [Grant Neilson](https://github.com/grantn5)\r\n- [gulfshores](https://github.com/gulfshores)\r\n- [Harshil Patel](https://github.com/drpatelh)\r\n- [James A. Fellows Yates](https://github.com/jfy133)\r\n- [Jesper Eisfeldt](https://github.com/J35P312)\r\n- [Johannes Alneberg](https://github.com/alneberg)\r\n- [Jos\u00e9 Fern\u00e1ndez Navarro](https://github.com/jfnavarro)\r\n- [J\u00falia Mir Pedrol](https://github.com/mirpedrol)\r\n- [Ken Brewer](https://github.com/kenibrewer)\r\n- [Lasse Westergaard Folkersen](https://github.com/lassefolkersen)\r\n- [Lucia Conde](https://github.com/lconde-ucl)\r\n- [Malin Larsson](https://github.com/malinlarsson)\r\n- [Marcel Martin](https://github.com/marcelm)\r\n- [Nick Smith](https://github.com/nickhsmith)\r\n- [Nicolas Schcolnicov](https://github.com/nschcolnicov)\r\n- [Nilesh Tawari](https://github.com/nilesh-tawari)\r\n- [Nils Homer](https://github.com/nh13)\r\n- [Olga Botvinnik](https://github.com/olgabot)\r\n- [Oskar Wacker](https://github.com/WackerO)\r\n- [pallolason](https://github.com/pallolason)\r\n- [Paul Cantalupo](https://github.com/pcantalupo)\r\n- [Phil Ewels](https://github.com/ewels)\r\n- [Sabrina Krakau](https://github.com/skrakau)\r\n- [Sam Minot](https://github.com/sminot)\r\n- [Sebastian-D](https://github.com/Sebastian-D)\r\n- [Silvia Morini](https://github.com/silviamorins)\r\n- [Simon Pearce](https://github.com/SPPearce)\r\n- [Solenne Correard](https://github.com/scorreard)\r\n- [Susanne Jodoin](https://github.com/SusiJo)\r\n- [Szilveszter Juhos](https://github.com/szilvajuhos)\r\n- [Tobias Koch](https://github.com/KochTobi)\r\n- [Winni Kretzschmar](https://github.com/winni2k)\r\n\r\n## Acknowledgements\r\n\r\n|      [![Barntum\u00f6rbanken](docs/images/BTB_logo.png)](https://ki.se/forskning/barntumorbanken)      |            [![SciLifeLab](docs/images/SciLifeLab_logo.png)](https://scilifelab.se)             |\r\n| :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |\r\n| [![National Genomics Infrastructure](docs/images/NGI_logo.png)](https://ngisweden.scilifelab.se/) | [![National Bioinformatics Infrastructure Sweden](docs/images/NBIS_logo.png)](https://nbis.se) |\r\n|              [![QBiC](docs/images/QBiC_logo.png)](https://www.qbic.uni-tuebingen.de)              |                   [![GHGA](docs/images/GHGA_logo.png)](https://www.ghga.de/)                   |\r\n|                     [![DNGC](docs/images/DNGC_logo.png)](https://eng.ngc.dk/)                     |                                                                                                |\r\n\r\n## Contributions & Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#sarek` channel](https://nfcore.slack.com/channels/sarek) (you can join with [this invite](https://nf-co.re/join/slack)), or contact us: [Maxime U Garcia](mailto:maxime.garcia@seqera.io?subject=[GitHub]%20nf-core/sarek), [Friederike Hanssen](mailto:friederike.hanssen@qbic.uni-tuebingen.de?subject=[GitHub]%20nf-core/sarek)\r\n\r\n## Citations\r\n\r\nIf you use `nf-core/sarek` for your analysis, please cite the `Sarek` article as follows:\r\n\r\n> Friederike Hanssen, Maxime U Garcia, Lasse Folkersen, Anders Sune Pedersen, Francesco Lescai, Susanne Jodoin, Edmund Miller, Oskar Wacker, Nicholas Smith, nf-core community, Gisela Gabernet, Sven Nahnsen **Scalable and efficient DNA sequencing analysis on different compute infrastructures aiding variant discovery** _NAR Genomics and Bioinformatics_ Volume 6, Issue 2, June 2024, lqae031, [doi: 10.1093/nargab/lqae031](https://doi.org/10.1093/nargab/lqae031).\r\n\r\n> Garcia M, Juhos S, Larsson M et al. **Sarek: A portable workflow for whole-genome sequencing analysis of germline and somatic variants [version 2; peer review: 2 approved]** _F1000Research_ 2020, 9:63 [doi: 10.12688/f1000research.16665.2](http://dx.doi.org/10.12688/f1000research.16665.2).\r\n\r\nYou can cite the sarek zenodo record for a specific version using the following [doi: 10.5281/zenodo.3476425](https://doi.org/10.5281/zenodo.3476425)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n\r\n## CHANGELOG\r\n\r\n- [CHANGELOG](CHANGELOG.md)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "1030",
        "keep": "Reject",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1030?version=3",
        "name": "IMPaCT-Data quality control workflow implementation in nf-core/Sarek",
        "number_of_steps": 0,
        "projects": [
            "EGA"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "ega-archive",
            "ngs",
            "nextflow",
            "wgs",
            "quality control",
            "variant calling",
            "wes"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-06-25",
        "versions": 3
    },
    {
        "create_time": "2024-06-25",
        "creators": [
            "Usman Rashid",
            "Chen Wu",
            "Jason Shiller",
            "Ken Smith",
            "Ross Crowhurst",
            "Marcus Davy",
            "Ting-Hsuan Chen",
            "Susan Thomson",
            "Cecilia Deng"
        ],
        "description": "[![GitHub Actions CI Status](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/ci.yml/badge.svg)](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/ci.yml)\r\n[![GitHub Actions Linting Status](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/linting.yml/badge.svg)](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/linting.yml)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.10647870-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.10647870)\r\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda \u274c](http://img.shields.io/badge/run%20with-conda%20\u274c-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/plant-food-research-open/assemblyqc)\r\n\r\n## Introduction\r\n\r\n**plant-food-research-open/assemblyqc** is a [NextFlow](https://www.nextflow.io/docs/latest/index.html) pipeline which evaluates assembly quality with multiple QC tools and presents the results in a unified html report. The tools are shown in the [Pipeline Flowchart](#pipeline-flowchart) and their references are listed in [CITATIONS.md](./CITATIONS.md).\r\n\r\n## Pipeline Flowchart\r\n\r\n```mermaid\r\n%%{init: {\r\n    'theme': 'base',\r\n    'themeVariables': {\r\n    'fontSize': '52px\",\r\n    'primaryColor': '#9A6421',\r\n    'primaryTextColor': '#ffffff',\r\n    'primaryBorderColor': '#9A6421',\r\n    'lineColor': '#B180A8',\r\n    'secondaryColor': '#455C58',\r\n    'tertiaryColor': '#ffffff'\r\n  }\r\n}}%%\r\nflowchart LR\r\n  forEachTag(Assembly) ==> VALIDATE_FORMAT[VALIDATE FORMAT]\r\n\r\n  VALIDATE_FORMAT ==> ncbiFCS[NCBI FCS\\nADAPTOR]\r\n  ncbiFCS ==> Check{Check}\r\n\r\n  VALIDATE_FORMAT ==> ncbiGX[NCBI FCS GX]\r\n  ncbiGX ==> Check\r\n  Check ==> |Clean|Run(Run)\r\n\r\n  Check ==> |Contamination|Skip(Skip All)\r\n  Skip ==> REPORT\r\n\r\n  VALIDATE_FORMAT ==> GFF_STATS[GENOMETOOLS GT STAT]\r\n\r\n  Run ==> ASS_STATS[ASSEMBLATHON STATS]\r\n  Run ==> BUSCO\r\n  Run ==> TIDK\r\n  Run ==> LAI\r\n  Run ==> KRAKEN2\r\n  Run ==> HIC_CONTACT_MAP[HIC CONTACT MAP]\r\n  Run ==> MUMMER\r\n  Run ==> MINIMAP2\r\n  Run ==> MERQURY\r\n\r\n  MUMMER ==> CIRCOS\r\n  MUMMER ==> DOTPLOT\r\n\r\n  MINIMAP2 ==> PLOTSR\r\n\r\n  ASS_STATS ==> REPORT\r\n  GFF_STATS ==> REPORT\r\n  BUSCO ==> REPORT\r\n  TIDK ==> REPORT\r\n  LAI ==> REPORT\r\n  KRAKEN2 ==> REPORT\r\n  HIC_CONTACT_MAP ==> REPORT\r\n  CIRCOS ==> REPORT\r\n  DOTPLOT ==> REPORT\r\n  PLOTSR ==> REPORT\r\n  MERQURY ==> REPORT\r\n```\r\n\r\n- [FASTA VALIDATOR](https://github.com/linsalrob/fasta_validator) + [SEQKIT RMDUP](https://github.com/shenwei356/seqkit): FASTA validation\r\n- [GENOMETOOLS GT GFF3VALIDATOR](https://genometools.org/tools/gt_gff3validator.html): GFF3 validation\r\n- [ASSEMBLATHON STATS](https://github.com/PlantandFoodResearch/assemblathon2-analysis/blob/a93cba25d847434f7eadc04e63b58c567c46a56d/assemblathon_stats.pl): Assembly statistics\r\n- [GENOMETOOLS GT STAT](https://genometools.org/tools/gt_stat.html): Annotation statistics\r\n- [NCBI FCS ADAPTOR](https://github.com/ncbi/fcs): Adaptor contamination pass/fail\r\n- [NCBI FCS GX](https://github.com/ncbi/fcs): Foreign organism contamination pass/fail\r\n- [BUSCO](https://gitlab.com/ezlab/busco): Gene-space completeness estimation\r\n- [TIDK](https://github.com/tolkit/telomeric-identifier): Telomere repeat identification\r\n- [LAI](https://github.com/oushujun/LTR_retriever/blob/master/LAI): Continuity of repetitive sequences\r\n- [KRAKEN2](https://github.com/DerrickWood/kraken2): Taxonomy classification\r\n- [HIC CONTACT MAP](https://github.com/igvteam/juicebox.js): Alignment and visualisation of HiC data\r\n- [MUMMER](https://github.com/mummer4/mummer) \u2192 [CIRCOS](http://circos.ca/documentation/) + [DOTPLOT](https://plotly.com) & [MINIMAP2](https://github.com/lh3/minimap2) \u2192 [PLOTSR](https://github.com/schneebergerlab/plotsr): Synteny analysis\r\n- [MERQURY](https://github.com/marbl/merqury): K-mer completeness, consensus quality and phasing assessment\r\n\r\n## Usage\r\n\r\nRefer to [usage](./docs/usage.md), [parameters](./docs/parameters.md) and [output](./docs/output.md) documents for details.\r\n\r\n> [!NOTE]\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\r\n\r\nPrepare an `assemblysheet.csv` file with following columns representing target assemblies and associated meta-data.\r\n\r\n- `tag:` A unique tag which represents the target assembly throughout the pipeline and in the final report\r\n- `fasta:` FASTA file\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\nnextflow run plant-food-research-open/assemblyqc \\\r\n   -profile <docker/singularity/.../institute> \\\r\n   --input assemblysheet.csv \\\r\n   --outdir <OUTDIR>\r\n```\r\n\r\n> [!WARNING]\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\n### Plant&Food Users\r\n\r\nDownload the pipeline to your `/workspace/$USER` folder. Change the parameters defined in the [pfr/params.json](./pfr/params.json) file. Submit the pipeline to SLURM for execution.\r\n\r\n```bash\r\nsbatch ./pfr_assemblyqc\r\n```\r\n\r\n## Credits\r\n\r\nplant-food-research-open/assemblyqc was originally written by Usman Rashid ([@gallvp](https://github.com/gallvp)) and Ken Smith ([@hzlnutspread](https://github.com/hzlnutspread)).\r\n\r\nRoss Crowhurst ([@rosscrowhurst](https://github.com/rosscrowhurst)), Chen Wu ([@christinawu2008](https://github.com/christinawu2008)) and Marcus Davy ([@mdavy86](https://github.com/mdavy86)) generously contributed their QC scripts.\r\n\r\nMahesh Binzer-Panchal ([@mahesh-panchal](https://github.com/mahesh-panchal)) helped port the pipeline modules and sub-workflows to [nf-core](https://nf-co.re) schema.\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- [Cecilia Deng](https://github.com/CeciliaDeng)\r\n- [Ignacio Carvajal](https://github.com/ignacio3437)\r\n- [Jason Shiller](https://github.com/jasonshiller)\r\n- [Sarah Bailey](https://github.com/SarahBailey1998)\r\n- [Susan Thomson](https://github.com/cflsjt)\r\n- [Ting-Hsuan Chen](https://github.com/ting-hsuan-chen)\r\n\r\nThe pipeline uses nf-core modules contributed by following authors:\r\n\r\n<a href=\"https://github.com/gallvp\"><img src=\"https://github.com/gallvp.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/drpatelh\"><img src=\"https://github.com/drpatelh.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/midnighter\"><img src=\"https://github.com/midnighter.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/mahesh-panchal\"><img src=\"https://github.com/mahesh-panchal.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/jfy133\"><img src=\"https://github.com/jfy133.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/adamrtalbot\"><img src=\"https://github.com/adamrtalbot.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/maxulysse\"><img src=\"https://github.com/maxulysse.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/matthdsm\"><img src=\"https://github.com/matthdsm.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/joseespinosa\"><img src=\"https://github.com/joseespinosa.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/ewels\"><img src=\"https://github.com/ewels.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/sofstam\"><img src=\"https://github.com/sofstam.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/sateeshperi\"><img src=\"https://github.com/sateeshperi.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/priyanka-surana\"><img src=\"https://github.com/priyanka-surana.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/phue\"><img src=\"https://github.com/phue.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/muffato\"><img src=\"https://github.com/muffato.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/lescai\"><img src=\"https://github.com/lescai.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/kevinmenden\"><img src=\"https://github.com/kevinmenden.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/jvhagey\"><img src=\"https://github.com/jvhagey.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/joon-klaps\"><img src=\"https://github.com/joon-klaps.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/jeremy1805\"><img src=\"https://github.com/jeremy1805.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/heuermh\"><img src=\"https://github.com/heuermh.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/grst\"><img src=\"https://github.com/grst.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/friederikehanssen\"><img src=\"https://github.com/friederikehanssen.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/felixkrueger\"><img src=\"https://github.com/felixkrueger.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/erikrikarddaniel\"><img src=\"https://github.com/erikrikarddaniel.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/edmundmiller\"><img src=\"https://github.com/edmundmiller.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/d4straub\"><img src=\"https://github.com/d4straub.png\" width=\"50\" height=\"50\"></a>\r\n<a href=\"https://github.com/charles-plessy\"><img src=\"https://github.com/charles-plessy.png\" width=\"50\" height=\"50\"></a>\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\n## Citations\r\n\r\nIf you use plant-food-research-open/assemblyqc for your analysis, please cite it as:\r\n\r\n> Rashid, U., Wu, C., Shiller, J., Smith, K., Crowhurst, R., Davy, M., Chen, T.-H., Thomson, S., & Deng, C. (2024). AssemblyQC: A NextFlow pipeline for evaluating assembly quality (2.0.0). Zenodo. https://doi.org/10.5281/zenodo.10647870\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Data quality management",
            "Genomics"
        ],
        "filtered_on": "profil.* in description",
        "id": "1058",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1058?version=1",
        "name": "AssemblyQC: A NextFlow pipeline for evaluating assembly quality",
        "number_of_steps": 0,
        "projects": [
            "Plant-Food-Research-Open"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "busco",
            "hi-c",
            "merqury",
            "report",
            "statistics",
            "adaptor",
            "contamination",
            "fcs",
            "genome",
            "k-mer",
            "n50",
            "phasing",
            "quality control",
            "repeat",
            "synteny",
            "taxonomy",
            "telomere"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-06-25",
        "versions": 1
    },
    {
        "create_time": "2025-07-31",
        "creators": [
            "ABRomics None",
            "Pierre Marin",
            "Clea Siguret"
        ],
        "description": "Annotation of an assembled bacterial genomes to detect genes, potential plasmids, integrons and Insertion sequence (IS) elements.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid.* in description",
        "id": "1050",
        "keep": "Keep",
        "latest_version": 12,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1050?version=12",
        "name": "bacterial_genome_annotation/main",
        "number_of_steps": 6,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "abromics",
            "annotation",
            "genomics",
            "bacterial-genomics",
            "fasta",
            "genome-annotation"
        ],
        "tools": [
            "tooldistillator",
            "isescan",
            "bakta",
            "integron_finder",
            "plasmidfinder",
            "tooldistillator_summarize"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 12
    },
    {
        "create_time": "2025-09-18",
        "creators": [
            "ABRomics None",
            "Pierre Marin",
            "Clea Siguret"
        ],
        "description": "Antimicrobial resistance gene detection from assembled bacterial genomes",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "amr in tags",
        "id": "1049",
        "keep": "Keep",
        "latest_version": 8,
        "license": "GPL-3.0-or-later",
        "link": "https:/workflowhub.eu/workflows/1049?version=8",
        "name": "amr_gene_detection/main",
        "number_of_steps": 5,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "abromics",
            "amr",
            "amr-detection",
            "genomics",
            "antibiotic-resistance",
            "antimicrobial resistance",
            "antimicrobial-resistance-genes",
            "bacterial-genomics",
            "fasta"
        ],
        "tools": [
            "tooldistillator",
            "abricate",
            "amrfinderplus",
            "tooldistillator_summarize",
            "staramr_search"
        ],
        "type": "Galaxy",
        "update_time": "2025-09-18",
        "versions": 8
    },
    {
        "create_time": "2024-06-11",
        "creators": [
            "Sarai Varona and Sara Monzon",
            "Patel H",
            "Varona S and Monzon S"
        ],
        "description": "Assembly and intrahost/low-frequency variant calling for viral samples",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "1027",
        "keep": "To Curate",
        "latest_version": 11,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/1027?version=11",
        "name": "nf-core/viralrecon",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "artic",
            "amplicon",
            "assembly",
            "metagenomics",
            "ont",
            "sars-cov-2",
            "virus",
            "covid-19",
            "covid19",
            "illumina",
            "long-read-sequencing",
            "nanopore",
            "oxford-nanopore",
            "variant-calling",
            "viral"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-06-11",
        "versions": 11
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Alexander Peltzer",
            "The nf-core/eager community",
            "Stephen Clayton",
            "James A Fellows-Yates"
        ],
        "description": "A fully reproducible and state-of-the-art ancient DNA analysis pipeline",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "983",
        "keep": "To Curate",
        "latest_version": 30,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/983?version=30",
        "name": "nf-core/eager",
        "number_of_steps": 0,
        "projects": [
            "nf-core"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "adna",
            "ancient-dna-analysis",
            "ancientdna",
            "genome",
            "pathogen-genomics",
            "population-genetics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2025-03-26",
        "versions": 30
    },
    {
        "create_time": "2024-11-27",
        "creators": [
            "Debjyoti Ghosh"
        ],
        "description": "Use DADA2 for sequence quality control. DADA2 is a pipeline for detecting and correcting (where possible) Illumina amplicon sequence data. As implemented in the q2-dada2 plugin, this quality control process will additionally filter any phiX reads (commonly present in marker gene Illumina sequence data) that are identified in the sequencing data, and will filter chimeric sequences.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in description",
        "id": "892",
        "keep": "Keep",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/892?version=3",
        "name": "qiime2-II-denoising/IIa-denoising-se",
        "number_of_steps": 4,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "qiime2__feature_table__summarize",
            "qiime2__feature_table__tabulate_seqs",
            "qiime2__metadata__tabulate",
            "qiime2__dada2__denoise_single"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 3
    },
    {
        "create_time": "2021-11-11",
        "creators": [
            "Michael Roach"
        ],
        "description": "A hecatomb is a great sacrifice or an extensive loss. Heactomb the software empowers an analyst to make data driven decisions to 'sacrifice' false-positive viral reads from metagenomes to enrich for true-positive viral reads. This process frequently results in a great loss of suspected viral sequences / contigs.\r\n\r\nFor information about installation, usage, tutorial etc please refer to the documentation: https://hecatomb.readthedocs.io/en/latest/\r\n\r\n### Quick start guide\r\n\r\nInstall Hecatomb from Bioconda\r\n```bash\r\n# create an env called hecatomb and install Hecatomb in it\r\nconda create -n hecatomb -c conda-forge -c bioconda hecatomb\r\n\r\n# activate conda env\r\nconda activate hecatomb\r\n\r\n# check the installation\r\nhecatomb -h\r\n\r\n# download the databases - you only have to do this once\r\nhecatomb install\r\n\r\n# Run the test dataset\r\nhecatomb run --test\r\n```",
        "doi": "10.48546/workflowhub.workflow.235.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "235",
        "keep": "Keep",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/235?version=1",
        "name": "Hecatomb",
        "number_of_steps": 0,
        "projects": [
            "HecatombDevelopment"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-05-13",
        "versions": 1
    },
    {
        "create_time": "2024-04-26",
        "creators": [
            "Dominik Lux",
            "Julian Uszkoreit"
        ],
        "description": "# ProGFASTAGen\r\n\r\nThe ProGFASTAGen (**Pro**tein-**G**raph-**FASTA**-**Gen**erator or **Pro**t**G**raph-**FASTA**-**Gen**erator) repository contains workflows to generate so-called precursor-specific-FASTAs (using the precursors from MGF-files) including feature-peptides, like VARIANTs or CONFLICTs if desired, or global-FASTAs (as described in [ProtGraph](https://github.com/mpc-bioinformatics/ProtGraph)). The single workflow scripts have been implemented with [Nextflow-DSL-2](https://www.nextflow.io/docs/latest/dsl2.html) and are independent to each other. Each of these workflows can be used on their own or can be imported to other workflows for other use-cases. Further, we included three main-workflows, to show how the single workflows can be chained together. The `main_workflow_protein_fasta.nf`-workflow converts Thermo-RAW-files into MGF, searches with Comet (and Percolator) and the identification results are then further summarized. The workflows `main_workflow_global_fasta.nf` and `main_workflow_precursor_specific_fasta.nf` generate specific FASTA-files before search-engine-identification. Below are example nextflow-calls, which can be used.\r\n\r\nRegarding the precursor-specific-FASTA-generation: The source-code of the C++ implementation for traversal can be found in `bin`. There, four implementations are present: `Float/Int`-Versions as well as `DryRun/VarLimitter`-Versions of the traversal. The `Float/Int`-Versions can be faster/slower depending on th processor-architecture and can be used via a flag in the `create_precursor_specific_fasta.nf`-workflow. The `DryRun`-Version does not generate a FASTA but tests the used system (depending on a query-timeout) to determine the maximum number of variants which can be used, while not timing out. The actual FASTA-generation happens in the `VarLimitter`-Version using the generated protein-graphs at hand.\r\n\r\nin **Prerequisites** a small description of dependencies and how to set up the host system is given. **Individual steps** describes the single workflows and how they can be called, while **Main Workflow Scripts** shows example-calls of the main workflows. In **Regenerate Results from Publication**, the calls and parameters are shown, which were used in the publication. Using the same FASTA or UniProt flat file format with a similar server-setting should yield similar results as used in the publication.\r\n\r\n## Prerequisites\r\n\r\n### Executing on Linux\r\n\r\nThis workflow can be only executed on linux (tested on Ubuntu 22.04 and ArchLinux). Before setting up the `bin`-folder, some requiered binaries need to be present on the OS. (Focusing on Ubuntu:) The following packages need to be installed on Ubuntu (via `apt`), if not already:\r\n\r\n```text\r\nbuild-essential\r\nwget\r\ncurl\r\nunzip\r\ncmake\r\nmono-complete\r\npython3-pip (or any environment with Python3, where pip is available)\r\npython-is-python3 (needed for ubuntu, so that python points to python3)\r\n```\r\n\r\nIf all packages are installed (and the python environment is set up), the setup-script needs to be executed, which downloads needed dependencies and compiles the source-code located in the `bin`-folder:\r\n\r\n```shell\r\nchmod +x compile_and_setup_depencies.sh  # In case this file is not executable\r\n./compile_and_setup_depencies.sh  # Downloads dependencies, compiles the C++-implementation and sets all binaries in the bin-folder as executable\r\n```\r\n\r\nIf the script exits without errors, the provided workflows can be executed with the command `nextflow`.\r\n\r\n### Executing in Docker\r\n\r\nAlternatively, docker can be used. For this, please follow the [installation guide](https://docs.docker.com/engine/install/ubuntu/) for docker. After installing docker, a local docker-container can be build with all needed dependencies for the workflows. We provide a `Dockerfile` in the `docker`-folder. To build it, execute (while beeing with a shell in the root-folder of this repository) the following:\r\n\r\n```shell\r\ndocker build -t progfastagen:local . -f docker/Dockerfile\r\n```\r\n\r\nThis command builds a local docker container, tagging it with `progfastagen:local`, which can be later used by nextflow. To use it with nextflow, make sure that `nextflow` is installed on the host-system. For each of the workflow example calls below, the `-with-docker progfastagen:local` then needs to be appended, to let `nextflow` know to use the local docker-container.\r\n\r\n## Individual Steps\r\n\r\nEach step has been implemented in such a way, that it can be executed on its own. Each subsection below, provides a brief overview and an example call of the required parameters to demonstrate how the workflow can be called. If you are interested for all the available parameters within a workflow and want modify or tune them, then please refer to the source of the workflows, where each parameter is described briefly.\r\n\r\n### Converting RAW-files to MGF\r\n\r\nThe workflow `convert_to_mgf.nf` is a wrapper around the ThermoRawFileParser and converts RAW-files to the MGF-format. The `ctm_raws` parameter needs to be set, in order to generate the MGF-files:\r\n\r\n```text\r\nnextflow run convert_to_mgf.nf \\\r\n    --ctm_raws < Folder containing RAW-files > \\\r\n    --ctm_outdir < Output-Folder, where the MGFs should be stored >\r\n```\r\n\r\n### Generating a Precursor-Specific-FASTA\r\n\r\nThe workflow `create_precursor_specific_fasta.nf` generates a precursor-specific-FASTA-file, tailored to a set of MGF-files. Here, Protein-Graphs are generated, using the UniProt flat file format (which can be downloaded from [UniProt](https://www.uniprot.org/) by selecting `Text` as format) and a python script prepares the queries, by extracting the MS2-precursors from the MGF-files (using a tolerance, in ppm). Using the Protein-Graphs and a `DryRun`-Version of the traversal, the maximum-variant-limits are determined for each Protein-Graph (and mass-query-range) using a binary-search. These limits are then used for the actual ms2-specific-FASTA-generation in conjunction with the extracted MS2-precursors and a compacted FASTA is returned, which is tailored to the MGF-files.\r\n\r\nAltough of the complexity, the workflow only requires the following parameters to generate such a FASTA:\r\n\r\n```text\r\nnextflow run create_precursor_specific_fasta.nf \\\r\n    --cmf_mgf_files < Folder containing MGF-files > \\\r\n    --cmf_sp_embl_file < Path to a SP-EMBL-File (UniProt flat file format) > \\\r\n    --cmf_outdir <The Output-Folder where the traversal-limits are saved and the ms2-specific-FASTA is stored >\r\n```\r\n\r\nThe optional parameter: `cmf_pg_additional_params` is added to ProtGraph directly, allowing every parameter, ProtGraph provides to be set there (e.g. useful if the digestion should be changed or features/PTMs should be included/excluded, etc...), allowing arbitrary settings to generate Protein-Graphs if desired. It defaults to use all features, ProtGraph can parse.\r\n\r\n**Note regarding PTMs/Tolerance**: The FASTA is tailored to the MS2-precursors, therefore variable and fixed modifications need to be set to the same settings as for the actual identification. This workflow defaults to carbamidomethylation (C, fixed) and oxidation (M, variable). See ProtGraph (and the workflow-parameter `cmf_pg_additional_params`) to set the PTMs accordingly in the Protein-Graphs. The same applies for the MS2-precursor-tolereance which can be set with `cmf_query_ppm` and defaults to `5ppm`.\r\n\r\n**Note regarding Limits**: This workflows defaults to allow up to 5 seconds per query and limits peptides to contain at most 5 variants (with a maximum of 5000 Da per peptide), resulting into FASTA-files which can be 15-200GB large (depending on dataset and species). Changing these settings can drastically increase/decrease the runtime/memory usage/disk usage. We advise to change those settings slightly and to pay attention on the runtime/memory usage/disk usage if run with the newly set limits (and dataset + species) the first time.\r\n\r\n**Note regarding identification**: If digestion is enabled (default is `Trypsin`), the resulting FASTA contains already digested entries, thus searching with a search-engine, the digestion should be set to `off/no_cut`.\r\n\r\n### Generating a Global-FASTA\r\n\r\nThis workflow generates a so called global-FASTA, using ProtGraph, the UniProt flat file format and some global limits for writing out peptides/proteins. Global-FASTAs can be generated with the `create_global_fasta.nf`-workflow. To generate a global-FASTA, only a path to a single SP-EMBL-file (UniProt flat file format) is required. Such a file can be downloaded from [UniProt](https://www.uniprot.org/) directly, by selecting `Text` instead of `FASTA` as the download format.\r\n\r\n```text\r\nnextflow run create_global_fasta.nf \\\r\n    --cgf_sp_embl_file < Path to a SP-EMBL-File (UniProt flat file format) > \\\r\n    --cgf_outdir < The output-folder, where the gloabl-FASTA and some Protein-Graph-statistics should be saved >\r\n```\r\n\r\nPer default, this workflow does not export feature-peptides and is set to only export peptides with up to 5000 Da mass and maximum of two miscleavages. It is possible to generate global-FASTA with some specific features (like containing, `SIGNAL`, `PEPTIDE` or others) and other limits. The parameters `cgf_features_in_graphs` and `cgf_peptide_limits` can be set accordingly. These are added to ProtGraph directly, hence every parameter ProtGraph provides, can be set here (including different digestion settings).\r\n\r\n**Note**: A dry run with ProtGraph to generate statistics how many peptide would be theoretically exported is advised prior for testing. Some Protein-Graphs with some features (e.g. P53 using variants) can contain to many peptides, which could result to very long runtimes and huge FASTAs.\r\n\r\n**Note regarding identification**: If digestion is enabled (default is `Trypsin`), the resulting FASTA contains already digested entries, thus searching with a search-engine, the digestion should be set to `off/no_cut`.\r\n\r\n### Identification via Coment (and Percolator)\r\n\r\nWe provide an identification workflow to showcase, that the generated FASTAs can be used with search-engines. The workflow `identification_via_comet.nf` identifies MGF-files individually, using custom search-settings for Comet (and if desired rescores the results with Percolator), applies an FDR-cutoff using the q-value (for each file) and exposes the identification results into an output-folder.\r\n\r\nThree parameters are required, to execute the workflow:\r\n\r\n1. The MGFs which should be identified\r\n2. The Comet-Parameter file to set the search-settings\r\n3. The FASTA-file which should be used for identification\r\n\r\nBelow is an example call with all required parameters (Percolator is enabled by default):\r\n\r\n```text\r\nnextflow run identification_via_comet.nf \\\r\n    --idc_mgf_folder < Folder containing MGF-files > \\\r\n    --idc_fasta_file < The FASTA which should be used for identification > \\\r\n    --idc_search_parameter_file < The Comet-Parameters file (Search Configuration) > \\\r\n    --idc_outdir < Output-Folder where the results of the identification files are stored >\r\n```\r\n\r\nHere is another example call with all required parameters (this time, turning Percolator off):\r\n\r\n```text\r\nnextflow run identification_via_comet.nf \\\r\n    --idc_mgf_folder < Folder containing MGF-files > \\\r\n    --idc_fasta_file < The FASTA which should be used for identification > \\\r\n    --idc_search_parameter_file < The Comet-Parameters file (Search Configuration) > \\\r\n    --idc_outdir < Output-Folder where the results of the identification files are stored > \\\r\n    --idc_use_percolator 0\r\n```\r\n\r\n**Note**: This identification-workflow defaults to an FDR-cutoff (q-value) of `--idc_fdr \"0.01\"`, reporting only 1% filtered PSMs. Arbitrary and multiple FDR-cutoffs can be set and can be changed to the desired value.\r\n\r\n### Summarization of results\r\n\r\nThe `summarize_ident_results.nf`-workflow genereates convenient summarization of the identification results. Here, the identification-results are binned into 4 groups:\r\n\r\n1. Unique PSMs (a match, which can only originate from one protein)\r\n2. Shared PSMs (a match, which can originate from multiple proteins)\r\n3. Unique Feature PSMs (as 1., but only containing peptides, which can be explained by a features)\r\n4. Shared Feature PSMs (as 2., but only can be explained by features from all originating proteins)\r\n\r\nFurthermore, heatmaps are generated to provide an overview of found peptides across all MGFs/RAW-files.\r\n\r\nTo call this method, a `glob` needs to be specified in this workflow:\r\n\r\n```text\r\nnextflow run summarize_ident_results.nf \\\r\n    --sir_identified_files_glob < The glob matching the desired output from the identification results >\r\n    --sir_outdir < The output directory where the summarized results should be saved >\r\n```\r\n\r\nIn case, the identification workflow was executed using an FDR of 0.01, you could use the following `glob`:\r\n\r\n```text\r\nnextflow run summarize_ident_results.nf \\\r\n    --sir_identified_files_glob \"<Path_to_folder>/*qvalue_no_decoys_fdr_0.01.tsv\"\r\n    --sir_outdir < The output directory where the summarized results should be saved >\r\n```\r\n\r\n**Note**: This step can be used only if specific columns are present in the tables. Furthermore, it distinguishes between the identification results from a FASTA by UniProt or by ProtGraph. The additional parameters control, whether to bin results in group 3 and 4, decide if variable modifications should be considered as unique, as well as if a peptide, which originates multiple times to the same protein should be considered as unique. The main-workflows set these parameters accordingly and can be used as an example.\r\n\r\n## Main Workflow Scripts\r\n\r\nEach individual step described above, is also imported and chained into three main-workflows:\r\n\r\n1. `main_workflow_protein_fasta.nf` (UniProt-FASTA-search)\r\n2. `main_workflow_global_fasta.nf` (Generation of a global-FASTA and search)\r\n3. `main_workflow_precursor_specific_fasta.nf` (Generation of a precursor-specific-FASTA and search)\r\n\r\ngenerating summarized identification results across multiple RAW-files.\r\n\r\nIn each of these workflows, it is possible to modify the parameters of the imported subworkflows, by using the imported subworkflows parameters directly (as shown in the **Individual Steps** above).\r\n\r\nFor protein-FASTA identification, only three parameters are required:\r\n\r\n```text\r\nnextflow run main_workflow_protein_fasta.nf \\\r\n    --main_fasta_file < The FASTA-file, to be used for identification > \\\r\n    --main_raw_files_folder < The folder containing RAW-files > \\\r\n    --main_comet_params < The parameters file for comet (for identification) > \\\r\n    --main_outdir < Output-Folder where all the results from the workflows should be saved >\r\n```\r\n\r\nThis is also true for the other two workflows, where instead of a FASTA-file, the UniProt flat file format needs to be provided. Such a file can be downloaded from [UniProt](https://www.uniprot.org/) directly, by selecting the format `Text` instead of the format `FASTA`.\r\n\r\nHere are the correpsonding calls for global-FASTA and precurosr-specific-FASTA generation and identification:\r\n\r\n```text\r\n# global-FASTA\r\nnextflow run main_workflow_global_fasta.nf \\\r\n    --main_sp_embl_file < The SP-EMBL-file used for Protein-Graph- and FASTA-generation (UniProt flat file format) > \\\r\n    --main_raw_files_folder < The folder containing RAW-files > \\\r\n    --main_comet_params< The parameters file for comet (for identification) > \\\r\n    --main_outdir < Output-Folder where all the results from the workflows should be saved >\r\n\r\n# precursor-specific-FASTA\r\nnextflow run main_workflow_precursor_specific_fasta.nf \\\r\n    --main_sp_embl_file < The SP-EMBL-file used for Protein-Graph- and FASTA-generation (UniProt flat file format) > \\\r\n    --main_raw_files_folder < The folder containing RAW-files > \\\r\n    --main_comet_params < The parameters file for comet (for identification) > \\\r\n    --main_outdir < Output-Folder where all the results from the workflows should be saved >\r\n```\r\n\r\n**Note**: Only defining the required parameters, uses the default parameters for every other setting. For all workflows, this would mean, that the FDR-cutoff (q-value) is set to `0.01` resulting into both FDRs considered. Furthermore, the global-FASTA and precursor-specific-FASTA workflows assume Trypsin digestion. For the global-FASTA-workflow, no features are exported by default, which may not be desired, if someone whishes to search for peptide-features (like `SIGNAL`, etc..). For the precursor-specific-FASTA-workflow, the PTMs carbamidomethylation (C, fixed) and oxidation (M, variable) are assumed, which may need to be modified.\r\n\r\n**Note regarding example calls**: Further below you can find the calls as used in the publication. These set the most minimal parameters for a correct execution on custom datasets and can be used as an example.\r\n\r\n## Regenerate Results from Publication\r\n\r\nIn this subsection you can find the nextflow-calls which were used to execute the 3 workflows. Executing this with the same UniProt flat file/FASTA-file should yield the similar/same results. For generated precursor-specific-FASTAs it may happen, that these are generated with slightly different variant-limits, therefore a slightly different FASTA to search with and slightly different identification results.\r\n\r\nThe FASTA/UniProt flat file used for identification can be found [here](https://cloud.mpc.rub.de/s/LJ2bgGNmsxzSaod). The Comet configuration files are provided in the `example_configuration`-folder. The datasets can be retrieved from [PRIDE](https://www.ebi.ac.uk/pride/).\r\n\r\n### PXD002171\r\n\r\n```shell\r\n# PXD002171 Precursor-Specific\r\nnextflow run main_workflow_precursor_specific_fasta.nf \\\r\n    -with-report \"PXD002171_results_precursor_specific/nextflow_report.html\" \\\r\n    -with-timeline \"PXD002171_results_precursor_specific/nextflow_timeline.html\" \\\r\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\r\n    --main_raw_files_folder PXD002171 \\\r\n    --main_comet_params example_configurations/PXD002171_no_dig.txt \\\r\n    --main_outdir PXD002171_results_precursor_specific \\\r\n    --cmf_max_precursor_da 5000 \\\r\n    --cmf_query_ppm 5 \\\r\n    --cmf_timeout_for_single_query 5 \\\r\n    --cmf_maximum_variant_limit 5 \\\r\n    --cmf_pg_additional_params \"-ft VARIANT -ft SIGNAL -ft INIT_MET -ft CONFLICT -ft VAR_SEQ -ft PEPTIDE -ft PROPEP -ft CHAIN -vm 'M:15.994915' -vm 'C:71.037114'\" \\\r\n    --idc_fdr \"0.01\"\r\n    \r\n# PXD002171 Global digested FASTA\r\nnextflow run main_workflow_global_fasta.nf \\\r\n    -with-report \"PXD002171_global_fasta/nextflow_report.html\" \\\r\n    -with-timeline \"PXD002171_global_fasta/nextflow_timeline.html\" \\\r\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\r\n    --main_raw_files_folder PXD002171 \\\r\n    --main_comet_params example_configurations/PXD002171_no_dig.txt \\\r\n    --main_outdir PXD002171_global_fasta \\\r\n    --cgf_features_in_graphs \"-ft None\" \\\r\n    --cgf_peptide_limits \"--pep_miscleavages 2 --pep_min_pep_length 5\" \\\r\n    --idc_fdr \"0.01\"\r\n\r\n# PXD002171 Protein FASTA\r\nnextflow run main_workflow_protein_fasta.nf \\\r\n    -with-report \"PXD002171_protein_fasta/nextflow_report.html\" \\\r\n    -with-timeline \"PXD002171_protein_fasta/nextflow_timeline.html\" \\\r\n    --main_fasta_file 20230619_homo_sapiens_proteome.fasta \\\r\n    --main_raw_files_folder PXD002171 \\\r\n    --main_comet_params example_configurations/PXD002171_trypsin_dig.txt \\\r\n    --main_outdir PXD002171_protein_fasta \\\r\n    --idc_fdr \"0.01\"\r\n```\r\n\r\n### PXD028605\r\n\r\n```shell\r\n# PXD028605 Precursor-Specific\r\nnextflow run main_workflow_precursor_specific_fasta.nf \\\r\n    -with-report \"PXD028605_results_precursor_specific/nextflow_report.html\" \\\r\n    -with-timeline \"PXD028605_results_precursor_specific/nextflow_timeline.html\" \\\r\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\r\n    --main_raw_files_folder PXD028605 \\\r\n    --main_comet_params example_configurations/PXD028605_no_dig.txt \\\r\n    --main_outdir PXD028605_results_precursor_specific \\\r\n    --cmf_max_precursor_da 5000 \\\r\n    --cmf_query_ppm 20 \\\r\n    --cmf_timeout_for_single_query 5 \\\r\n    --cmf_maximum_variant_limit 5 \\\r\n    --cmf_pg_additional_params \"-ft VARIANT -ft SIGNAL -ft INIT_MET -ft CONFLICT -ft VAR_SEQ -ft PEPTIDE -ft PROPEP -ft CHAIN -fm 'C:57.021464' -vm 'M:15.9949'\" \\\r\n    --idc_fdr \"0.01\"\r\n\r\n# PXD028605 Global digested FASTA\r\nnextflow run main_workflow_global_fasta.nf \\\r\n    -with-report \"PXD028605_global_fasta/nextflow_report.html\" \\\r\n    -with-timeline \"PXD028605_global_fasta/nextflow_timeline.html\" \\\r\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\r\n    --main_raw_files_folder PXD028605 \\\r\n    --main_comet_params example_configurations/PXD028605_no_dig.txt \\\r\n    --main_outdir PXD028605_global_fasta \\\r\n    --cgf_features_in_graphs \"-ft None\" \\\r\n    --cgf_peptide_limits \"--pep_miscleavages 2 --pep_min_pep_length 5\" \\\r\n    --idc_fdr \"0.01\"\r\n\r\n# PXD028605 Protein FASTA\r\nnextflow run main_workflow_protein_fasta.nf \\\r\n    -with-report \"PXD028605_protein_fasta/nextflow_report.html\" \\\r\n    -with-timeline \"PXD028605_protein_fasta/nextflow_timeline.html\" \\\r\n    --main_fasta_file 20230619_homo_sapiens_proteome.fasta \\\r\n    --main_raw_files_folder PXD028605 \\\r\n    --main_comet_params example_configurations/PXD028605_trypsin_dig.txt \\\r\n    --main_outdir PXD028605_protein_fasta \\\r\n    --idc_fdr \"0.01\"\r\n```\r\n",
        "doi": "10.48546/workflowhub.workflow.837.1",
        "edam_operation": [
            "Peptide identification",
            "Protein identification"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Protein variants",
            "Proteomics"
        ],
        "filtered_on": "binn.* in description",
        "id": "837",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "BSD-3-Clause",
        "link": "https:/workflowhub.eu/workflows/837?version=1",
        "name": "ProGFASTAGen - Protein-Graph FASTA Generation (and Identification) Workflows",
        "number_of_steps": 0,
        "projects": [
            "Medizinisches Proteom-Center, Medical Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "proteomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2024-04-26",
        "versions": 1
    },
    {
        "create_time": "2024-04-09",
        "creators": [
            "Damon-Lee Pointon",
            "William Eagles",
            "Ying Sims"
        ],
        "description": "[![Cite with Zenodo](https://zenodo.org/badge/509096312.svg)](https://zenodo.org/doi/10.5281/zenodo.10047653)\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.10.1-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/treeval)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/treeval [1.1.0 - Ancient Aurora]** is a bioinformatics best-practice analysis pipeline for the generation of data supplemental to the curation of reference quality genomes. This pipeline has been written to generate flat files compatible with [JBrowse2](https://jbrowse.org/jb2/) as well as HiC maps for use in Juicebox, PretextView and HiGlass.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nYou can also set up and attempt to run the pipeline here: https://gitpod.io/#https://github.com/BGAcademy23/treeval-curation\r\nThis is a gitpod set up for BGA23 with a version of TreeVal, although for now gitpod will not run a nextflow pipeline die to issues with using singularity. We will be replacing this with an AWS instance soon.\r\n\r\nThe treeval pipeline has a sister pipeline currently named [curationpretext](https://github.com/sanger-tol/curationpretext) which acts to regenerate the pretext maps and accessory files during genomic curation in order to confirm interventions. This pipeline is sufficiently different to the treeval implementation that it is written as it's own pipeline.\r\n\r\n1. Parse input yaml ( YAML_INPUT )\r\n2. Generate my.genome file ( GENERATE_GENOME )\r\n3. Generate insilico digests of the input assembly ( INSILICO_DIGEST )\r\n4. Generate gene alignments with high quality data against the input assembly ( GENE_ALIGNMENT )\r\n5. Generate a repeat density graph ( REPEAT_DENSITY )\r\n6. Generate a gap track ( GAP_FINDER )\r\n7. Generate a map of self complementary sequence ( SELFCOMP )\r\n8. Generate syntenic alignments with a closely related high quality assembly ( SYNTENY )\r\n9. Generate a coverage track using PacBio data ( LONGREAD_COVERAGE )\r\n10. Generate HiC maps, pretext and higlass using HiC cram files ( HIC_MAPPING )\r\n11. Generate a telomere track based on input motif ( TELO_FINDER )\r\n12. Run Busco and convert results into bed format ( BUSCO_ANNOTATION )\r\n13. Ancestral Busco linkage if available for clade ( BUSCO_ANNOTATION:ANCESTRAL_GENE )\r\n14. Count KMERs with FastK and plot the spectra using MerquryFK ( KMER )\r\n15. Generate a coverge track using KMER data ( KMER_READ_COVERAGE )\r\n\r\n## Usage\r\n\r\n> **Note**\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\r\n> to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\r\n> with `-profile test` before running the workflow on actual data.\r\n\r\nCurrently, it is advised to run the pipeline with docker or singularity as a small number of major modules do not currently have a conda env associated with them.\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\n# For the FULL pipeline\r\nnextflow run main.nf -profile singularity --input treeval.yaml --outdir {OUTDIR}\r\n\r\n# For the RAPID subset\r\nnextflow run main.nf -profile singularity --input treeval.yaml -entry RAPID --outdir {OUTDIR}\r\n```\r\n\r\nAn example treeval.yaml can be found [here](assets/local_testing/nxOscDF5033.yaml).\r\n\r\nFurther documentation about the pipeline can be found in the following files: [usage](https://pipelines.tol.sanger.ac.uk/treeval/dev/usage), [parameters](https://pipelines.tol.sanger.ac.uk/treeval/dev/parameters) and [output](https://pipelines.tol.sanger.ac.uk/treeval/dev/output).\r\n\r\n> **Warning:**\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\r\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\n## Credits\r\n\r\nsanger-tol/treeval has been written by Damon-Lee Pointon (@DLBPointon), Yumi Sims (@yumisims) and William Eagles (@weaglesBio).\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n<ul>\r\n  <li>@gq1 - For building the infrastructure around TreeVal and helping with code review</li>\r\n  <li>@ksenia-krasheninnikova - For help with C code implementation and YAML parsing</li>\r\n  <li>@mcshane - For guidance on algorithms </li>\r\n  <li>@muffato - For code reviews and code support</li>\r\n  <li>@priyanka-surana - For help with the majority of code reviews and code support</li>\r\n</ul>\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\n## Citations\r\n\r\n<!--TODO: Citation-->\r\n\r\nIf you use sanger-tol/treeval for your analysis, please cite it using the following doi: [10.5281/zenodo.10047653](https://doi.org/10.5281/zenodo.10047653).\r\n\r\n### Tools\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [
            "Genome alignment",
            "Genome assembly",
            "Sequence assembly visualisation"
        ],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "813",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/813?version=1",
        "name": "sanger-tol/treeval v1.1.0 - Ancient Aurora",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "genome_assembly"
        ],
        "tools": [
            "SAMtools",
            "BEDTools",
            "Minimap2",
            "pyfastaq",
            "Nextflow",
            "BUSCO",
            "seqtk",
            "UCSC Genome Browser Utilities",
            "PretextView",
            "JBrowse 2",
            "Merqury",
            "miniprot",
            "tabix",
            "MUMmer"
        ],
        "type": "Nextflow",
        "update_time": "2024-04-09",
        "versions": 1
    },
    {
        "create_time": "2025-03-26",
        "creators": [
            "Matthias Bernt"
        ],
        "description": "dada2 amplicon analysis for paired end data\n\nThe workflow has three main outputs: \n- the sequence table (output of makeSequenceTable)\n- the taxonomy (output of assignTaxonomy)\n- the counts which allow to track the number of sequences in the samples through the steps (output of sequence counts)",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in tags",
        "id": "790",
        "keep": "Keep",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/790?version=3",
        "name": "dada2/main",
        "number_of_steps": 14,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "name:amplicon"
        ],
        "tools": [
            "dada2_removeBimeraDenovo",
            "__UNZIP_COLLECTION__",
            "dada2_plotQualityProfile",
            "dada2_dada",
            "dada2_seqCounts",
            "__APPLY_RULES__",
            "dada2_mergePairs",
            "dada2_filterAndTrim",
            "dada2_assignTaxonomyAddspecies",
            "dada2_learnErrors",
            "dada2_makeSequenceTable"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 3
    },
    {
        "create_time": "2024-02-13",
        "creators": [],
        "description": "## EBP-Nor Genome Assembly pipeline\r\n\r\nThis repository contains the EBP-Nor genome assembly pipeline. This pipeline is implemented in snakemake.\r\nThis pipeline is developed to create haplotype-resolved genome assemblies from PacBio HiFi reads and HiC reads,\r\nand is primarly designed for diploid eukaryotic organisms. The pipeline is designed to work on a linux cluster with slurm as workload manager.\r\n\r\n## Requirements & Setup\r\n\r\nSome software need to be configured/installed before the pipeline can be run\r\n\r\n### Conda setup\r\n\r\nMost required software, including snakemake itself, can be installed using [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html).\r\n\r\nOnce conda is installed, you can create a new environment containing most necessary software from the provided asm_pipeline.yaml file as follows:\r\n\r\n```shell\r\nconda create -n asm_pipeline --file=worfklow/envs/asm_pipeline.yaml\r\n```\r\n\r\n### Other software setup\r\n\r\nThe following software need to be installed manually:\r\n\r\n- KMC v3.1.1 (https://github.com/tbenavi1/KMC)\r\n- HiFiAdapterFilt (https://github.com/sheinasim/HiFiAdapterFilt)\r\n- Oatk (https://github.com/c-zhou/oatk)\r\n- OatkDB (https://github.com/c-zhou/OatkDB)\r\n- NCBI FCS-Adaptor (https://github.com/ncbi/fcs/wiki/FCS-adaptor)\r\n- NCBI FCS-GX (https://github.com/ncbi/fcs/wiki/FCS-GX)\r\n\r\nPlease refer to their respective installation instructions to properly install them. You will need to privide the installation paths of these software to the config file (see Parameter section).\r\n\r\n### BUSCO database setup\r\n\r\nAs in general, computing nodes are not connected to the internet, BUSCO lineage datasets need to be downloaded manually before running the pipeline.\r\nThis can easily be done by running\r\n\r\n```shell\r\nbusco --download eukaryota\r\n```\r\n\r\nYou will need to specify the folder where you downloaded the busco lineages in the config file (see Parameter section).\r\n\r\n### Data\r\n\r\nThis pipeline is created for using PacBio HiFi reads together with paired-end Hi-C data.\r\nYou will need to specify the absolute paths to these files in the config file (see Parameters section).\r\n\r\n### Parameters\r\n\r\nThe necessary config files for running the pipeline can be found in the config folder.\r\n\r\nGeneral snakemake and cluster submission parameters are defined in ```config/config.yaml```, \r\ndata- and software-specfic parameters are defined in ```config/asm_params.yaml```.\r\n\r\nFirst, define the paths of the input files you want to use:\r\n- pacbio: path to the location of the PacBio HiFi reads (```.fastq.gz```)\r\n- hicF and hicR: path to the forward and reverse HiC reads respectively\r\n\r\nFor software not installed by conda, the installation path needs to be provided to the Snakemake pipeline by editing following parameters in the ```config/asm_params.yaml```:\r\n\r\n- Set the \"adapterfilt_install_dir\" parameter to the installation path of HiFiAdapterFilt\r\n- Set the \"KMC_path\" parameter to the installation path of KMC\r\n- Set the \"oatk_dir\" parameter to the installation path of oatk\r\n- Set the \"oatk_db\" parameter to the directory where you downloaded the oatk_db files\r\n- Set the \"fcs_path\" parameter to the location of the ```run_fcsadaptor.sh``` and ```fcs.py``` scripts\r\n- Set the \"fcs_adaptor_image\" and \"fcs_gx_image\" parameters to the paths to the ```fcs-adaptor.sif``` and ```fcs-gx.sif``` files respectively\r\n- Set the \"fcs_gx_db\" parameter to the path of the fcs-gx database\r\n\r\nA couple of other parameters need to be verified as well in the config/asm_params.yaml file before running the pipeline:\r\n\r\n- The location of the input data (```input_dir```) should be set to the folder containing the input data.\r\n- The location of the downloaded busco lineages (```busco_db_dir```) should be set to the folder containing the busco lineages files downloaded earlier\r\n- The required BUSCO lineage for running the BUSCO analysis needs to set (```busco_lineage``` parameter). Run ```busco --list-datasets``` to get an overview of all available datasets.\r\n- The required oatk lineage for running organelle genome assembly (```oatk_lineage``` parameter). Check https://github.com/c-zhou/OatkDB for an overview of available lineages.\r\n- A boolean value wether the species is plant (for plastid prediction) or not (```oatk_isPlant```; set to either True or False)\r\n- The NCBI taxid of your species, required for the decontamination step (```taxid``` parameter)\r\n\r\n## Usage and run modes\r\n\r\nBefore running, make sure to activate the conda environment containing the necessary software: ```conda activate asm_assembly```.\r\nTo run the pipeline, run the following command:\r\n\r\n```\r\nsnakemake --profile config/ --configfile config/asm_params.yaml --snakefile workflow/Snakefile {run_mode}\r\n```\r\n\r\nIf you invoke the snakemake command in another directory than the one containing the ```workflow``` and ```config``` folders, \r\nor if the config files (```config.yaml``` and ```asm_params.yaml```) are in another location, you need to specify their correct paths on the command line.\r\n\r\nThe workflow parameters can be modified in 3 ways:\r\n- Directly modifying the ```config/asm_parameters.yaml``` file\r\n- Overriding the default parameters on the command line: ```--config parameter=new_value```\r\n- Overriding the default parameters using a different yaml file: ```--configfile path_to_parameters.yaml```\r\n\r\nThe pipeline has different runing modes, and the run mode should always be the last argument on the command line:\r\n\r\n- \"all\" (default): will run the full workflow including pre-assembly (genomescope & smudgeplot), assembly, scaffolding, decontamination, and organelle assembly\r\n- \"pre_assembly\": will run only the pre-assembly steps (genomescope & smudgeplot)\r\n- \"assembly\": will filter the HiFi reads and assemble them using hifiasm (also using the Hi-C reads), and run busco\r\n- \"scaffolding\": will run all steps necessary for scaffolding (filtering, assembly, HiC filtering, scaffolding, busco), but without pre-assembly\r\n- \"decontamination\": will run assembly, scaffolding, and decontamination, but without pre-assembly and busco analyses\r\n- \"organelles\": will run only organnelle genome assembly\r\n\r\n## Output\r\n\r\nAll generated output will be present in the \"results\" directory, which will be created in the folder from where you invoke the snakemake command.\r\nThis results directory contains different subdirectories related to the different steps in the assembly:\r\n- results/pre_assembly: genomescope and smudgeplot output (each in its own subfolder)\r\n- results/assembly: Hifiasm assembly output and corresponding busco results\r\n- results/scaffolding: scaffolding output, separated in two folders:\r\n  - meryl: meryl databases used for filtering HiC reads\r\n  - yahs: scaffolding output, including final scaffolds and their corresponding busco results\r\n- results/decontamination: decontamination output of the final scaffolded assembly\r\n- results/organelles: assembled organellar genomes\r\n\r\nAdditionally, a text file containing all software versions will be created in the specified input directory.\r\nThe log files of the different steps in the workflow can be found in the ```logs``` directory that will be created.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "740",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/740?version=1",
        "name": "EBP-Nor Genome Assembly Pipeline",
        "number_of_steps": 0,
        "projects": [
            "EBP-Nor"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-02-13",
        "versions": 1
    },
    {
        "create_time": "2024-01-24",
        "creators": [],
        "description": "![workflow](https://github.com/naturalis/barcode-constrained-phylogeny/actions/workflows/python-package-conda.yml/badge.svg)\r\n[![License: Apache-2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\r\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10519081.svg)](https://doi.org/10.5281/zenodo.10519081)\r\n\r\n![Logo](https://github.com/naturalis/barcode-constrained-phylogeny/blob/main/doc/logo-small.png?raw=true)\r\n\r\n# Bactria: BarCode TRee Inference and Analysis\r\nThis repository contains code and data for building very large, topologically-constrained \r\nbarcode phylogenies through a divide-and-conquer strategy. Such trees are useful as \r\nreference materials for curating barcode data by detecting rogue terminals (indicating\r\nincorrect taxonomic annotation) and in the comparable calculation of alpha and beta \r\nbiodiversity metrics across metabarcoding assays. \r\n\r\nThe input data for the approach we develop here currently comes from BOLD data dumps. \r\nThe international database [BOLD Systems](https://www.boldsystems.org/index.php) \r\ncontains DNA barcodes for hundreds of thousands of species, with multiple barcodes per \r\nspecies. The data dumps we use here are TSV files whose columns conform to the nascent\r\nBCDM (barcode data model) vocabulary. As such, other data sources that conform to this\r\nvocabulary could in the future be used as well, such as [UNITE](https://unite.ut.ee/).\r\n\r\nTheoretically, such data could be filtered and aligned per DNA marker to make \r\nphylogenetic trees. However, there are two limiting factors: building very large \r\nphylogenies is computationally intensive, and barcodes are not considered ideal for \r\nbuilding big trees because they are short (providing insufficient signal to resolve large \r\ntrees) and because they tend to saturate across large patristic distances.\r\n\r\n![concept](https://github.com/naturalis/barcode-constrained-phylogeny/blob/main/doc/concept.png)\r\n\r\nBoth problems can be mitigated by using the \r\n[Open Tree of Life](https://tree.opentreeoflife.org/opentree/argus/opentree13.4@ott93302) \r\nas a further source of phylogenetic signal. The BOLD data can be split into chunks that \r\ncorrespond to Open Tree of Life clades. These chunks can be made into alignments and \r\nsubtrees. The OpenTOL can be used as a constraint in the algorithms to make these. The \r\nchunks are then combined in a large synthesis by grafting them on a backbone made from \r\nexemplar taxa from the subtrees. Here too, the OpenTOL is a source of phylogenetic \r\nconstraint.\r\n\r\nIn this repository this concept is developed for both animal species and plant species.\r\n\r\n## Installation\r\n\r\nThe pipeline and its dependencies are managed using conda. On a linux or osx system, you \r\ncan follow these steps to set up the `bactria` Conda environment using an `environment.yml` \r\nfile and a `requirements.txt` file:\r\n\r\n1. **Clone the Repository:**  \r\n   Clone the repository containing the environment files to your local machine:\r\n   ```bash\r\n   git clone https://github.com/naturalis/barcode-constrained-phylogeny.git\r\n   cd barcode-constrained-phylogeny\r\n   ```\r\n2. **Create the Conda Environment:**\r\n   Create the bactria Conda environment using the environment.yml file with the following \r\n   command:\r\n   ```bash\r\n   conda env create -f workflow/envs/environment.yml\r\n   ```\r\n   This command will create a new Conda environment named bactria with the packages \r\n   specified in the environment.yml file. This step is largely a placeholder because\r\n   most of the dependency management is handled at the level of individual pipeline\r\n   steps, which each have their own environment specification.\r\n3. **Activate the Environment:**\r\n   After creating the environment, activate it using the conda activate command:\r\n   ```bash\r\n   conda activate bactria\r\n   ```\r\n4. **Verify the Environment:**\r\n   Verify that the bactria environment was set up correctly and that all packages were \r\n   installed using the conda list command:\r\n   ```bash\r\n   conda list\r\n   ```\r\n   This command will list all packages installed in the active conda environment. You should \r\n   see all the packages specified in the environment.yml file and the requirements.txt file.\r\n\r\n## How to run\r\n\r\nThe pipeline is implemented using snakemake, which is available within the conda \r\nenvironment that results from the installation. Important before running the snakemake pipeline \r\nis to change in [config/config.yaml](config/config.yaml) the number of threads available on your \r\ncomputer. Which marker gene is used in the pipeline is also specified in the config.yaml (default \r\nCOI-5P). Prior to execution, the BOLD data package to use (we used the \r\n[release of 30 December 2022](https://www.boldsystems.org/index.php/datapackage?id=BOLD_Public.30-Dec-2022)) \r\nmust be downloaded manually and stored in the [resources/](resources/) directory. If a BOLD release \r\nfrom another date is used the file names in config.yaml need to be updated. \r\n\r\nHow to run the entire pipeline:\r\n\r\n```bash \r\nsnakemake -j {number of threads} --use-conda\r\n```\r\n\r\nSnakemake rules can be performed separately:\r\n```bash \r\nsnakemake -R {Rule} -j {number of threads} --use-conda\r\n```\r\n\r\nEnter the same number at {number of threads} as you filled in previously in src/config.yaml.\r\nIn {Rule} insert the rule to be performed.\r\n\r\nHere is an overview of all the rules in the Snakefile:\r\n\r\n![graphviz (1)](https://github.com/naturalis/barcode-constrained-phylogeny/blob/main/doc/dag.svg)\r\n(zoomed view is available [here](https://raw.githubusercontent.com/naturalis/barcode-constrained-phylogeny/main/doc/dag.svg))\r\n\r\n## Repository layout\r\n\r\nBelow is the top-level layout of the repository. This layout is in line with \r\n[community standards](https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html) and must be adhered to.\r\nAll of these subfolders contains further explanatory READMEs to explain their contents in more detail.\r\n\r\n- [config](config/) - configuration files\r\n- [doc](doc/) - documentation and background literature\r\n- [logs](logs/) - where log files are written during pipeline runtime\r\n- [resources](resources/) - external data resources (from BOLD and OpenTree) are downloaded here\r\n- [results](results/) - intermediate and final results are generated here\r\n- [workflow](workflow/) - script source code and driver snakefile \r\n\r\n## License\r\n\r\n&copy; 2023 Naturalis Biodiversity Center\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except \r\nin compliance with the License. You may obtain a copy of the License at\r\n\r\n[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\r\n   \r\nUnless required by applicable law or agreed to in writing, software distributed under the License \r\nis distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express \r\nor implied. See the License for the specific language governing permissions and limitations under \r\nthe License.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metab.* in description",
        "id": "706",
        "keep": "Keep",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/706?version=1",
        "name": "Bactria: BarCode TRee Inference and Analysis",
        "number_of_steps": 0,
        "projects": [
            "Biodiversity Genomics Europe (general)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "python",
            "snakemake",
            "phylogenetics"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-02-05",
        "versions": 1
    },
    {
        "create_time": "2024-01-10",
        "creators": [
            "Michael Hall"
        ],
        "description": "# Pangenome databases provide superior host removal and mycobacteria classification from clinical metagenomic data\r\n\r\n> Hall, M, Coin, L., Pangenome databases provide superior host removal and mycobacteria classification from clinical metagenomic data. bioRxiv 2023. doi: [10.1101/2023.09.18.558339][doi]\r\n\r\nBenchmarking different ways of doing read (taxonomic) classification, with a focus on\r\nremoval of contamination and classification of _M. tuberculosis_ reads.\r\n\r\nThis repository contains the code and snakemake pipeline to build/download the\r\ndatabases, obtain all results from [the paper][doi], along with accompanying configuration\r\nfiles.\r\n\r\nCustom databases have all been uploaded to Zenodo, along with the simulated reads:\r\n\r\n- Nanopore simulated metagenomic reads - <https://doi.org/10.5281/zenodo.8339788>\r\n- Illumina simulated metagenomic reads - <https://doi.org/10.5281/zenodo.8339790>\r\n- Nanopore and Illumina artificial real reads - <https://doi.org/10.5281/zenodo.10472796>\r\n- Kraken2 database built from the Human Pangenome Reference Consortium\r\n  genomes - <https://doi.org/10.5281/zenodo.8339731>\r\n- Kraken2 database built from the kraken2 Human\r\n  library - <https://doi.org/10.5281/zenodo.8339699>\r\n- Kraken2 database built from a *Mycobacterium* representative set of\r\n  genomes - <https://doi.org/10.5281/zenodo.8339821>\r\n- A (fasta) database of representative genomes from the *Mycobacterium*\r\n  genus - <https://doi.org/10.5281/zenodo.8339940>\r\n- A (fasta) database of *M. tuberculosis* genomes from a variety of\r\n  lineages - <https://doi.org/10.5281/zenodo.8339947>\r\n- The fasta file built from the [Clockwork](https://github.com/iqbal-lab-org/clockwork)\r\n  decontamination pipeline - <https://doi.org/10.5281/zenodo.8339802>\r\n\r\n## Example usage\r\n\r\nWe provide some usage examples showing how to download the databases and then use them\r\non your reads.\r\n\r\n### Human read removal\r\n\r\nThe method we found to give the best balance of runtime, memory usage, and precision and\r\nrecall was kraken2 with a database built from the Human Pangenome Reference Consortium\r\ngenomes.\r\n\r\nThis example has been wrapped into a standalone tool called [`nohuman`](https://github.com/mbhall88/nohuman/) which takes a fastq as input and returns a fastq with human reads removed.\r\n\r\n#### Download human database\r\n\r\n```\r\nmkdir HPRC_db/\r\ncd HPRC_db\r\nURL=\"https://zenodo.org/record/8339732/files/k2_HPRC_20230810.tar.gz\"\r\nwget \"$URL\"\r\ntar -xzf k2_HPRC_20230810.tar.gz\r\nrm k2_HPRC_20230810.tar.gz\r\n```\r\n\r\n#### Run kraken2 with HPRC database\r\n\r\nYou'll need [kraken2](https://github.com/DerrickWood/kraken2) installed for this step.\r\n\r\n```\r\nkraken2 --threads 4 --db HPRC_db/ --output classifications.tsv reads.fq\r\n```\r\n\r\nIf you are using Illumina reads, a slight adjustment is needed\r\n\r\n```\r\nkraken2 --paired --threads 4 --db HPRC_db/ --output classifications.tsv reads_1.fq reads_2.fq\r\n```\r\n\r\n#### Extract non-human reads\r\n\r\nYou'll need [seqkit](https://github.com/shenwei356/seqkit) installed for this step\r\n\r\nFor Nanopore data\r\n\r\n```\r\nawk -F'\\t' '$1==\"U\" {print $2}' classifications.tsv | \\\r\n  seqkit grep -f - -o reads.depleted.fq reads.fq\r\n```\r\n\r\nFor Illumina data\r\n\r\n```\r\nawk -F'\\t' '$1==\"U\" {print $2}' classifications.tsv > ids.txt\r\nseqkit grep --id-regexp '^(\\S+)/[12]' -f ids.txt -o reads_1.depleted.fq reads_1.fq\r\nseqkit grep --id-regexp '^(\\S+)/[12]' -f ids.txt -o reads_2.depleted.fq reads_2.fq\r\n```\r\n\r\n### *M. tuberculosis* classification/enrichment\r\n\r\nFor this step we recommend either [minimap2](https://github.com/lh3/minimap2) or kraken2\r\nwith a *Mycobacterium* genus database. We leave it to the user to decide which approach\r\nthey prefer based on the results in our manuscript.\r\n\r\n#### Download databases\r\n\r\n```\r\nmkdir Mycobacterium_db\r\ncd Mycobacterium_db\r\n# download database for use with minimap2\r\nURL=\"https://zenodo.org/record/8339941/files/Mycobacterium.rep.fna.gz\"\r\nwget \"$URL\"\r\nIDS_URL=\"https://zenodo.org/record/8343322/files/mtb.ids\"\r\nwget \"$IDS_URL\"\r\n# download kraken database\r\nURL=\"https://zenodo.org/record/8339822/files/k2_Mycobacterium_20230817.tar.gz\"\r\nwget \"$URL\"\r\ntar -xzf k2_Mycobacterium_20230817.tar.gz\r\nrm k2_Mycobacterium_20230817.tar.gz\r\n```\r\n\r\n#### Classify reads\r\n\r\n**minimap2**\r\n\r\n```\r\n# nanopore\r\nminimap2 --secondary=no -c -t 4 -x map-ont -o reads.aln.paf Mycobacterium_db/Mycobacterium.rep.fna.gz reads.depleted.fq\r\n# illumina\r\nminimap2 --secondary=no -c -t 4 -x sr -o reads.aln.paf Mycobacterium_db/Mycobacterium.rep.fna.gz reads_1.depleted.fq reads_2.depleted.fq\r\n```\r\n\r\n**kraken2**\r\n\r\n```\r\n# nanopore\r\nkraken2 --db Mycobacterium_db --threads 4 --report myco.kreport --output classifications.myco.tsv reads.depleted.fq\r\n# illumina\r\nkraken2 --db Mycobacterium_db --paired --threads 4 --report myco.kreport --output classifications.myco.tsv reads_1.depleted.fq reads_2.depleted.fq\r\n```\r\n\r\n#### Extract *M. tuberculosis* reads\r\n\r\n**minimap2**\r\n\r\n```\r\n# nanopore\r\ngrep -Ff Mycobacterium_db/mtb.ids reads.aln.paf | cut -f1 | \\\r\n  seqkit grep -f - -o reads.enriched.fq reads.depleted.fq\r\n# illumina\r\ngrep -Ff Mycobacterium_db/mtb.ids reads.aln.paf | cut -f1 > keep.ids\r\nseqkit grep -f keep.ids -o reads_1.enriched.fq reads_1.depleted.fq\r\nseqkit grep -f keep.ids -o reads_2.enriched.fq reads_2.depleted.fq\r\n```\r\n\r\n**kraken2**\r\n\r\nWe'll use\r\nthe [`extract_kraken_reads.py` script](https://github.com/jenniferlu717/KrakenTools#extract_kraken_readspy)\r\nfor this\r\n\r\n```\r\n# nanopore\r\npython extract_kraken_reads.py -k classifications.myco.tsv -1 reads.depleted.fq -o reads.enriched.fq -t 1773 -r myco.kreport --include-children\r\n# illumina\r\npython extract_kraken_reads.py -k classifications.myco.tsv -1 reads_1.depleted.fq -2 reads_2.depleted.fq -o reads_1.enriched.fq -o2 reads_2.enriched.fq -t 1773 -r myco.kreport --include-children\r\n```\r\n\r\n[doi]: https://doi.org/10.1101/2023.09.18.558339 \r\n",
        "doi": "10.48546/workflowhub.workflow.700.2",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in name",
        "id": "700",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/700?version=2",
        "name": "Pangenome databases provide superior host removal and mycobacteria classification from clinical metagenomic data",
        "number_of_steps": 0,
        "projects": [
            "Pangenome database project"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-01-10",
        "versions": 2
    },
    {
        "create_time": "2024-01-08",
        "creators": [
            "Diego De Panis"
        ],
        "description": "The workflow takes a trimmed Illumina paired-end reads collection, runs Meryl to create a K-mer database, Genomescope2 to estimate genome properties and Smudgeplot to estimate ploidy. The main results are K-mer ddatabase and genome profiling plots, tables, and values useful for downstream analysis. Default K-mer length and ploidy for Genomescope are 21 and 2, respectively. ",
        "doi": null,
        "edam_operation": [
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Whole genome sequencing"
        ],
        "filtered_on": "profil.* in tags",
        "id": "698",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/698?version=1",
        "name": "ERGA Profiling Illumina v2311 (WF1)",
        "number_of_steps": 17,
        "projects": [
            "ERGA Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [
            "erga",
            "illumina",
            "name:profiling"
        ],
        "tools": [
            "Add_a_column1",
            "tp_grep_tool",
            "genomescope",
            "Convert characters1",
            "smudgeplot",
            "Cut1",
            "tp_cut_tool",
            "tp_find_and_replace",
            "param_value_from_file",
            "meryl"
        ],
        "type": "Galaxy",
        "update_time": "2024-01-08",
        "versions": 1
    },
    {
        "create_time": "2024-01-09",
        "creators": [],
        "description": "# BACPAGE\r\n\r\nThis repository contains an easy-to-use pipeline for the assembly and analysis of bacterial genomes using ONT long-read or Illumina short-read technology. \r\nRead the complete documentation and instructions for bacpage and each of its functions [here](https://cholgen.github.io/sequencing-resources/bacpage-command.html)\r\n\r\n# Introduction\r\nAdvances in sequencing technology during the COVID-19 pandemic has led to massive increases in the generation of sequencing data. Many bioinformatics tools have been developed to analyze this data, but very few tools can be utilized by individuals without prior bioinformatics training.\r\n\r\nThis pipeline was designed to encapsulate pre-existing tools to automate analysis of whole genome sequencing of bacteria. \r\nInstallation is fast and straightfoward. \r\nThe pipeline is easy to setup and contains rationale defaults, but is highly modular and configurable by more advance users.\r\nBacpage has individual commands to generate consensus sequences, perform *de novo* assembly, construct phylogenetic tree, and generate quality control reports.\r\n\r\n# Features\r\nWe anticipate the pipeline will be able to perform the following functions:\r\n- [x] Reference-based assembly of Illumina paired-end reads\r\n- [x] *De novo* assembly of Illumina paired-end reads\r\n- [ ] *De novo* assembly of ONT long reads\r\n- [x] Run quality control checks\r\n- [x] Variant calling using [bcftools](https://github.com/samtools/bcftools)\r\n- [x] Maximum-likelihood phylogenetic inference of processed samples and background dataset using [iqtree](https://github.com/iqtree/iqtree2) \r\n- [x] MLST profiling and virulence factor detection\r\n- [x] Antimicrobial resistance genes detection\r\n- [ ] Plasmid detection\r\n\r\n# Installation\r\n1. Install `mamba` by running the following two command:\r\n```commandline\r\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\r\nbash Mambaforge-$(uname)-$(uname -m).sh\r\n```\r\n\r\n2. Clone the bacpage repository:\r\n```commandline\r\ngit clone https://github.com/CholGen/bacpage.git\r\n```\r\n\r\n3. Switch to the development branch of the pipeline:\r\n```commandline\r\ncd bacpage/\r\ngit checkout -b split_into_command\r\n```\r\n\r\n3. Install and activate the pipeline's conda environment:\r\n```commandline\r\nmamba env create -f environment.yaml\r\nmamba activate bacpage\r\n```\r\n\r\n4. Install the `bacpage` command:\r\n```commandline\r\npip install .\r\n```\r\n\r\n5. Test the installation:\r\n```commandline\r\nbacpage -h\r\nbacpage version\r\n```\r\nThese command should print the help and version of the program. Please create an issue if this is not the case.\r\n\r\n# Updating\r\n\r\n1. Navigate to the directory where you cloned the bacpage repository on the command line:\r\n```commandline\r\ncd bacpage/\r\n```\r\n2. Activate the bacpage conda environment:\r\n```commandline\r\nmamba activate bacpage\r\n```\r\n3. Pull the lastest changes from GitHub:\r\n```commandline\r\ngit pull\r\n```\r\n4. Update the bacpage conda environemnt:\r\n```commandline\r\nmamba env update -f environment.yaml\r\n```\r\n5. Reinstall the `bacpage` command:\r\n```commandline\r\npip install .\r\n```\r\n\r\n# Usage\r\n0. Activate the bacpage conda environment:\r\n```commandline\r\nmamba activate bacpage\r\n```\r\n1. Create a directory specifically for the batch of samples you would like to analyze (called a project directory).\r\n```commandline\r\nbacpage setup [your-project-directory-name]\r\n```\r\n2. Place paired sequencing reads in the `input/` directory of your project directory.\r\n3. From the pipeline's directory, run the reference-based assembly pipeline on your samples using the following command:\r\n```commandline\r\nbacpage assemble [your-project-directory-name]\r\n```\r\nThis will generate a consensus sequence in FASTA format for each of your samples and place them in \r\n`<your-project-directory-name>/results/consensus_sequences/<sample>.masked.fasta`. An HTML report containing alignment \r\nand quality metrics for your samples can be found at `<your-project-directory-name>/results/reports/qc_report.html`.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid.* in description",
        "id": "695",
        "keep": "Reject",
        "latest_version": 2,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/695?version=2",
        "name": "Phylogeny reconstruction using bacpage",
        "number_of_steps": 0,
        "projects": [
            "CholGen"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Workflow Description Language",
        "update_time": "2024-01-09",
        "versions": 1
    },
    {
        "create_time": "2023-12-20",
        "creators": [],
        "description": "![bacpage](https://raw.githubusercontent.com/CholGen/bacpage/split_into_command/.github/logo_dark.png){width=500}\r\n\r\nThis repository contains an easy-to-use pipeline for the assembly and analysis of bacterial genomes using ONT long-read or Illumina short-read technology.\r\n\r\n# Introduction\r\nAdvances in sequencing technology during the COVID-19 pandemic has led to massive increases in the generation of sequencing data. Many bioinformatics tools have been developed to analyze this data, but very few tools can be utilized by individuals without prior bioinformatics training.\r\n\r\nThis pipeline was designed to encapsulate pre-existing tools to automate analysis of whole genome sequencing of bacteria. Installation is fast and straightfoward. The pipeline is easy to setup and contains rationale defaults, but is highly modular and configurable by more advance users.\r\nA successful run generates consensus sequences, typing information, phylogenetic tree, and quality control report.\r\n\r\n# Features\r\nWe anticipate the pipeline will be able to perform the following functions:\r\n- [x] Reference-based assembly of Illumina paired-end reads\r\n- [x] *De novo* assembly of Illumina paired-end reads\r\n- [ ] *De novo* assembly of ONT long reads\r\n- [x] Run quality control checks\r\n- [x] Variant calling using [bcftools](https://github.com/samtools/bcftools)\r\n- [x] Maximum-likelihood phylogenetic inference of processed samples and background dataset using [iqtree](https://github.com/iqtree/iqtree2) \r\n- [x] MLST profiling and virulence factor detection\r\n- [x] Antimicrobial resistance genes detection\r\n- [ ] Plasmid detection\r\n\r\n# Installation\r\n1. Install `miniconda` by running the following two command:\r\n```commandline\r\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\r\nbash Mambaforge-$(uname)-$(uname -m).sh\r\n```\r\n\r\n2. Clone the repository:\r\n```commandline\r\ngit clone https://github.com/CholGen/bacpage.git\r\n```\r\n\r\n3. Install and activate the pipeline's conda environment:\r\n```commandline\r\ncd bacpage/\r\nmamba env create -f environment.yaml\r\nmamba activate bacpage\r\n```\r\n\r\n4. Install the `bacpage` command:\r\n```commandline\r\npip install .\r\n```\r\n\r\n5. Test the installation:\r\n```commandline\r\nbacpage -h\r\nbacpage version\r\n```\r\nThese command should print the help and version of the program. Please create an issue if this is not the case.\r\n\r\n# Usage\r\n0. Navigate to the pipeline's directory.\r\n1. Copy the `example/` directory to create a directory specifically for each batch of samples.\r\n```commandline\r\ncp example/ <your-project-directory-name>\r\n```\r\n2. Place raw sequencing reads in the `input/` directory of your project directory.\r\n3. Record the name and absolute path of raw sequencing reads in the `sample_data.csv` found within your project directory.\r\n4. Replace the values `<your-project-directory-name>` and `<sequencing-directory>` in `config.yaml` found within your project directory, with the absolute path of your project directory and pipeline directory, respectively.\r\n5. Determine how many cores are available on your computer:\r\n```commandline\r\ncat /proc/cpuinfo | grep processor\r\n```\r\n6. From the pipeline's directory, run the entire pipeline on your samples using the following command:\r\n```commandline\r\nsnakemake --configfile <your-project-directory-name>/config.yaml --cores <cores>\r\n```\r\nThis will generate a consensus sequence in FASTA format for each of your samples and place them in `<your-project-directory-name>/results/consensus_sequences/<sample>.masked.fasta`. An HTML report containing alignment and quality metrics for your samples can be found at `<your-project-directory-name>/results/reports/qc_report.html`. A phylogeny comparing your sequences to the background dataset can be found at `<your-project-directory-name>/results/phylogeny/phylogeny.tree`\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid.* in description",
        "id": "693",
        "keep": "Reject",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/693?version=1",
        "name": "Reference-based assembly with bacpage",
        "number_of_steps": 0,
        "projects": [
            "CholGen"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Workflow Description Language",
        "update_time": "2023-12-20",
        "versions": 1
    },
    {
        "create_time": "2023-11-23",
        "creators": [
            "Phuong Doan"
        ],
        "description": "# ANNOTATO - Annotation workflow To Annotate Them Oll\r\n\r\n- [ANNOTATO - Annotation workflow To Annotate Them Oll](#annotato---annotation-workflow-to-annotate-them-oll)\r\n  - [Overview of the workflow](#overview-of-the-workflow)\r\n    - [Input data](#input-data)\r\n    - [Pipeline steps](#pipeline-steps)\r\n    - [Output data](#output-data)\r\n  - [Prerequisites](#prerequisites)\r\n  - [Installation](#installation)\r\n  - [Running ANNOTATO](#running-annotato)\r\n    - [Before running the pipeline (IMPORTANT)](#before-running-the-pipeline-important)\r\n    - [Without RNASeq and protein data](#without-rnaseq-and-protein-data)\r\n    - [Running ANNOTATO with RNASeq data](#running-annotato-with-rnaseq-data)\r\n    - [Running ANNOTATO with protein data](#running-annotato-with-protein-data)\r\n    - [Running ANNOTATO with both protein and RNASeq data](#running-annotato-with-both-protein-and-rnaseq-data)\r\n    - [Running ANNOTATO with params.json](#running-annotato-with-paramsjson)\r\n    - [Other parameters for running the analysis](#other-parameters-for-running-the-analysis)\r\n  - [Evaluating output GFF to the exon level](#evaluating-output-gff-to-the-exon-level)\r\n  - [Performance of the workflow on annotating difference eukaryote genomes](#performance-of-the-workflow-on-annotating-difference-eukaryote-genomes)\r\n  - [Future work](#future-work)\r\n\r\n## Overview of the workflow\r\n\r\nThe pipeline is based on `Funannotate` or `BRAKER` and was initially developed and tested on the two datasets:\r\n- Drosophila melanogaster: [https://doi.org/10.5281/zenodo.8013373](https://doi.org/10.5281/zenodo.8013373)\r\n- *Pocillopora* cf. *effusa*: [https://www.ncbi.nlm.nih.gov/biosample/26809107](https://www.ncbi.nlm.nih.gov/biosample/26809107)\r\n\r\nThen, it was further tested on these species during the [BioHackathon 2023 - project 20](https://github.com/elixir-europe/biohackathon-projects-2023/tree/main/20)\r\n\r\n- Helleia helle\r\n- Homo sapiens chrom 19\r\n- Melampus jaumei\r\n- Phakellia ventilabrum\r\n- Trifolium dubium\r\n\r\n### Input data\r\n\r\n- Reference genome `genome.[.fna, .fa, .fasta][.gz]`\r\n- RNAseq data listed in a metadata csv file. Input type can be mixed between long and short reads, with the option of single-end read. The input file should follow the format below:\r\n\r\n```\r\nsample_id,R1_path,R2_path,read_type\r\nSAM1,/path/to/R1,,long             # For long reads\r\nSAM2,/path/to/R1,/path/to/R2,short # For PE reads\r\nSAM3,/path/to/R1,,short            # For SE reads\r\n```\r\n\r\n- Protein sequence data in fasta format, could be gzip or not\r\n\r\n### Pipeline steps\r\n\r\n![Pipeline](./assets/images/annotato-workflow.drawio.svg)\r\n\r\nThe main pipeline is divided into five different subworkflows.\r\n- `Preprocess RNA` is where the input RNASeq data are QC and trimmed.\r\n- `Process RNA Minimap` is triggered when long reads FastQ are in the input CSV file.\r\n- `Process RNA STAR` will run when short reads FastQ are in the input CSV.\r\n- `Genome Masking` runs by default if not skipped. It assumes the input genome fasta is not masked and will run Denovo repeat masking with RepeatModeler and RepeatMasker.\r\n- `Filter Repeat` whenever there is a Denovo masking step, this sub-workflow will be triggered to remove the repeat sequences that appeared in the Uniprot Swissprot protein data. \r\n\r\n### Output data\r\n\r\n- MultiQC report for the RNASeq data, before and after trimming, mapping rate of short reads, and the BUSCO results of predicted genes.\r\n- RepeatMasker report containing quantity of masked sequence and distribution among TE families\r\n- Protein-coding gene annotation file in gff3 format\r\n- BUSCO summary of annotated sequences\r\n\r\n## Prerequisites\r\n\r\nThe following programs are required to run the workflow and the listed version were tested. \r\n\r\n`nextflow v23.04.0 or higher`\r\n\r\n`singularity`\r\n\r\n`conda` and `mamba` (currently, having problem with Funannotate and BRAKER installation)\r\n\r\n`docker` (have not been tested but in theory should work fine)\r\n\r\n## Installation\r\n\r\nSimply get the code from github or workflowhub and directly use it for the analysis with `nextflow`.\r\n\r\n```\r\ngit clone https://github.com/ERGA-consortium/pipelines/tree/main/annotation/nextflow\r\n```\r\n\r\n## Running ANNOTATO\r\n\r\n### Before running the pipeline (IMPORTANT)\r\n\r\nOne thing with Nextflow is that it is running off a Java Virtual Machine (JVM), and it will try to use all available memory for Nextflow even though it is unnecessary (for workflow management and job control). This will cause much trouble if you run a job on an HPC cluster. Thus, to minimize the effect of it, we need to limit the maximum memory the JVM can use.\r\n\r\n```\r\nexport NFX_OPTS=\"-Xms=512m -Xmx=3g\"\r\n```\r\n\r\n`-Xms` is the lower limit, which is set as 512 MB.\r\n`-Xmx` is the upper limit, which in this case is set as 3 GB.\r\nPlease modify this according to your situation.\r\n\r\n### Without RNASeq and protein data\r\n\r\nPerform the analysis with only the draft genome and busco database.\r\n\r\n```\r\nnextflow run main.nf --genome /path/to/genome.fasta --species \"Abc def\" --buscodb 'metazoa' \r\n```\r\n\r\nThe workflow will run Denovo repeat masking on the draft genome, then softmask the repeat region and use the genome to run `funannotate`. Add `--run_braker` to run the genome prediction using `BRAKER` instead.\r\n\r\n### Running ANNOTATO with RNASeq data\r\n\r\nWhen you want to let the workflow run the mapping by itself, uses `input.csv` as input with the link to all `FASTQ` file.\r\n\r\n```\r\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --rnaseq /path/to/input.csv --species \"Abc def\" --buscodb 'metazoa' \r\n```\r\n\r\nBased on the content of the `input.csv` file to trigger different RNASeq processing workflows. The output `bam` file will then be used for genome prediction.\r\n\r\nWhen reads are mapped to the reference genome, the aligned `bam` file can be used as input to the pipeline instead of the raw `FASTQ`\r\n\r\n```\r\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --short_rna_bam /path/to/shortreads.bam [--long_rna_bam /path/to/longreads.bam] --species \"Abc def\" --buscodb 'metazoa' \r\n```\r\n\r\n**ATTENTION**: One major drawback of the current workflow is that the input genome will be sorted and renamed by the `funannotate sort` function. This is because `AUGUSTUS` and `Funannotate` won't work normally when the header of the input genome is too long and contains weird characters. Therefore, if you want to provide a `bam` file as input instead of the raw `FASTQ`, please run `funannotate sort` on the genome fasta first and then use it as the reference for running alignment. Or in case your genome headers are already shorter than 16 character, please add `--skip_rename` when running the pipeline.\r\n\r\n### Running ANNOTATO with protein data\r\n\r\n```\r\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --protein /path/to/protein.fasta[.gz] --species \"Abc def\" --buscodb 'metazoa' \r\n```\r\n\r\nWhen only protein data is provided, the workflow will run denovo masking then repeat filter with the additional protein data. The masked genome and protein fasta will then be used for gene prediction.\r\n\r\n### Running ANNOTATO with both protein and RNASeq data\r\n\r\nThe full pipeline is triggered when both RNASeq data and protein fasta is provided.\r\n\r\n```\r\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --protein /path/to/protein.fasta[.gz] --rnaseq /path/to/input.csv --species \"Abc def\" --buscodb 'metazoa' \r\n```\r\n\r\n### Running ANNOTATO with params.json\r\n\r\nOne plus side with Nextflow is that it can use a parameter JSON file called `params.json` to start the analysis pipeline with all required parameters. Please modify the content of the `params.json` according to your need then run the following command.\r\n\r\n```\r\nnextflow run main.nf -params-file params.json\r\n```\r\n\r\n### Other parameters for running the analysis\r\n\r\n```\r\nCompulsory input:\r\n--genome                       Draft genome fasta file contain the assembled contigs/scaffolds\r\n--species                      Species name for the annotation pipeline, e.g. \"Drosophila melanogaster\"\r\n\r\nOptional input:\r\n--protein                      Fasta file containing known protein sequences used as an additional information for gene prediction pipeline.\r\n                               Ideally this should come from the same species and/or closely related species. [default: null]\r\n--rnaseq                       A CSV file following the pattern: sample_id,R1_path,R2_path,read_type.\r\n                               This could be generated using gen_input.py. Run `python gen_input.py --help` for more information. \r\n                               [default: null]\r\n--long_rna_bam                 A BAM file for the alignment of long reads (if any) to the draft genome. Noted that the header of the draft\r\n                               genome need to be renamed first before alignment otherwise it will causes trouble for AUGUSTUS and funannotate. \r\n                               [default: null]\r\n--short_rna_bam                A BAM file for the alignment of short reads (if any) to the draft genome. Noted that the header of the draft \r\n                               genome need to be renamed first before alignment otherwise it will causes trouble for AUGUSTUS and funannotate. \r\n                               [default: null]\r\n--knownrepeat                  Fasta file containing known repeat sequences of the species, this will be used directly for masking \r\n                               (if --skip_denovo_masking) or in combination with the denovo masking. [default: null]\r\n\r\nOutput option:\r\n--outdir                       Output directory. \r\n--tracedir                     Pipeline information. \r\n--publish_dir_mode             Option for nextflow to move data to the output directory. [default: copy]\r\n--tmpdir                       Database directory. \r\n\r\nFunannotate params:\r\n--run_funannotate              Whether to use funannotate for gene prediction. [default: true]\r\n--organism                     Fungal-specific option. Should be change to \"fungus\" if the annotated organism is fungal. [default: other]\r\n--ploidy                       Set the ploidy for gene prediction, in case of haploid, a cleaning step will be performed by funannotate to remove\r\n                               duplicated contigs/scaffold. [default: 2]\r\n--buscodb                      BUSCO database used for AUGUSTUS training and evaluation. [default: eukaryota]\r\n--buscoseed                    AUGUSTUS pre-trained species to start BUSCO. Will be override if rnaseq data is provided. [default: null]\r\n\r\nBraker params:\r\n--run_braker                   Whether to use BRAKER for gene prediction. [default: false]\r\n\r\nSkipping options:\r\n--skip_rename                  Skip renaming genome fasta file by funannotate sort. \r\n--skip_all_masking             Skip all masking processes, please be sure that your --genome input is soft-masked before triggering this \r\n                               parameter. [default: false]\r\n--skip_denovo_masking          Skip denovo masking using RepeatModeler, this option can only be run when --knownrepeat fasta is provided. \r\n                               [default: false]\r\n--skip_functional_annotation   Skip functional annotation step. [default: false]\r\n--skip_read_preprocessing      Skip RNASeq preprocessing step. [default: false]\r\n\r\nExecution/Engine profiles:\r\nThe pipeline supports profiles to run via different Executers and Engines e.g.: -profile local,conda\r\n\r\nExecuter (choose one):\r\n  local\r\n  slurm\r\n\r\nEngines (choose one):\r\n  conda\r\n  mamba\r\n  docker\r\n  singularity\r\n\r\nPer default: -profile slurm,singularity is executed.\r\n```\r\n\r\n## Evaluating output GFF to the exon level\r\n\r\nWe provided a script to analyze the output GFF of ANNOTATO (which also could be applied to the GFF file output of other pipelines) to report the number of exons per mRNA/tRNA. To run this, simply use:\r\n\r\n```\r\npython bin/analyze_exons.py -f ${GFF}\r\n```\r\n\r\nBelow is the sample output of this script\r\n\r\n```\r\nINFORMATION REGARDING mRNA\r\nNumber of transcripts: 33086\r\nLargest number of exons in all transcripts: 128\r\nMonoexonic transcripts: 4085\r\nMultiexonic transcripts: 29001\r\nMono:Mult Ratio: 0.14\r\nBoxplot of number of exons per transcript:\r\nMin: 1\r\n25%: 2\r\n50%: 4\r\n75%: 8\r\nMax: 128\r\nMean: 6.978812790908542\r\n==================================================\r\nINFORMATION REGARDING tRNA\r\nNumber of transcripts: 2017\r\nLargest number of exons in all transcripts: 1\r\nMonoexonic transcripts: 2017\r\nMultiexonic transcripts: 0\r\nNo multiexonic transcripts, unable to calculate Mono:Mult Ratio\r\nBoxplot of number of exons per transcript:\r\nMin: 1\r\n25%: 1\r\n50%: 1\r\n75%: 1\r\nMax: 1\r\nMean: 1.0\r\n==================================================\r\n```\r\n\r\nThis script was originally written by [Katharina Hoff](https://github.com/Gaius-Augustus/GALBA/blob/main/scripts/analyze_exons.py) and was modified accordingly to suit the analysis of GFF file.\r\n\r\n## Performance of the workflow on annotating difference eukaryote genomes\r\n\r\nThe following table is the result predicted by ANNOTATO on difference species during the [Europe BioHackathon 2023](https://github.com/elixir-europe/biohackathon-projects-2023/tree/main/20).\r\n\r\n| Species                    | Genome size | N.Genes | N.Exons | N.mRNA | BUSCO lineage | BUSCO score                             | OMArk Completeness                                                 | OMArk Consistency                                                                       |\r\n| :---:                      | :---:       | :---:   | :---:   | :---:  | :---:         | :---:                                   | :---:                                                              | :---:                                                                                   |\r\n| Drosophila melanogaster    | 143M        | 14,753  | 57,343  | 14,499 | diptera       | C:96.1%[S:95.6%,D:0.5%],F:1.2%,M:2.7%   | melanogaster subgroup, C:90.38%[S:84.32%,D:6.06%],M:9.62%,,n:12442 | A:94.21%[P:4.05%,F:7.28%],I:1.61%[P:0.5%,F:0.42%],C:0.00%[P:0.00%,F:0.00%],U:4.19%      |\r\n| Helleia helle              | 547M        | 37,367  | 139,302 | 28,445 | lepidoptera   | C:74.6%[S:73.4%,D:1.2%],F:5.4%,M:20.0%  | Papilionidea, C:82.04%[S:66.12%,D:15.92%],M:17.96%, n:7939         | A:44.78%[P:14.41%,F:6.02%],I:3.53%[P:2.1%,F:0.7%],C:0.00%[P:0.00%,F:0.00%],U:51.69%     |\r\n| Homo sapiens chrom 19      | 58M         | 1,872   | 11,937  | 1,862  | primates      | C:5.0%[S:4.8%,D:0.2%],F:0.5%,M:94.5%    | Hominidae, C:8.57%[S:7.74%,D:0.83%],M:91.43%, n=17843              | A:87.54%[P:12.73%,F:13.1%],I:4.78%[P:1.5%,F:2.04%],C:0.00%[P:0.00%,F:0.00%],U:7.68%     |\r\n| Melampus jaumei            | 958M        | 61,128  | 335,483 | 60,720 | mollusca      | C:80.4%[S:67.2%,D:13.2%],F:3.8%,M:15.8% | Lophotrochozoa, C: 92.5%[S: 66.29%, D: 26.21%], M:7.5%, n:2373     | A:41.45%[P:15.72%,F:9.97%],I:15.97%[P:10.68%,F:3.07%],C:0.00%[P:0.00%,F:0.00%],U:42.57% |\r\n| Phakellia ventilabrum      | 186M        | 19,073  | 157,441 | 18,855 | metazoa       | C:80.9%[S:79.2%,D:1.7%],F:6.5%,M:12.6%  | Metazoa, C:86.79%[S:76.9%,D:9.9%],M:13.21% , n:3021                | A:53.81%[P:18.92%,F:5.06%],I:5.0%[P:2.7%,F:0.68%],C:0.00%[P:0.00%,F:0.00%],U:41.19%     |\r\n| *Pocillopora* cf. *effusa* | 347M        | 35,103  | 230,901 | 33,086 | metazoa       | C:95.1%[S:92.2%,D:2.9%],F:1.7%,M:3.2%   | Eumetazoa, C:94.16%[S:84.3%,D:9.86%],M:5.84%,n:3255                | A:52.94%[P:22.30%,F:3.69%],I:3.44%[P:2.08%,F:0.28%],C:0.00%[P:0.00%,F:0.00%],U:43.62%   |\r\n| Trifolium dubium           | 679M        | 78,810  | 354,662 | 77,763 | fabales       | C:95.1%[S:19.5%,D:75.6%],F:1.5%,M:3.4%  | NPAAA clade, C:94.58%[S:19.21%,D:75.38%],M:5.42%,n:15412           | A:71.99%[P:11.03%,F:6.63%],I:2.77%[P:1.66%,F:0.52%],C:0.00%[P:0.00%,F:0.00%],U:25.23%   |\r\n\r\n## Future work\r\n- Python wrapper function to remove intermediate files\r\n- Adding functional annotation with `Interproscan` and `eggnog`\r\n- Adding PASA results to further improve the accuracy of the training\r\n- Adding custom parameter for both `BRAKER` and `funannotate`",
        "doi": "10.48546/workflowhub.workflow.654.2",
        "edam_operation": [],
        "edam_topic": [
            "Gene structure",
            "Gene transcripts",
            "Genomics"
        ],
        "filtered_on": "profil.* in description",
        "id": "654",
        "keep": "Reject",
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/654?version=2",
        "name": "ANNOTATO - ERGA Genome Annotation Workflow in Nextflow",
        "number_of_steps": 0,
        "projects": [
            "ERGA Annotation",
            "Bioinformatics Laboratory for Genomics and Biodiversity (LBGB)"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "bge",
            "biodiversity",
            "bioinformatics",
            "de_novo",
            "erga",
            "genomics",
            "nextflow",
            "transcriptomics",
            "workflows",
            "rna-seq"
        ],
        "tools": [
            "funannotate",
            "BRAKER1",
            "Minimap2",
            "STAR",
            "RepeatMasker",
            "RepeatModeler",
            "BLAST",
            "StringTie",
            "FastQC",
            "fastp",
            "BUSCO",
            "MultiQC",
            "protexcluder"
        ],
        "type": "Nextflow",
        "update_time": "2023-11-24",
        "versions": 2
    },
    {
        "create_time": "2023-10-26",
        "creators": [
            "Valentine Murigneux",
            "Mike Thang",
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut"
        ],
        "description": "The aim of this workflow is to handle the routine part of shotgun metagenomics data processing on Galaxy Australia. \r\n\r\nThe workflow is using the tools MetaPhlAn2 for taxonomy classification and HUMAnN2 for functional profiling of the metagenomes. The workflow is based on the Galaxy Training tutorial 'Analyses of metagenomics data - The global picture' (Saskia Hiltemann, B\u00e9r\u00e9nice Batut) https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/general-tutorial/tutorial.html#shotgun-metagenomics-data. \r\n\r\nThe how-to guide is available here: https://vmurigneu.github.io/shotgun_howto_ga_workflows/\r\n",
        "doi": "10.48546/workflowhub.workflow.624.1",
        "edam_operation": [
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomic sequencing",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "624",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/624?version=1",
        "name": "Analyses of shotgun metagenomics data with MetaPhlAn2",
        "number_of_steps": 17,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gucfg2galaxy",
            "metagenomics",
            "shotgun"
        ],
        "tools": [
            "",
            "metaphlan2",
            "humann2_regroup_table",
            "Cut1",
            "merge_metaphlan_tables",
            "taxonomy_krona_chart",
            "humann2",
            "metaphlan2krona",
            "humann2_renorm_table"
        ],
        "type": "Galaxy",
        "update_time": "2024-04-05",
        "versions": 1
    },
    {
        "create_time": "2023-11-14",
        "creators": [
            "Damon-Lee Pointon",
            "Will Eagles",
            "Ying Sims",
            "Matthieu Muffato",
            "Priyanka Surana"
        ],
        "description": "[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.10.1-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/treeval)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/treeval** is a bioinformatics best-practice analysis pipeline for the generation of data supplemental to the curation of reference quality genomes. This pipeline has been written to generate flat files compatible with [JBrowse2](https://jbrowse.org/jb2/).\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nThe treeval pipeline has a sister pipeline currently named [curationpretext](https://github.com/sanger-tol/curationpretext) which acts to regenerate the pretext maps and accessory files during genomic curation in order to confirm interventions. This pipeline is sufficiently different to the treeval implementation that it is written as it's own pipeline.\r\n\r\n1. Parse input yaml ( YAML_INPUT )\r\n2. Generate my.genome file ( GENERATE_GENOME )\r\n3. Generate insilico digests of the input assembly ( INSILICO_DIGEST )\r\n4. Generate gene alignments with high quality data against the input assembly ( GENE_ALIGNMENT )\r\n5. Generate a repeat density graph ( REPEAT_DENSITY )\r\n6. Generate a gap track ( GAP_FINDER )\r\n7. Generate a map of self complementary sequence ( SELFCOMP )\r\n8. Generate syntenic alignments with a closely related high quality assembly ( SYNTENY )\r\n9. Generate a coverage track using PacBio data ( LONGREAD_COVERAGE )\r\n10. Generate HiC maps, pretext and higlass using HiC cram files ( HIC_MAPPING )\r\n11. Generate a telomere track based on input motif ( TELO_FINDER )\r\n12. Run Busco and convert results into bed format ( BUSCO_ANNOTATION )\r\n13. Ancestral Busco linkage if available for clade ( BUSCO_ANNOTATION:ANCESTRAL_GENE )\r\n\r\n## Usage\r\n\r\n> **Note**\r\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\r\n> to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\r\n> with `-profile test` before running the workflow on actual data.\r\n\r\nCurrently, it is advised to run the pipeline with docker or singularity as a small number of major modules do not currently have a conda env associated with them.\r\n\r\nNow, you can run the pipeline using:\r\n\r\n```bash\r\n# For the FULL pipeline\r\nnextflow run main.nf -profile singularity --input treeval.yaml --outdir {OUTDIR}\r\n\r\n# For the RAPID subset\r\nnextflow run main.nf -profile singularity --input treeval.yaml -entry RAPID --outdir {OUTDIR}\r\n```\r\n\r\nAn example treeval.yaml can be found [here](assets/local_testing/nxOscDF5033.yaml).\r\n\r\nFurther documentation about the pipeline can be found in the following files: [usage](https://pipelines.tol.sanger.ac.uk/treeval/dev/usage), [parameters](https://pipelines.tol.sanger.ac.uk/treeval/dev/parameters) and [output](https://pipelines.tol.sanger.ac.uk/treeval/dev/output).\r\n\r\n> **Warning:**\r\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\r\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\r\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\r\n\r\n## Credits\r\n\r\nsanger-tol/treeval has been written by Damon-Lee Pointon (@DLBPointon), Yumi Sims (@yumisims) and William Eagles (@weaglesBio).\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n<ul>\r\n  <li>@gq1 - For building the infrastructure around TreeVal and helping with code review</li>\r\n  <li>@ksenia-krasheninnikova - For help with C code implementation and YAML parsing</li>\r\n  <li>@mcshane - For guidance on algorithms </li>\r\n  <li>@muffato - For code reviews and code support</li>\r\n  <li>@priyanka-surana - For help with the majority of code reviews and code support</li>\r\n</ul>\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\n## Citations\r\n\r\n<!--TODO: Citation-->\r\n\r\nIf you use sanger-tol/treeval for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX).\r\n\r\n### Tools\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nYou can cite the `nf-core` publication as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "668",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/668?version=1",
        "name": "sanger-tol/treeval v1.0 - Ancient Atlantis",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Assembly"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2023-11-14",
        "creators": [
            "Matthieu Muffato",
            "Priyanka Surana"
        ],
        "description": "# ![sanger-tol/ensemblrepeatdownload](docs/images/sanger-tol-ensemblrepeatdownload_logo.png)\r\n\r\n[![GitHub Actions CI Status](https://github.com/sanger-tol/ensemblrepeatdownload/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/ensemblrepeatdownload/actions?query=workflow%3A%22nf-core+CI%22)\r\n\r\n<!-- [![GitHub Actions Linting Status](https://github.com/sanger-tol/ensemblrepeatdownload/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/ensemblrepeatdownload/actions?query=workflow%3A%22nf-core+linting%22) -->\r\n\r\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7183380-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7183380)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-SangerTreeofLife%20%23pipelines-4A154B?labelColor=000000&logo=slack)](https://SangerTreeofLife.slack.com/channels/pipelines)\r\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40sangertol-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/sangertol)\r\n[![Watch on YouTube](http://img.shields.io/badge/youtube-tree--of--life-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/channel/UCFeDpvjU58SA9V0ycRXejhA)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/ensemblrepeatdownload** is a pipeline that downloads repeat annotations from Ensembl into a Tree of Life directory structure.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the GitHub CI infrastructure. This ensures that the pipeline runs in a third-party environment, and has sensible resource allocation defaults set to run on real-world datasets.\r\n\r\n## Pipeline summary\r\n\r\n## Overview\r\n\r\nThe pipeline takes a CSV file that contains assembly accession number, Ensembl species names (as they may differ from Tree of Life ones !), output directories.\r\nAssembly accession numbers are optional too. If missing, the pipeline assumes it can be retrieved from files named `ACCESSION` in the standard location on disk.\r\nThe pipeline downloads the repeat annotation as the masked Fasta file and a BED file.\r\nAll files are compressed with `bgzip`, and indexed with `samtools faidx` or `tabix`.\r\n\r\nSteps involved:\r\n\r\n- Download the masked fasta file from Ensembl.\r\n- Extract the coordinates of the masked regions into a BED file.\r\n- Compress and index the BED file with `bgzip` and `tabix`.\r\n\r\n## Quick Start\r\n\r\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.04.0`)\r\n\r\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\r\n\r\n3. Download the pipeline and test it on a minimal dataset with a single command:\r\n\r\n   ```bash\r\n   nextflow run sanger-tol/ensemblrepeatdownload -profile test,YOURPROFILE --outdir <OUTDIR>\r\n   ```\r\n\r\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\r\n\r\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\r\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\r\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\r\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\r\n\r\n4. Start running your own analysis!\r\n\r\n   ```console\r\n   nextflow run sanger-tol/ensemblrepeatdownload --input $PWD/assets/samplesheet.csv --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\r\n   ```\r\n\r\n## Documentation\r\n\r\nThe sanger-tol/ensemblrepeatdownload pipeline comes with documentation about the pipeline [usage](docs/usage.md) and [output](docs/output.md).\r\n\r\n## Credits\r\n\r\nsanger-tol/ensemblrepeatdownload was originally written by @muffato.\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/ensemblrepeatdownload/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/ensemblrepeatdownload for your analysis, please cite it using the following doi: [10.5281/zenodo.7183380](https://doi.org/10.5281/zenodo.7183380)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "667",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/667?version=1",
        "name": "sanger-tol/ensemblrepeatdownload v1.0.0 - Gwaihir the Windlord",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2023-11-14",
        "creators": [
            "Matthieu Muffato",
            "Priyanka Surana"
        ],
        "description": "# ![sanger-tol/ensemblgenedownload](docs/images/sanger-tol-ensemblgenedownload_logo.png)\r\n\r\n[![GitHub Actions CI Status](https://github.com/sanger-tol/ensemblgenedownload/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/ensemblgenedownload/actions?query=workflow%3A%22nf-core+CI%22)\r\n\r\n<!-- [![GitHub Actions Linting Status](https://github.com/sanger-tol/ensemblgenedownload/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/ensemblgenedownload/actions?query=workflow%3A%22nf-core+linting%22) -->\r\n\r\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7183206-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7183206)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-SangerTreeofLife%20%23pipelines-4A154B?labelColor=000000&logo=slack)](https://SangerTreeofLife.slack.com/channels/pipelines)\r\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40sangertol-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/sangertol)\r\n[![Watch on YouTube](http://img.shields.io/badge/youtube-tree--of--life-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/channel/UCFeDpvjU58SA9V0ycRXejhA)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/ensemblgenedownload** is a pipeline that downloads gene annotations from Ensembl into the Tree of Life directory structure.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the GitHub CI infrastructure. This ensures that the pipeline runs in a third-party environment, and has sensible resource allocation defaults set to run on real-world datasets.\r\n\r\n## Pipeline summary\r\n\r\n## Overview\r\n\r\nThe pipeline takes a CSV file that contains assembly accession number, Ensembl species names (as they may differ from Tree of Life ones !), output directories, and geneset versions.\r\nAssembly accession numbers are optional. If missing, the pipeline assumes it can be retrieved from files named `ACCESSION` in the standard location on disk.\r\nThe pipeline downloads the Fasta files of the genes (cdna, cds, and protein sequences) as well as the GFF3 file.\r\nAll files are compressed with `bgzip`, and indexed with `samtools faidx` or `tabix`.\r\n\r\nSteps involved:\r\n\r\n- Download from Ensembl the GFF3 file, and the sequences of the genes in\r\n  Fasta format.\r\n- Compress and index all Fasta files with `bgzip`, `samtools faidx`, and\r\n  `samtools dict`.\r\n- Compress and index the GFF3 file with `bgzip` and `tabix`.\r\n\r\n## Quick Start\r\n\r\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.04.0`)\r\n\r\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\r\n\r\n3. Download the pipeline and test it on a minimal dataset with a single command:\r\n\r\n   ```bash\r\n   nextflow run sanger-tol/ensemblgenedownload -profile test,YOURPROFILE --outdir <OUTDIR>\r\n   ```\r\n\r\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\r\n\r\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\r\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\r\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\r\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\r\n\r\n4. Start running your own analysis!\r\n\r\n   ```console\r\n   nextflow run sanger-tol/ensemblgenedownload --input $PWD/assets/samplesheet.csv --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\r\n   ```\r\n\r\n## Documentation\r\n\r\nThe sanger-tol/ensemblgenedownload pipeline comes with documentation about the pipeline [usage](docs/usage.md) and [output](docs/output.md).\r\n\r\n## Credits\r\n\r\nsanger-tol/ensemblgenedownload was originally written by @muffato.\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/ensemblgenedownload/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/ensemblgenedownload for your analysis, please cite it using the following doi: [10.5281/zenodo.7183206](https://doi.org/10.5281/zenodo.7183206)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "666",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/666?version=1",
        "name": "sanger-tol/insdcdownload v1.0.1 - Hefty m\u00fbmakil",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2023-11-02",
        "creators": [
            "Matthieu Muffato",
            "Priyanka Surana"
        ],
        "description": "# ![sanger-tol/insdcdownload](docs/images/sanger-tol-insdcdownload_logo.png)\r\n\r\n[![GitHub Actions CI Status](https://github.com/sanger-tol/insdcdownload/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/insdcdownload/actions?query=workflow%3A%22nf-core+CI%22)\r\n\r\n<!-- [![GitHub Actions Linting Status](https://github.com/sanger-tol/insdcdownload/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/insdcdownload/actions?query=workflow%3A%22nf-core+linting%22) -->\r\n\r\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7155119-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7155119)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.04.0-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n\r\n[![Get help on Slack](http://img.shields.io/badge/slack-SangerTreeofLife%20%23pipelines-4A154B?labelColor=000000&logo=slack)](https://SangerTreeofLife.slack.com/channels/pipelines)\r\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40sangertol-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/sangertol)\r\n[![Watch on YouTube](http://img.shields.io/badge/youtube-tree--of--life-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/channel/UCFeDpvjU58SA9V0ycRXejhA)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/insdcdownload** is a pipeline that downloads assemblies from INSDC into a Tree of Life directory structure.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the GitHub CI infrastructure. This ensures that the pipeline runs in a third-party environment, and has sensible resource allocation defaults set to run on real-world datasets.\r\n\r\n## Pipeline summary\r\n\r\n## Overview\r\n\r\nThe pipeline takes an assembly accession number, as well as the assembly name, and downloads it. It also builds a set of common indices (such as `samtools faidx`), and extracts the repeat-masking performed by the NCBI.\r\n\r\nSteps involved:\r\n\r\n- Download from the NCBI the genomic sequence (Fasta) and the assembly\r\n  stats and reports files.\r\n- Turn the masked Fasta file into an unmasked one.\r\n- Compress and index all Fasta files with `bgzip`, `samtools faidx`, and\r\n  `samtools dict`.\r\n- Generate the `.sizes` file usually required for conversion of data\r\n  files to UCSC's \"big\" formats, e.g. bigBed.\r\n- Extract the coordinates of the masked regions into a BED file.\r\n- Compress and index the BED file with `bgzip` and `tabix`.\r\n\r\n## Quick Start\r\n\r\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.04.0`)\r\n\r\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\r\n\r\n3. Download the pipeline and test it on a minimal dataset with a single command:\r\n\r\n   ```bash\r\n   nextflow run sanger-tol/insdcdownload -profile test,YOURPROFILE --outdir <OUTDIR>\r\n   ```\r\n\r\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\r\n\r\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\r\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\r\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\r\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\r\n\r\n4. Start running your own analysis!\r\n\r\n   ```console\r\n   nextflow run sanger-tol/insdcdownload --assembly_accession GCA_927399515.1 --assembly_name gfLaeSulp1.1 --outdir results\r\n   ```\r\n\r\n## Documentation\r\n\r\nThe sanger-tol/insdcdownload pipeline comes with documentation about the pipeline [usage](docs/usage.md) and [output](docs/output.md).\r\n\r\n## Credits\r\n\r\nsanger-tol/insdcdownload was mainly written by @muffato, with major borrowings from @priyanka-surana's [read-mapping](https://github.com/sanger-tol/readmapping) pipeline, e.g. the script to remove the repeat-masking, and the overall structure and layout of the sub-workflows.\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/insdcdownload/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/insdcdownload for your analysis, please cite it using the following doi: [10.5281/zenodo.7155119](https://doi.org/10.5281/zenodo.7155119)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "638",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/638?version=1",
        "name": "sanger-tol/insdcdownload v1.1.0 - Deciduous ent",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2023-11-14",
        "creators": [
            " Priyanka Surana"
        ],
        "description": "# ![sanger-tol/readmapping](docs/images/sanger-tol-readmapping_logo.png)\r\n\r\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.6563577-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.6563577)\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.10.1-23aa62.svg)](https://www.nextflow.io/)\r\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\r\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\r\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\r\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/readmapping)\r\n\r\n## Introduction\r\n\r\n**sanger-tol/readmapping** is a bioinformatics best-practice analysis pipeline for mapping reads generated using Illumina, HiC, PacBio and Nanopore technologies against a genome assembly.\r\n\r\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\nOn merge to `dev` and `main` branch, automated continuous integration tests run the pipeline on a full-sized dataset on the Wellcome Sanger Institute HPC farm using the Nextflow Tower infrastructure. This ensures that the pipeline runs on full sized datasets, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources.\r\n\r\n## Pipeline summary\r\n\r\n<img src=\"https://raw.githubusercontent.com/sanger-tol/readmapping/976525ad7b5327607a049aa85bbca36a48c6ba48/docs/images/sanger-tol-readmapping_workflow.png\" height=\"700\">\r\n\r\n## Quick Start\r\n\r\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.10.1`)\r\n\r\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\r\n\r\n3. Download the pipeline and test it on a minimal dataset with a single command:\r\n\r\n   ```bash\r\n   nextflow run sanger-tol/readmapping -profile test,YOURPROFILE --outdir <OUTDIR>\r\n   ```\r\n\r\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\r\n\r\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\r\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\r\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\r\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\r\n\r\n4. Start running your own analysis!\r\n\r\n   ```bash\r\n   nextflow run sanger-tol/readmapping --input samplesheet.csv --fasta genome.fa.gz --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\r\n   ```\r\n\r\n## Credits\r\n\r\nsanger-tol/readmapping was originally written by [Priyanka Surana](https://github.com/priyanka-surana).\r\n\r\nWe thank the following people for their extensive assistance in the development of this pipeline:\r\n\r\n- [Matthieu Muffato](https://github.com/muffato) for the text logo\r\n- [Guoying Qi](https://github.com/gq1) for being able to run tests using Nf-Tower and the Sanger HPC farm\r\n\r\n## Contributions and Support\r\n\r\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\r\n\r\nFor further information or help, don't hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/readmapping/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\r\n\r\n## Citations\r\n\r\nIf you use sanger-tol/readmapping for your analysis, please cite it using the following doi: [10.5281/zenodo.6563577](https://doi.org/10.5281/zenodo.6563577)\r\n\r\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\r\n\r\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "665",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/665?version=1",
        "name": "sanger-tol/readmapping v1.1.0 - Hebridean Black",
        "number_of_steps": 0,
        "projects": [
            "Tree of Life Genome Analysis"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2021-02-12",
        "creators": [
            "David F. Nieuwenhuijse",
            " Alexey Sokolov"
        ],
        "description": "A workflow for mapping and consensus generation of SARS-CoV2 whole genome amplicon nanopore data implemented in the Nextflow framework. Reads are mapped to a reference genome using Minimap2 after trimming the amplicon primers with a fixed length at both ends of the amplicons using Cutadapt. The consensus is called using Pysam based on a majority read support threshold per position of the Minimap2 alignment and positions with less than 30x coverage are masked using \u2018N\u2019.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "104",
        "keep": "Reject",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/104?version=1",
        "name": "ENA SARS-CoV-2 Nanopore Amplicon Sequencing Analysis Workflow",
        "number_of_steps": 0,
        "projects": [
            "SARS-CoV-2 Data Hubs"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-24",
        "versions": 1
    },
    {
        "create_time": "2023-11-14",
        "creators": [
            "David Yuan"
        ],
        "description": "# covid-sequence-analysis-workflow\r\n\r\nThis is the official repository of the SARS-CoV-2 variant surveillance pipeline developed by Danish Technical University (DTU), Eotvos Lorand University (ELTE), EMBL-EBI, Erasmus Medical Center (EMC) under the [Versatile Emerging infectious disease Observatory (VEO)](https://www.globalsurveillance.eu/projects/veo-versatile-emerging-infectious-disease-observatory) project. The project consists of 20 European partners. It is funded by the European Commission.\r\n\r\nThe pipeline has been integrated on EMBL-EBI infrastructure to automatically process raw SARS-CoV-2 read data, presenting in the COVID-19 Data Portal: https://www.covid19dataportal.org/sequences?db=sra-analysis-covid19&size=15&crossReferencesOption=all#search-content.\r\n\r\n## Architecture\r\n\r\nThe pipeline supports sequence reads from both Illumina and Nanopore platforms. It is designed to be highly portable for both Google Cloud Platform and High Performance Computing cluster with IBM Spectrum LSF. We have performed secondary and tertiary analysis on millions of public samples. The pipeline shows good performance for large scale production. \r\n\r\n![Component diagram](doc/img/pipeline.components.png)\r\n\r\nThe pipeline takes SRA from the public FTP from ENA. It submits analysis objects back to ENA on the fly. The intermediate results and logs are stored in the cloud storage buckets or high performance local POSIX file system. The metadata is stored in Google BigQuery for metadata and status tracking and analysis. The runtime is created with Docker / Singularity containers and NextFlow. \r\n\r\n## Process to run the pipelines\r\n\r\nThe pipeline requires the Nextflow Tower for the application level monitoring. A free test account can be created for evaluation purposes at https://tower.nf/.\r\n\r\n### Preparation\r\n\r\n1. Store `export TOWER_ACCESS_TOKEN='...'` in `$HOME/.bash_profile`. Restart the current session or source the updated `$HOME/.bash_profile`.\r\n2. Run `git clone https://github.com/enasequence/covid-sequence-analysis-workflow`.\r\n3. Create `./covid-sequence-analysis-workflow/data/projects_accounts.csv` with submission_account_id and submission_passwor, for example:\r\n>  project_id,center_name,meta_key,submission_account_id,submission_password,ftp_password\r\n>  PRJEB45555,\"European Bioinformatics Institute\",public,,,\r\n\r\n### Running pipelines\r\n\r\n1. Run `./covid-sequence-analysis-workflow/init.sra_index.sh` to initialize or reinitialize the metadata in BigQuery.\r\n2. Run `./covid-sequence-analysis-workflow/./start.lsf.jobs.sh` with proper parameters to start the batch jobs on LSF or `./covid-sequence-analysis-workflow/./start.gls.jobs.sh` with proper parameters to start the batch jobs on GCP.\r\n\r\n### Error handling\r\n\r\nIf a job is killed or died, run the following to update the metadata to avoid reprocessing samples completed successfully.\r\n\r\n1. Run `./covid-sequence-analysis-workflow/update.receipt.sh <batch_id>` to collect the submission receipts and to update submission metadata. The script can be run at anytime. It needs to be run if a batch job is killed instead of completed for any reason.\r\n2. Run `./covid-sequence-analysis-workflow/set.archived.sh` to update stats for analyses submitted. The script can be run at anytime. It needs to be run at least once before ending a snapshot to make sure that the stats are up-to-date.\r\n\r\nTo reprocess the samples failed, delete the record in `sra_processing`.\r\n",
        "doi": "10.48546/workflowhub.workflow.664.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "664",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/664?version=1",
        "name": "covid-sequence-analysis-workflow",
        "number_of_steps": 0,
        "projects": [
            "SARS-CoV-2 Data Hubs"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "sars-cov-2",
            "pathogen"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-14",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Yvan Le Bras",
            "Coline Royaux"
        ],
        "description": "Galaxy Workflow created on Galaxy-E european instance, ecology.usegalaxy.eu, related to the Galaxy training tutorial \"[Metabarcoding/eDNA through Obitools](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/Obitools-metabarcoding/tutorial.html)\" .\r\n\r\nThis workflow allows to analyze DNA metabarcoding / eDNA data produced on Illumina sequencers using the OBITools.",
        "doi": "10.48546/workflowhub.workflow.655.1",
        "edam_operation": [
            "DNA barcoding"
        ],
        "edam_topic": [
            "Biodiversity",
            "Ecology",
            "Genetics",
            "Phylogenetics"
        ],
        "filtered_on": "metabar.* in name",
        "id": "655",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/655?version=1",
        "name": "Obitools eDNA metabarcoding",
        "number_of_steps": 23,
        "projects": [
            "PNDB"
        ],
        "source": "WorkflowHub",
        "tags": [
            "biodiversity",
            "ecology"
        ],
        "tools": [
            "obi_grep",
            "obi_ngsfilter",
            "obi_annotate",
            "seq_filter_by_id",
            "obi_illumina_pairend",
            "obi_uniq",
            "ncbi_blastn_wrapper",
            "fastqc",
            "Cut1",
            "obi_tab",
            "obi_stat",
            "fastq_groomer",
            "join1",
            "Filter1",
            "obi_clean",
            "wc_gnu",
            "unzip"
        ],
        "type": "Galaxy",
        "update_time": "2024-12-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for pipeline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "653",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/653?version=1",
        "name": "Workflow 7 : Beta Diversity [16S Microbial Analysis With Mothur]",
        "number_of_steps": 4,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_tree_shared",
            "mothur_dist_shared",
            "newick_display",
            "mothur_heatmap_sim"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for pipeline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "652",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/652?version=1",
        "name": "Workflow 6: Alpha Diversity [16S Microbial Analysis With Mothur]",
        "number_of_steps": 3,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_summary_single",
            "mothur_rarefaction_single",
            "XY_Plot_1"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for pipeline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "651",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/651?version=1",
        "name": "Workflow 5: OTU Clustering [16S Microbial Analysis With Mothur]",
        "number_of_steps": 5,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_sub_sample",
            "mothur_count_groups",
            "mothur_classify_otu",
            "mothur_cluster_split",
            "mothur_make_shared"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for pipeline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "650",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/650?version=1",
        "name": "Workflow 3: Classification [Galaxy Training: 16S Microbial Analysis With Mothur]",
        "number_of_steps": 2,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_remove_lineage",
            "mothur_classify_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia  Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "16S Microbial Analysis with mothur (short)\r\n\r\nThe workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for piepline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "648",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/648?version=1",
        "name": "Workflow 1: Further Quality Control [16S Microbial Analysis With Mothur]",
        "number_of_steps": 5,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_summary_seqs",
            "mothur_screen_seqs",
            "mothur_count_seqs",
            "mothur_unique_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-11-09",
        "creators": [
            "Saskia Hiltemann",
            "B\u00e9r\u00e9nice Batut",
            "Dave Clements",
            "Ahmed Mehdi"
        ],
        "description": "The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, B\u00e9r\u00e9nice Batut, Dave Clements), adapted for piepline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \r\n\r\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "649",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/649?version=1",
        "name": "Workflow 2: Data Cleaning And Chimera Removal [16S Microbial Analysis With Mothur]",
        "number_of_steps": 9,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "mothur_screen_seqs",
            "mothur_filter_seqs",
            "mothur_chimera_vsearch",
            "mothur_remove_seqs",
            "mothur_summary_seqs",
            "mothur_pre_cluster",
            "mothur_unique_seqs"
        ],
        "type": "Galaxy",
        "update_time": "2023-11-09",
        "versions": 1
    },
    {
        "create_time": "2023-10-22",
        "creators": [
            "Yasmmin Martins"
        ],
        "description": "## Summary\r\n\r\nThe PPI information aggregation pipeline starts getting all the datasets in [GEO](https://www.ncbi.nlm.nih.gov/geo/) database whose material was generated using expression profiling by high throughput sequencing. From each database identifiers, it extracts the supplementary files that had the counts table. Once finishing the download step, it identifies those that were normalized or had the raw counts to normalize.  It also identify and map the gene ids to uniprot (the ids found usually were from HGNC and Ensembl). For each normalized counts table belonging to some experiment, il filters those which have the proteins (already mapped from HGNC to Uniprot identifiers) in the pairs in evaluation. Then, it calculates the correlation matrix based on Pearson method in the tables and saves the respective pairs correlation value for each table. Finally, a repor is made for each pair in descending order of correlation value with the experiment identifiers.\r\n\r\n## Requirements:\r\n* Python packages needed:\r\n\t- os\r\n\t- scipy\r\n\t- pandas\r\n\t- sklearn\r\n\t- Bio python\r\n\t- numpy\r\n\r\n## Usage Instructions\r\n* Preparation:\r\n\t1. ````git clone https://github.com/YasCoMa/PipeAggregationInfo.git````\r\n\t2. ````cd PipeAggregationInfo````\r\n\t3. ````pip3 install -r requirements.txt````\r\n\r\n### Preprocessing pipeline\r\n* Go to the ncbi [GDS database webpage](https://www.ncbi.nlm.nih.gov/gds), use the key words to filter your gds datasets of interest and save the results as file (\"Send to\" option), and choose \"Summary (text)\"\r\n* Alternatively, we already saved the results concerning protein interactions, you may use them to run preprocessing in order to obtain the necessary files for the main pipeline\r\n* Running preprocessing:\r\n    - ````cd preprocessing````\r\n    - ````python3 data_preprocessing.py ./workdir_preprocessing filter_files````\r\n    - ````cd ../````\r\n    - Copy the generated output folder \"data_matrices_count\" into the workflow folder: ````cp -R preprocessing/workdir_preprocessing/data_matrices_count .````\r\n\r\n### Main pipeline\r\n\r\n* Pipeline parameters:\r\n\t- __-rt__ or __--running_type__ <br>\r\n\t\tUse to indicate the step you want to execute (it is desirable following the order): <br>\r\n\t\t1 - Make the process of finding the experiments and ranking them by correlation <br>\r\n\t\t2 - Select pairs that were already processed and ranked making a separated folder of interest\r\n\r\n\t- __-fo__ or __--folder__ <br>\r\n\t\tFolder to store the files (use the folder where the other required file can be found)\r\n\t\r\n\t- __-if__ or __--interactome_file__ <br>\r\n\t\tFile with the pairs (two columns with uniprot identifiers in tsv format)<br>\r\n\t\t\r\n\t\tExample of this file: running_example/all_pairs.tsv\r\n\r\n\t- __-spf__ or __--selected_pairs_file__ <br>\r\n\t\tFile with PPIs of interest (two columns with uniprot identifiers in tsv format)<br>\r\n\t\t\r\n\t\tExample of this file: running_example/selected_pairs.tsv\r\n\r\n* Running modes examples:\r\n\t1. Run step 1: <br>\r\n\t````python3 pipeline_expression_pattern.py -rt 1 -fo running_example/ -if all_pairs.tsv ````\r\n\r\n\t2. Run step 2: <br>\r\n\t````python3 pipeline_expression_pattern.py -rt 2 -fo running_example/ -spf selected_pairs.tsv ````\r\n\r\n## Bug Report\r\nPlease, use the [Issue](https://github.com/YasCoMa/PipeAggregationInfo/issues) tab to report any bug.",
        "doi": null,
        "edam_operation": [
            "Data retrieval",
            "Expression correlation analysis",
            "Protein interaction network analysis"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Gene expression",
            "Protein interactions"
        ],
        "filtered_on": "profil.* in description",
        "id": "619",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/619?version=1",
        "name": "PipePatExp - Pipeline to aggregate gene expression correlation information for PPI",
        "number_of_steps": 0,
        "projects": [
            "yPublish - Bioinfo tools"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "gene expression correlation",
            "gene expression data wrangling",
            "geo database mining"
        ],
        "tools": [],
        "type": "Python",
        "update_time": "2023-10-22",
        "versions": 1
    },
    {
        "create_time": "2024-06-22",
        "creators": [
            "Matthias Bernt"
        ],
        "description": "Automated inference of stable isotope incorporation rates in proteins for functional metaproteomics ",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metap.* in name",
        "id": "613",
        "keep": "Keep",
        "latest_version": 2,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/613?version=2",
        "name": "openms-metaprosip/main",
        "number_of_steps": 8,
        "projects": [
            "Intergalactic Workflow Commission (IWC)"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "PeptideIndexer",
            "DecoyDatabase",
            "FeatureFinderMultiplex",
            "IDMapper",
            "FalseDiscoveryRate",
            "MetaProSIP",
            "__SORTLIST__",
            "MSGFPlusAdapter"
        ],
        "type": "Galaxy",
        "update_time": "2025-08-18",
        "versions": 2
    },
    {
        "create_time": "2023-09-25",
        "creators": [
            "Davide Gurnari"
        ],
        "description": "This repository contains the python code to reproduce the experiments in D\u0142otko, Gurnari \"Euler Characteristic Curves and Profiles: a stable shape invariant for big data problems\"",
        "doi": "10.48546/workflowhub.workflow.576.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "576",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/576?version=1",
        "name": "ECP experiments",
        "number_of_steps": 0,
        "projects": [
            "Dioscuri TDA"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Python",
        "update_time": "2023-09-25",
        "versions": 1
    },
    {
        "create_time": "2023-11-24",
        "creators": [
            "Casper de Visser",
            "Anna Niehues"
        ],
        "description": "This workflow is designed to analyze to a multi-omics data set that comprises genome-wide DNA methylation profiles, targeted metabolomics, and behavioral data of two cohorts that participated in the ACTION Biomarker Study (ACTION, Aggression in Children: Unraveling gene-environment interplay to inform Treatment and InterventiON strategies. (Boomsma 2015, Bartels 2018, Hagenbeek 2020, van Dongen 2021, Hagenbeek 2022). The ACTION-NTR cohort consists of twins that are either longitudinally concordant or discordant for childhood aggression. The ACTION-Curium-LUMC cohort consists of children referred to the Dutch LUMC Curium academic center for child and youth psychiatry. With the joint analysis of multi-omics data and behavioral data, we aim to identify substructures in the ACTION-NTR cohort and link them to aggressive behavior. First, the individuals are clustered using Similarity Network Fusion (SNF, Wang 2014), and latent feature dimensions are uncovered using different unsupervised methods including Multi-Omics Factor Analysis (MOFA) (Argelaguet 2018) and Multiple Correspondence Analysis (MCA, L\u00ea 2008, Husson 2017). In a second step, we determine correlations between -omics and phenotype dimensions, and use them to explain the subgroups of individuals from the ACTION-NTR cohort. In order to validate the results, we project data of the ACTION-Curium-LUMC cohort onto the latent dimensions and determine if correlations between omics and phenotype data can be reproduced.",
        "doi": "10.48546/workflowhub.workflow.402.8",
        "edam_operation": [
            "Clustering",
            "Dimensionality reduction"
        ],
        "edam_topic": [
            "Epigenomics",
            "Metabolomics"
        ],
        "filtered_on": "profil.* in description",
        "id": "402",
        "keep": "To Curate",
        "latest_version": 8,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/402?version=8",
        "name": "X-omics ACTIONdemonstrator analysis workflow",
        "number_of_steps": 0,
        "projects": [
            "X-omics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "behavioral data",
            "epigenomics",
            "fair",
            "metabolomics",
            "multi-omics",
            "nextflow"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-11-24",
        "versions": 8
    },
    {
        "create_time": "2023-08-02",
        "creators": [
            "Nachida Tadrent",
            "Franck Dedeine",
            "Vincent Herv\u00e9"
        ],
        "description": "[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.0.0-brightgreen.svg?style=flat)](https://snakemake.readthedocs.io)\r\n\r\n\r\n# About SnakeMAGs\r\nSnakeMAGs is a workflow to reconstruct prokaryotic genomes from metagenomes. The main purpose of SnakeMAGs is to process Illumina data from raw reads to metagenome-assembled genomes (MAGs).\r\nSnakeMAGs is efficient, easy to handle and flexible to different projects. The workflow is CeCILL licensed, implemented in Snakemake (run on multiple cores) and available for Linux.\r\nSnakeMAGs performed eight main steps:\r\n- Quality filtering of the reads\r\n- Adapter trimming\r\n- Filtering of the host sequences (optional)\r\n- Assembly\r\n- Binning\r\n- Evaluation of the quality of the bins\r\n- Classification of the MAGs\r\n- Estimation of the relative abundance of the MAGs\r\n\r\n\r\n![scheme of workflow](SnakeMAGs_schema.jpg?raw=true)\r\n\r\n# How to use SnakeMAGs\r\n## Install conda\r\nThe easiest way to install and run SnakeMAGs is to use [conda](https://www.anaconda.com/products/distribution). These package managers will help you to easily install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html).\r\n\r\n## Install and activate Snakemake environment\r\nNote: The workflow was developed with Snakemake 7.0.0\r\n```\r\nconda activate\r\n\r\n# First, set up your channel priorities\r\nconda config --add channels defaults\r\nconda config --add channels bioconda\r\nconda config --add channels conda-forge\r\n\r\n# Then, create a new environment for the Snakemake version you require\r\nconda create -n snakemake_7.0.0 snakemake=7.0.0\r\n\r\n# And activate it\r\nconda activate snakemake_7.0.0\r\n```\r\n\r\nAlternatively, you can also install Snakemake via mamba:\r\n```\r\n# If you do not have mamba yet on your machine, you can install it with:\r\nconda install -n base -c conda-forge mamba\r\n\r\n# Then you can install Snakemake\r\nconda activate base\r\nmamba create -c conda-forge -c bioconda -n snakemake snakemake\r\n\r\n# And activate it\r\nconda activate snakemake\r\n\r\n```\r\n\r\n## SnakeMAGs executable\r\nThe easiest way to procure SnakeMAGs and its related files is to clone the repository using git:\r\n```\r\ngit clone https://github.com/Nachida08/SnakeMAGs.git\r\n```\r\nAlternatively, you can download the relevant files:\r\n```\r\nwget https://github.com/Nachida08/SnakeMAGs/blob/main/SnakeMAGs.smk https://github.com/Nachida08/SnakeMAGs/blob/main/config.yaml\r\n```\r\n\r\n## SnakeMAGs input files\r\n- Illumina paired-end reads in FASTQ.\r\n- Adapter sequence file ([adapter.fa](https://github.com/Nachida08/SnakeMAGs/blob/main/adapters.fa)).\r\n- Host genome sequences in FASTA (if host_genome: \"yes\"), in case you work with host-associated metagenomes (e.g. human gut metagenome).\r\n\r\n## Download Genome Taxonomy Database (GTDB)\r\nGTDB-Tk requires ~66G+ of external data (GTDB) that need to be downloaded and unarchived. Because this database is voluminous, we let you decide where you want to store it.\r\nSnakeMAGs do not download automatically GTDB, you have to do it:\r\n\r\n```\r\n#Download the latest release (tested with release207)\r\n#Note: SnakeMAGs uses GTDBtk v2.1.0 and therefore require release 207 as minimum version. See https://ecogenomics.github.io/GTDBTk/installing/index.html#installing for details.\r\nwget https://data.gtdb.ecogenomic.org/releases/latest/auxillary_files/gtdbtk_v2_data.tar.gz\r\n#Decompress\r\ntar -xzvf *tar.gz\r\n#This will create a folder called release207_v2\r\n```\r\nAll you have to do now is to indicate the path to the database folder (in our example, the folder is called release207_v2) in the config file, Classification section.\r\n\r\n## Download the GUNC database (required if gunc: \"yes\")\r\nGUNC accepts either a progenomes or GTDB based reference database. Both can be downloaded using the ```gunc download_db``` command. For our study we used the default proGenome-derived GUNC database. It requires less resources with similar performance.\r\n\r\n```\r\nconda activate\r\n# Install and activate GUNC environment\r\nconda create --prefix /path/to/gunc_env\r\nconda install -c bioconda metabat2 --prefix /path/to/gunc_env\r\nsource activate /path/to/gunc_env\r\n\r\n#Download the proGenome-derived GUNC database (tested with gunc_db_progenomes2.1)\r\n#Note: SnakeMAGs uses GUNC v1.0.5\r\ngunc download_db -db progenomes /path/to/GUNC_DB\r\n```\r\nAll you have to do now is to indicate the path to the GUNC database file in the config file,  Bins quality section.\r\n\r\n## Edit config file\r\nYou need to edit the config.yaml file. In particular, you need to set the correct paths: for the working directory, to specify where are your fastq files, where you want to place the conda environments (that will be created using the provided .yaml files available in [SnakeMAGs_conda_env directory](https://github.com/Nachida08/SnakeMAGs/tree/main/SnakeMAGs_conda_env)), where are the adapters, where is GTDB and optionally where is the GUNC database and where is your host genome reference.\r\n\r\nLastly, you need to allocate the proper computational resources (threads, memory) for each of the main steps. These can be optimized according to your hardware.\r\n\r\n\r\n\r\nHere is an example of a config file:\r\n\r\n```\r\n#####################################################################################################\r\n#####  _____    ___    _              _   _    ______   __    __              _______   _____   #####\r\n##### /  ___|  |   \\  | |     /\\     | | / /  |  ____| |  \\  /  |     /\\     /  _____| /  ___|  #####\r\n##### | (___   | |\\ \\ | |    /  \\    | |/ /   | |____  |   \\/   |    /  \\    | |   __  | (___   #####\r\n#####  \\___ \\  | | \\ \\| |   / /\\ \\   | |\\ \\   |  ____| | |\\  /| |   / /\\ \\   | |  |_ |  \\___ \\  #####\r\n#####  ____) | | |  \\   |  / /__\\ \\  | | \\ \\  | |____  | | \\/ | |  / /__\\ \\  | |____||  ____) | #####\r\n##### |_____/  |_|   \\__| /_/    \\_\\ |_|  \\_\\ |______| |_|    |_| /_/    \\_\\  \\______/ |_____/  #####\r\n#####                                                                                           #####\r\n#####################################################################################################\r\n\r\n############################\r\n### Execution parameters ###\r\n############################\r\n\r\nworking_dir: /path/to/working/directory/                                 #The main directory for the project\r\nraw_fastq: /path/to/raw_fastq/                                           #The directory that contains all the fastq files of all the samples (eg. sample1_R1.fastq & sample1_R2.fastq, sample2_R1.fastq & sample2_R2.fastq...)\r\nsuffix_1: \"_R1.fastq\"                                                    #Main type of suffix for forward reads file (eg. _1.fastq or _R1.fastq or _r1.fastq or _1.fq or _R1.fq or _r1.fq )\r\nsuffix_2: \"_R2.fastq\"                                                    #Main type of suffix for reverse reads file (eg. _2.fastq or _R2.fastq or _r2.fastq or _2.fq or _R2.fq or _r2.fq )\r\n\r\n###########################\r\n### Conda environnemnts ###\r\n###########################\r\n\r\nconda_env: \"/path/to/SnakeMAGs_conda_env/\"                               #Path to the provided SnakeMAGs_conda_env directory which contains the yaml file for each conda environment\r\n\r\n#########################\r\n### Quality filtering ###\r\n#########################\r\nemail: name.surname@your-univ.com                                        #Your e-mail address\r\nthreads_filter: 10                                                       #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_filter: 150                                                    #Memory according to tools need (in GB)\r\n\r\n########################\r\n### Adapter trimming ###\r\n########################\r\nadapters: /path/to/working/directory/adapters.fa                         #A fasta file contanning a set of various Illumina adaptors (this file is provided and is also available on github)\r\ntrim_params: \"2:40:15\"                                                   #For further details, see the Trimmomatic documentation\r\nthreads_trim: 10                                                         #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_trim: 150                                                      #Memory according to tools need (in GB)\r\n\r\n######################\r\n### Host filtering ###\r\n######################\r\nhost_genome: \"yes\"                                                      #yes or no. An optional step for host-associated samples (eg. termite, human, plant...)\r\nthreads_bowtie2: 50                                                     #The number of threads to run this process. To be adjusted according to your hardware\r\nhost_genomes_directory: /path/to/working/host_genomes/                  #the directory where the host genome is stored\r\nhost_genomes: /path/to/working/host_genomes/host_genomes.fa             #A fasta file containing the DNA sequences of the host genome(s)\r\nthreads_samtools: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_host_filtering: 150                                           #Memory according to tools need (in GB)\r\n\r\n################\r\n### Assembly ###\r\n################\r\nthreads_megahit: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\r\nmin_contig_len: 1000                                                   #Minimum length (in bp) of the assembled contigs\r\nk_list: \"21,31,41,51,61,71,81,91,99,109,119\"                           #Kmer size (for further details, see the megahit documentation)\r\nresources_megahit: 250                                                 #Memory according to tools need (in GB)\r\n\r\n###############\r\n### Binning ###\r\n###############\r\nthreads_bwa: 50                                                        #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_bwa: 150                                                     #Memory according to tools need (in GB)\r\nthreads_samtools: 50                                                   #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_samtools: 150                                                #Memory according to tools need (in GB)\r\nseed: 19860615                                                         #Seed number for reproducible results\r\nthreads_metabat: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\r\nminContig: 2500                                                        #Minimum length (in bp) of the contigs\r\nresources_binning: 250                                                 #Memory according to tools need (in GB)\r\n\r\n####################\r\n### Bins quality ###\r\n####################\r\n#checkM\r\nthreads_checkm: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_checkm: 250                                                 #Memory according to tools need (in GB)\r\n#bins_quality_filtering\r\ncompletion: 50                                                        #The minimum completion rate of bins\r\ncontamination: 10                                                     #The maximum contamination rate of bins\r\nparks_quality_score: \"yes\"                                            #yes or no. If yes bins are filtered according to the Parks quality score (completion-5*contamination >= 50)\r\n#GUNC\r\ngunc: \"yes\"                                                           #yes or no. An optional step to detect and discard chimeric and contaminated genomes using the GUNC tool\r\nthreads_gunc: 50                                                      #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_gunc: 250                                                   #Memory according to tools need (in GB)\r\nGUNC_db: /path/to/GUNC_DB/gunc_db_progenomes2.1.dmnd                  #Path to the downloaded GUNC database (see the readme file)\r\n\r\n######################\r\n### Classification ###\r\n######################\r\nGTDB_data_ref: /path/to/downloaded/GTDB                                #Path to uncompressed GTDB-Tk reference data (GTDB)\r\nthreads_gtdb: 10                                                       #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_gtdb: 250                                                    #Memory according to tools need (in GB)\r\n\r\n##################\r\n### Abundances ###\r\n##################\r\nthreads_coverM: 10                                                     #The number of threads to run this process. To be adjusted according to your hardware\r\nresources_coverM: 150                                                  #Memory according to tools need (in GB)\r\n```\r\n# Run SnakeMAGs\r\nIf you are using a workstation with Ubuntu (tested on Ubuntu 22.04):\r\n```{bash}\r\nsnakemake --cores 30 --snakefile SnakeMAGs.smk --use-conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --configfile /path/to/config.yaml --keep-going --latency-wait 180\r\n```\r\n\r\nIf you are working on a cluster with Slurm (tested with version 18.08.7):\r\n```{bash}\r\nsnakemake --snakefile SnakeMAGs.smk --cluster 'sbatch -p <cluster_partition> --mem <memory> -c <cores> -o \"cluster_logs/{wildcards}.{rule}.{jobid}.out\" -e \"cluster_logs/{wildcards}.{rule}.{jobid}.err\" ' --jobs <nbr_of_parallel_jobs> --use-conda --conda-frontend conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --jobname \"{rule}.{wildcards}.{jobid}\" --latency-wait 180 --configfile /path/to/config.yaml --keep-going\r\n```\r\n\r\nIf you are working on a cluster with SGE (tested with version 8.1.9):\r\n```{bash}\r\nsnakemake --snakefile SnakeMAGs.smk --cluster \"qsub -cwd -V -q <short.q/long.q> -pe thread {threads} -e cluster_logs/{rule}.e{jobid} -o cluster_logs/{rule}.o{jobid}\" --jobs <nbr_of_parallel_jobs> --use-conda --conda-frontend conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --jobname \"{rule}.{wildcards}.{jobid}\" --latency-wait 180 --configfile /path/to/config.yaml --keep-going\r\n```\r\n\r\n\r\n# Test\r\nWe provide you a small data set in the [test](https://github.com/Nachida08/SnakeMAGs/tree/main/test) directory which will allow you to validate your instalation and take your first steps with SnakeMAGs. This data set is a subset from [ZymoBiomics Mock Community](https://www.zymoresearch.com/blogs/blog/zymobiomics-microbial-standards-optimize-your-microbiomics-workflow) (250K reads) used in this tutoriel [metagenomics_tutorial](https://github.com/pjtorres/metagenomics_tutorial).\r\n\r\n1. Before getting started make sure you have cloned the SnakeMAGs repository or you have downloaded all the necessary files (SnakeMAGs.smk, config.yaml, chr19.fa.gz, insub732_2_R1.fastq.gz, insub732_2_R2.fastq.gz). See the [SnakeMAGs executable](#snakemags-executable) section.\r\n2. Unzip the fastq files and the host sequences file.\r\n```\r\ngunzip fastqs/insub732_2_R1.fastq.gz fastqs/insub732_2_R2.fastq.gz host_genomes/chr19.fa.gz\r\n```\r\n3. For better organisation put all the read files in the same directory (eg. fastqs) and the host sequences file in a separate directory (eg. host_genomes)\r\n4. Edit the config file (see [Edit config file](#edit-config-file) section)\r\n5. Run the test (see [Run SnakeMAGs](#run-snakemags) section)\r\n\r\nNote: the analysis of these files took 1159.32 secondes to complete on a Ubuntu 22.04 LTS with an Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz x 40 processor, 96GB of RAM.\r\n\r\n# Genome reference for host reads filtering\r\nFor host-associated samples, one can remove host sequences from the metagenomic reads by mapping these reads against a reference genome. In the case of termite gut metagenomes, we are providing [here](https://zenodo.org/record/6908287#.YuAdFXZBx8M) the relevant files (fasta and index files) from termite genomes.\r\n\r\nUpon request, we can help you to generate these files for your own reference genome and make them available to the community.\r\n\r\nNB. These steps of mapping generate voluminous files such as .bam and .sam. Depending on your disk space, you might want to delete these files after use.\r\n\r\n\r\n# Use case\r\nDuring the test phase of the development of SnakeMAGs, we used this workflow to process 10 publicly available termite gut metagenomes generated by Illumina sequencing, to ultimately reconstruct prokaryotic MAGs. These metagenomes were retrieved from the NCBI database using the following accession numbers: SRR10402454; SRR14739927; SRR8296321; SRR8296327; SRR8296329; SRR8296337; SRR8296343; DRR097505; SRR7466794; SRR7466795. They come from five different studies: Waidele et al, 2019; Tokuda et al, 2018; Romero Victorica et al, 2020; Moreira et al, 2021; and Calusinska et al, 2020.\r\n\r\n## Download the Illumina pair-end reads\r\nWe use fasterq-dump tool to extract data in FASTQ-format from SRA-accessions. It is a commandline-tool which offers a faster solution for downloading those large files.\r\n\r\n```\r\n# Install and activate sra-tools environment\r\n## Note: For this study we used sra-tools 2.11.0\r\n\r\nconda activate\r\nconda install -c bioconda sra-tools\r\nconda activate sra-tools\r\n\r\n# Download fastqs in a single directory\r\nmkdir raw_fastq\r\ncd raw_fastq\r\nfasterq-dump <SRA-accession> --threads <threads_nbr> --skip-technical --split-3\r\n```\r\n\r\n## Download Genome reference for host reads filtering\r\n```\r\nmkdir host_genomes\r\ncd host_genomes\r\nwget https://zenodo.org/record/6908287/files/termite_genomes.fasta.gz\r\ngunzip termite_genomes.fasta.gz\r\n```\r\n\r\n## Edit the config file\r\nSee [Edit config file](#edit-config-file) section.\r\n\r\n## Run SnakeMAGs\r\n```\r\nconda activate snakemake_7.0.0\r\nmkdir cluster_logs\r\nsnakemake --snakefile SnakeMAGs.smk --cluster 'sbatch -p <cluster_partition> --mem <memory> -c <cores> -o \"cluster_logs/{wildcards}.{rule}.{jobid}.out\" -e \"cluster_logs/{wildcards}.{rule}.{jobid}.err\" ' --jobs <nbr_of_parallel_jobs> --use-conda --conda-frontend conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --jobname \"{rule}.{wildcards}.{jobid}\" --latency-wait 180 --configfile /path/to/config.yaml --keep-going\r\n```\r\n\r\n## Study results\r\nThe MAGs reconstructed from each metagenome and their taxonomic classification are available in this [repository](https://doi.org/10.5281/zenodo.7661004).\r\n\r\n# Citations\r\n\r\nIf you use SnakeMAGs, please cite:\r\n> Tadrent N, Dedeine F and Herv\u00e9 V. SnakeMAGs: a simple, efficient, flexible and scalable workflow to reconstruct prokaryotic genomes from metagenomes [version 2; peer review: 2 approved]. F1000Research 2023, 11:1522 (https://doi.org/10.12688/f1000research.128091.2)\r\n\r\n\r\nPlease also cite the dependencies:\r\n- [Snakemake](https://doi.org/10.12688/f1000research.29032.2) : M\u00f6lder, F., Jablonski, K. P., Letcher, B., Hall, M. B., Tomkins-tinch, C. H., Sochat, V., Forster, J., Lee, S., Twardziok, S. O., Kanitz, A., Wilm, A., Holtgrewe, M., Rahmann, S., Nahnsen, S., & K\u00f6ster, J. (2021) Sustainable data analysis with Snakemake [version 2; peer review: 2 approved]. *F1000Research* 2021, 10:33.\r\n- [illumina-utils](https://doi.org/10.1371/journal.pone.0066643) : Murat Eren, A., Vineis, J. H., Morrison, H. G., & Sogin, M. L. (2013). A Filtering Method to Generate High Quality Short Reads Using Illumina Paired-End Technology. *PloS ONE*, 8(6), e66643.\r\n- [Trimmomatic](https://doi.org/10.1093/bioinformatics/btu170) : Bolger, A. M., Lohse, M., & Usadel, B. (2014). Genome analysis Trimmomatic: a flexible trimmer for Illumina sequence data. *Bioinformatics*, 30(15), 2114-2120.\r\n- [Bowtie2](https://doi.org/10.1038/nmeth.1923) : Langmead, B., & Salzberg, S. L. (2012). Fast gapped-read alignment with Bowtie 2. *Nature Methods*, 9(4), 357\u2013359.\r\n- [SAMtools](https://doi.org/10.1093/bioinformatics/btp352) : Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., Marth, G., Abecasis, G., & Durbin, R. (2009). The Sequence Alignment/Map format and SAMtools. *Bioinformatics*, 25(16), 2078\u20132079.\r\n- [BEDtools](https://doi.org/10.1093/bioinformatics/btq033) : Quinlan, A. R., & Hall, I. M. (2010). BEDTools: A flexible suite of utilities for comparing genomic features. *Bioinformatics*, 26(6), 841\u2013842.\r\n- [MEGAHIT](https://doi.org/10.1093/bioinformatics/btv033) : Li, D., Liu, C. M., Luo, R., Sadakane, K., & Lam, T. W. (2015). MEGAHIT: An ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph. *Bioinformatics*, 31(10), 1674\u20131676.\r\n- [bwa](https://doi.org/10.1093/bioinformatics/btp324) : Li, H., & Durbin, R. (2009). Fast and accurate short read alignment with Burrows-Wheeler transform. *Bioinformatics*, 25(14), 1754\u20131760.\r\n- [MetaBAT2](https://doi.org/10.7717/peerj.7359) : Kang, D. D., Li, F., Kirton, E., Thomas, A., Egan, R., An, H., & Wang, Z. (2019). MetaBAT 2: An adaptive binning algorithm for robust and efficient genome reconstruction from metagenome assemblies. *PeerJ*, 2019(7), 1\u201313.\r\n- [CheckM](https://doi.org/10.1101/gr.186072.114) : Parks, D. H., Imelfort, M., Skennerton, C. T., Hugenholtz, P., & Tyson, G. W. (2015). CheckM: Assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes. *Genome Research*, 25(7), 1043\u20131055.\r\n- [GTDB-Tk](https://doi.org/10.1093/BIOINFORMATICS/BTAC672) : Chaumeil, P.-A., Mussig, A. J., Hugenholtz, P., Parks, D. H. (2022). GTDB-Tk v2: memory friendly classification with the genome taxonomy database. *Bioinformatics*.\r\n- [CoverM](https://github.com/wwood/CoverM)\r\n- [Waidele et al, 2019](https://doi.org/10.1101/526038) : Waidele, L., Korb, J., Voolstra, C. R., Dedeine, F., & Staubach, F. (2019). Ecological specificity of the metagenome in a set of lower termite species supports contribution of the microbiome to adaptation of the host. *Animal Microbiome*, 1(1), 1\u201313.\r\n- [Tokuda et al, 2018](https://doi.org/10.1073/pnas.1810550115) : Tokuda, G., Mikaelyan, A., Fukui, C., Matsuura, Y., Watanabe, H., Fujishima, M., & Brune, A. (2018). Fiber-associated spirochetes are major agents of hemicellulose degradation in the hindgut of wood-feeding higher termites. *Proceedings of the National Academy of Sciences of the United States of America*, 115(51), E11996\u2013E12004.\r\n- [Romero Victorica et al, 2020](https://doi.org/10.1038/s41598-020-60850-5) : Romero Victorica, M., Soria, M. A., Batista-Garc\u00eda, R. A., Ceja-Navarro, J. A., Vikram, S., Ortiz, M., Onta\u00f1on, O., Ghio, S., Mart\u00ednez-\u00c1vila, L., Quintero Garc\u00eda, O. J., Etcheverry, C., Campos, E., Cowan, D., Arneodo, J., & Talia, P. M. (2020). Neotropical termite microbiomes as sources of novel plant cell wall degrading enzymes. *Scientific Reports*, 10(1), 1\u201314.\r\n- [Moreira et al, 2021](https://doi.org/10.3389/fevo.2021.632590) : Moreira, E. A., Persinoti, G. F., Menezes, L. R., Paix\u00e3o, D. A. A., Alvarez, T. M., Cairo, J. P. L. F., Squina, F. M., Costa-Leonardo, A. M., Rodrigues, A., Sillam-Duss\u00e8s, D., & Arab, A. (2021). Complementary contribution of Fungi and Bacteria to lignocellulose digestion in the food stored by a neotropical higher termite. *Frontiers in Ecology and Evolution*, 9(April), 1\u201312.\r\n- [Calusinska et al, 2020](https://doi.org/10.1038/s42003-020-1004-3) : Calusinska, M., Marynowska, M., Bertucci, M., Untereiner, B., Klimek, D., Goux, X., Sillam-Duss\u00e8s, D., Gawron, P., Halder, R., Wilmes, P., Ferrer, P., Gerin, P., Roisin, Y., & Delfosse, P. (2020). Integrative omics analysis of the termite gut system adaptation to Miscanthus diet identifies lignocellulose degradation enzymes. *Communications Biology*, 3(1), 1\u201312.\r\n- [Orakov et al, 2021](https://doi.org/10.1186/s13059-021-02393-0) : Orakov, A., Fullam, A., Coelho, L. P., Khedkar, S., Szklarczyk, D., Mende, D. R., Schmidt, T. S. B., & Bork, P. (2021). GUNC: detection of chimerism and contamination in prokaryotic genomes. *Genome Biology*, 22(1).\r\n- [Parks et al, 2015](https://doi.org/10.1101/gr.186072.114) : Parks, D. H., Imelfort, M., Skennerton, C. T., Hugenholtz, P., & Tyson, G. W. (2015). CheckM: Assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes. *Genome Research*, 25(7), 1043\u20131055.\r\n# License\r\nThis project is licensed under the CeCILL License - see the [LICENSE](https://github.com/Nachida08/SnakeMAGs/blob/main/LICENCE) file for details.\r\n\r\nDeveloped by Nachida Tadrent at the Insect Biology Research Institute ([IRBI](https://irbi.univ-tours.fr/)), under the supervision of Franck Dedeine and Vincent Herv\u00e9.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioinformatics",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "554",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CECILL-2.1",
        "link": "https:/workflowhub.eu/workflows/554?version=1",
        "name": "SnakeMAGs: a simple, efficient, flexible and scalable workflow to reconstruct prokaryotic genomes from metagenomes",
        "number_of_steps": 0,
        "projects": [
            "Metagenomic tools"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "mag",
            "metagenomics",
            "binning"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-08-02",
        "versions": 1
    },
    {
        "create_time": "2023-07-09",
        "creators": [
            "Stevie Pederson"
        ],
        "description": "# prepareChIPs\r\n\r\nThis is a simple `snakemake` workflow template for preparing **single-end** ChIP-Seq data.\r\nThe steps implemented are:\r\n\r\n1. Download raw fastq files from SRA\r\n2. Trim and Filter raw fastq files using `AdapterRemoval`\r\n3. Align to the supplied genome using `bowtie2`\r\n4. Deduplicate Alignments using `Picard MarkDuplicates`\r\n5. Call Macs2 Peaks using `macs2`\r\n\r\nA pdf of the rulegraph is available [here](workflow/rules/rulegraph.pdf)\r\n\r\nFull details for each step are given below.\r\nAny additional parameters for tools can be specified using `config/config.yml`, along with many of the requisite paths\r\n\r\nTo run the workflow with default settings, simply run as follows (after editing `config/samples.tsv`)\r\n\r\n```bash\r\nsnakemake --use-conda --cores 16\r\n```\r\n\r\nIf running on an HPC cluster, a snakemake profile will required for submission to the queueing system and appropriate resource allocation.\r\nPlease discuss this will your HPC support team.\r\nNodes may also have restricted internet access and rules which download files may not work on many HPCs.\r\nPlease see below or discuss this with your support team\r\n\r\nWhilst no snakemake wrappers are explicitly used in this workflow, the underlying scripts are utilised where possible to minimise any issues with HPC clusters with restrictions on internet access.\r\nThese scripts are based on `v1.31.1` of the snakemake wrappers\r\n\r\n### Important Note Regarding OSX Systems\r\n\r\nIt should be noted that this workflow is **currently incompatible with OSX-based systems**. \r\nThere are two unsolved issues\r\n\r\n1. `fasterq-dump` has a bug which is specific to conda environments. This has been updated in v3.0.3 but this patch has not yet been made available to conda environments for OSX. Please check [here](https://anaconda.org/bioconda/sra-tools) to see if this has been updated.\r\n2. The following  error appears in some OSX-based R sessions, in a system-dependent manner:\r\n```\r\nError in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  : \r\n  polygon edge not found\r\n```\r\n\r\nThe fix for this bug is currently unknown\r\n\r\n## Download Raw Data\r\n\r\n### Outline\r\n\r\nThe file `samples.tsv` is used to specify all steps for this workflow.\r\nThis file must contain the columns: `accession`, `target`, `treatment` and `input`\r\n\r\n1. `accession` must be an SRA accession. Only single-end data is currently supported by this workflow\r\n2. `target` defines the ChIP target. All files common to a target and treatment will be used to generate summarised coverage in bigWig Files\r\n3. `treatment` defines the treatment group each file belongs to. If only one treatment exists, simply use the value 'control' or similar for every file\r\n4. `input` should contain the accession for the relevant input sample. These will only be downloaded once. Valid input samples are *required* for this workflow\r\n\r\nAs some HPCs restrict internet access for submitted jobs, *it may be prudent to run the initial rules in an interactive session* if at all possible.\r\nThis can be performed using the following (with 2 cores provided as an example)\r\n\r\n```bash\r\nsnakemake --use-conda --until get_fastq --cores 2\r\n```\r\n\r\n### Outputs\r\n\r\n- Downloaded files will be gzipped and written to `data/fastq/raw`.\r\n- `FastQC` and `MultiQC` will also be run, with output in `docs/qc/raw`\r\n\r\nBoth of these directories are able to be specified as relative paths in `config.yml`\r\n\r\n## Read Filtering\r\n\r\n### Outline\r\n\r\nRead trimming is performed using [AdapterRemoval](https://adapterremoval.readthedocs.io/en/stable/).\r\nDefault settings are customisable using config.yml, with the defaults set to discard reads shorter than 50nt, and to trim using quality scores with a threshold of Q30.\r\n\r\n### Outputs\r\n\r\n- Trimmed fastq.gz files will be written to `data/fastq/trimmed`\r\n- `FastQC` and `MultiQC` will also be run, with output in `docs/qc/trimmed`\r\n- AdapterRemoval 'settings' files will be written to `output/adapterremoval`\r\n\r\n## Alignments\r\n\r\n### Outline\r\n\r\nAlignment is performed using [`bowtie2`](https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml) and it is assumed that this index is available before running this workflow.\r\nThe path and prefix must be provided using config.yml\r\n\r\nThis index will also be used to produce the file `chrom.sizes` which is essential for conversion of bedGraph files to the more efficient bigWig files.\r\n\r\n### Outputs\r\n\r\n- Alignments will be written to `data/aligned`\r\n- `bowtie2` log files will be written to `output/bowtie2` (not the conenvtional log directory)\r\n- The file `chrom.sizes` will be written to `output/annotations`\r\n\r\nBoth sorted and the original unsorted alignments will be returned.\r\nHowever, the unsorted alignments are marked with `temp()` and can be deleted using \r\n\r\n```bash\r\nsnakemake --delete-temp-output --cores 1\r\n```\r\n\r\n## Deduplication\r\n\r\n### Outline\r\n\r\nDeduplication is performed using [MarkDuplicates](https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard-) from the Picard set of tools.\r\nBy default, deduplication will remove the duplicates from the set of alignments.\r\nAll resultant bam files will be sorted and indexed.\r\n\r\n### Outputs\r\n\r\n- Deduplicated alignments are written to `data/deduplicated` and are indexed\r\n- DuplicationMetrics files are written to `output/markDuplicates`\r\n\r\n## Peak Calling\r\n\r\n### Outline\r\n\r\nThis is performed using [`macs2 callpeak`](https://pypi.org/project/MACS2/).\r\n\r\n- Peak calling will be performed on:\r\n    a. each sample individually, and \r\n    b. merged samples for those sharing a common ChIP target and treatment group.\r\n- Coverage bigWig files for each individual sample are produced using CPM values (i.e. Signal Per Million Reads, SPMR)\r\n- For all combinations of target and treatment coverage bigWig files are also produced, along with fold-enrichment bigWig files\r\n\r\n### Outputs\r\n\r\n- Individual outputs are written to `output/macs2/{accession}`\r\n\t+ Peaks are written in `narrowPeak` format along with `summits.bed`\r\n\t+ bedGraph files are automatically converted to bigWig files, and the originals are marked with `temp()` for subsequent deletion\r\n\t+ callpeak log files are also added to this directory\r\n- Merged outputs are written to `output/macs2/{target}/`\r\n\t+ bedGraph Files are also converted to bigWig and marked with `temp()`\r\n\t+ Fold-Enrichment bigWig files are also created with the original bedGraph files marked with `temp()`\r\n",
        "doi": "10.48546/workflowhub.workflow.528.1",
        "edam_operation": [
            "Transcription factor binding site prediction"
        ],
        "edam_topic": [
            "ChIP-seq"
        ],
        "filtered_on": "profil.* in description",
        "id": "528",
        "keep": "Reject",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/528?version=1",
        "name": "prepareChIPs:",
        "number_of_steps": 0,
        "projects": [
            "Black Ochre Data Labs"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "genomics",
            "transcriptomics"
        ],
        "tools": [
            "MACS",
            "Bowtie 2",
            "AdapterRemoval",
            "R",
            "FastQC",
            "MultiQC",
            "Bioconductor",
            "ngsReports"
        ],
        "type": "Snakemake",
        "update_time": "2023-07-09",
        "versions": 1
    },
    {
        "create_time": "2023-07-05",
        "creators": [
            "Bugra Oezdemir"
        ],
        "description": "# BatchConvert  ![DOI:10.5281](https://zenodo.org/badge/doi/10.5281/zenodo.7955974.svg)\r\n\r\nA command line tool for converting image data into either of the standard file formats OME-TIFF or OME-Zarr. \r\n\r\nThe tool wraps the dedicated file converters bfconvert and bioformats2raw to convert into OME-TIFF or OME-Zarr,\r\nrespectively. The workflow management system NextFlow is used to perform conversion in parallel for batches of images. \r\n\r\nThe tool also wraps s3 and Aspera clients (go-mc and aspera-cli, respectively). Therefore, input and output locations can \r\nbe specified as local or remote storage and file transfer will be performed automatically. The conversion can be run on \r\nHPC with Slurm.  \r\n\r\n![](figures/diagram.png)\r\n\r\n## Installation & Dependencies\r\n\r\n**Important** note: The package has been so far only tested on Ubuntu 20.04.\r\n\r\nThe minimal dependency to run the tool is NextFlow, which should be installed and made accessible from the command line.\r\n\r\nIf conda exists on your system, you can install BatchConvert together with NextFlow using the following script:\r\n```\r\ngit clone https://github.com/Euro-BioImaging/BatchConvert.git && \\ \r\nsource BatchConvert/installation/install_with_nextflow.sh\r\n```\r\n\r\n\r\nIf you already have NextFlow installed and accessible from the command line (or if you prefer to install it manually \r\ne.g., as shown [here](https://www.nextflow.io/docs/latest/getstarted.html)), you can also install BatchConvert alone, using the following script:\r\n```\r\ngit clone https://github.com/Euro-BioImaging/BatchConvert.git && \\ \r\nsource BatchConvert/installation/install.sh\r\n```\r\n\r\n\r\nOther dependencies (which will be **automatically** installed):\r\n- bioformats2raw (entrypoint bioformats2raw)\r\n- bftools (entrypoint bfconvert)\r\n- go-mc (entrypoint mc)\r\n- aspera-cli (entrypoint ascp)\r\n\r\nThese dependencies will be pulled and cached automatically at the first execution of the conversion command. \r\nThe mode of dependency management can  be specified by using the command line option ``--profile`` or `-pf`. Depending \r\non how this option is specified, the dependencies will be acquired / run either via conda or via docker/singularity containers. \r\n\r\nSpecifying ``--profile conda`` (default) will install the dependencies to an \r\nenvironment at ``./.condaCache`` and use this environment to run the workflow. This option \r\nrequires that miniconda/anaconda is installed on your system.    \r\n\r\nAlternatively, specifying ``--profile docker`` or ``--profile singularity`` will pull a docker or \r\nsingularity image with the dependencies, respectively, and use this image to run the workflow.\r\nThese options assume that the respective container runtime (docker or singularity) is available on \r\nyour system. If singularity is being used, a cache directory will be created at the path \r\n``./.singularityCache`` where the singularity image is stored. \r\n\r\nFinally, you can still choose to install the dependencies manually and use your own installations to run\r\nthe workflow. In this case, you should specify ``--profile standard`` and make sure the entrypoints\r\nspecified above are recognised by your shell.  \r\n\r\n\r\n## Configuration\r\n\r\nBatchConvert can be configured to have default options for file conversion and transfer. Probably, the most important sets of parameters\r\nto be configured include credentials for the remote ends. The easiest way to configure remote stores is by running the interactive \r\nconfiguration command as indicated below.\r\n\r\n### Configuration of the s3 object store\r\n\r\nRun the interactive configuration command: \r\n\r\n`batchconvert configure_s3_remote`\r\n\r\nThis will start a sequence of requests for s3 credentials such as name, url, access, etc. Provide each requested credential and click\r\nenter. Continue this cycle until the process is finished. Upon completing the configuration, the sequence of commands should roughly look like this:\r\n\r\n```\r\noezdemir@pc-ellenberg108:~$ batchconvert configure_s3_remote\r\nenter remote name (for example s3)\r\ns3\r\nenter url:\r\nhttps://s3.embl.de\r\nenter access key:\r\n\"your-access-key\"\r\nenter secret key:\r\n\"your-secret-key\"\r\nenter bucket name:\r\n\"your-bucket\"\r\nConfiguration of the default s3 credentials is complete\r\n```\r\n\r\n\r\n### Configuration of the BioStudies user space\r\n\r\nRun the interactive configuration command: \r\n\r\n`batchconvert configure_bia_remote`\r\n\r\nThis will prompt a request for the secret directory to connect to. Enter the secret directory for your user space and click enter. \r\nUpon completing the configuration, the sequence of commands should roughly look like this:\r\n\r\n```\r\noezdemir@pc-ellenberg108:~$ batchconvert configure_bia_remote\r\nenter the secret directory for BioImage Archive user space:\r\n\"your-secret-directory\"\r\nconfiguration of the default bia credentials is complete\r\n```\r\n\r\n### Configuration of the slurm options\r\n\r\nBatchConvert can also run on slurm clusters. In order to configure the slurm parameters, run the interactive configuration command: \r\n\r\n`batchconvert configure_slurm`\r\n\r\nThis will start a sequence of requests for slurm options. Provide each requested option and click enter. \r\nContinue this cycle until the process is finished. Upon completing the configuration, the sequence of commands should \r\nroughly look like this:\r\n\r\n```\r\noezdemir@pc-ellenberg108:~$ batchconvert configure_slurm\r\nPlease enter value for queue_size\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the current value \u00b450\u00b4\r\ns\r\nPlease enter value for submit_rate_limit\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the current value \u00b410/2min\u00b4\r\ns\r\nPlease enter value for cluster_options\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the current value \u00b4--mem-per-cpu=3140 --cpus-per-task=16\u00b4\r\ns\r\nPlease enter value for time\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the current value \u00b46h\u00b4\r\ns\r\nconfiguration of the default slurm parameters is complete\r\n```\r\n\r\n### Configuration of the default conversion parameters\r\n\r\nWhile all conversion parameters can be specified as command line arguments, it can\r\nbe useful for the users to set their own default parameters to avoid re-entering those\r\nparameters for subsequent executions. BatchConvert allows for interactive configuration of \r\nconversion in the same way as configuration of the remote stores described above.\r\n\r\nTo configure the conversion into OME-TIFF, run the following command:\r\n\r\n`batchconvert configure_ometiff`\r\n\r\nThis will prompt the user to enter a series of parameters, which will then be saved as the \r\ndefault parameters to be passed to the `batchconvert ometiff` command. Upon completing the \r\nconfiguration, the sequence of commands should look similar to:\r\n\r\n```\r\noezdemir@pc-ellenberg108:~$ batchconvert configure_ometiff\r\nPlease enter value for noflat\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bfconvert defaults\"\r\ns\r\nPlease enter value for series\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bfconvert defaults\"\r\ns\r\nPlease enter value for timepoint\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bfconvert defaults\"\r\ns\r\n...\r\n...\r\n...\r\n...\r\n...\r\n...\r\nConfiguration of the default parameters for 'bfconvert' is complete\r\n```\r\n\r\n\r\nTo configure the conversion into OME-Zarr, run the following command:\r\n\r\n`batchconvert configure_omezarr`\r\n\r\nSimilarly, this will prompt the user to enter a series of parameters, which will then be saved as the \r\ndefault parameters to be passed to the `batchconvert omezarr` command. Upon completing the configuration, \r\nthe sequence of commands should look similar to:\r\n\r\n```\r\noezdemir@pc-ellenberg108:~$ batchconvert configure_omezarr\r\nPlease enter value for resolutions_zarr\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bioformats2raw defaults\"\r\ns\r\nPlease enter value for chunk_h\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bioformats2raw defaults\"\r\ns\r\nPlease enter value for chunk_w\r\nClick enter if this parameter is not applicable\r\nEnter \"skip\" or \"s\" if you would like to keep the parameter\u00b4s current value, which is \"bioformats2raw defaults\"\r\n...\r\n...\r\n...\r\n...\r\n...\r\n...\r\nConfiguration of the default parameters for 'bioformats2raw' is complete\r\n```\r\n\r\nIt is important to note that the initial defaults for the conversion parameters are the same as the defaults\r\nof the backend tools bfconvert and bioformats2raw, as noted in the prompt excerpt above. Through interactive configuration, \r\nthe user is overriding these initial defaults and setting their own defaults. It is possible to reset the initial\r\ndefaults by running the following command.\r\n\r\n`batchconvert reset_defaults`\r\n\r\nAnother important point is that any of these configured parameters can be overridden by passing a value to that\r\nparameter in the commandline. For instance, in the following command, the value of 20 will be assigned to `chunk_h` parameter \r\neven if the value for the same parameter might be different in the configuration file. \r\n\r\n`batchconvert omezarr --chunk_h 20 \"path/to/input\" \"path/to/output\"`\r\n\r\n\r\n## Examples\r\n\r\n### Local conversion\r\n\r\n#### Parallel conversion of files to separate OME-TIFFs / OME-Zarrs:\r\nConvert a batch of images on your local storage into OME-TIFF format. \r\nNote that the `input_path` in the command given below is typically a \r\ndirectory with multiple image files but a single image file can also be passed:\\\r\n`batchconvert ometiff -pf conda \"input_path\" \"output_path\"` \r\n\r\nNote that if this is your first conversion with the profile `conda`, \r\nit will take a while for a conda environment with the dependencies to be\r\ncreated. All the subsequent conversion commands with the profile `conda`,\r\nhowever, will use this environment, and thus show no such delay.\r\n\r\nSince conda is the default profile, it does not have to be \r\nexplicitly included in the command line. Thus, the command can be shortened to:\\\r\n`batchconvert ometiff \"input_path\" \"output_path\"`\r\n\r\nConvert only the first channel of the images:\\\r\n`batchconvert ometiff -chn 0 \"input_path\" \"output_path\"`\r\n\r\nCrop the images being converted along x and y axis by 150 pixels:\\\r\n`batchconvert ometiff -cr 0,0,150,150 \"input_path\" \"output_path\"`\r\n\r\nConvert into OME-Zarr instead:\\\r\n`batchconvert omezarr \"input_path\" \"output_path\"`\r\n\r\nConvert into OME-Zarr with 3 resolution levels:\\\r\n`batchconvert omezarr -rz 3 \"input_path\" \"output_path\"`\r\n\r\nSelect a subset of images with a matching string such as \"mutation\":\\\r\n`batchconvert omezarr -p mutation \"input_path\" \"output_path\"`\r\n\r\nSelect a subset of images using wildcards. Note that the use of \"\" around \r\nthe input path is necessary when using wildcards:\\\r\n`batchconvert omezarr \"input_path/*D3*.oir\" \"output_path\"`\r\n\r\nConvert by using a singularity container instead of conda environment (requires\r\nsingularity to be installed on your system):\\\r\n`batchconvert omezarr -pf singularity \"input_path/*D3*.oir\" \"output_path\"`\r\n\r\nConvert by using a docker container instead of conda environment (requires docker\r\nto be installed on your system):\\\r\n`batchconvert omezarr -pf docker \"input_path/*D3*.oir\" \"output_path\"`\r\n\r\nNote that similarly to the case with the profile `conda`, the first execution of\r\na conversion with the profile `singularity` or `docker` will take a while for the\r\ncontainer image to be pulled. All the subsequent conversion commands using a \r\ncontainer option will use this image, and thus show no such delay. \r\n\r\nConvert local data and upload the output to an s3 bucket. Note that the output \r\npath is created relative to the bucket specified in your s3 configuration:\\\r\n`batchconvert omezarr -dt s3 \"input_path\" \"output_path\"`\r\n\r\nReceive input files from an s3 bucket, convert locally and upload the output to \r\nthe same bucket. Note that wildcards cannot be used when the input is from s3. \r\nUse pattern matching option `-p` for selecting a subset of input files:\\\r\n`batchconvert omezarr -p mutation -st s3 -dt s3 \"input_path\" \"output_path\"`\r\n\r\nReceive input files from your private BioStudies user space and convert them locally.\r\nUse pattern matching option `-p` for selecting a subset of input files:\\\r\n`batchconvert omezarr -p mutation -st bia \"input_path\" \"output_path\"`\r\n\r\nReceive an input from an s3 bucket, convert locally and upload the output to your \r\nprivate BioStudies user space. Use pattern matching option `-p` for selecting a subset \r\nof input files:\\\r\n`batchconvert omezarr -p mutation -st s3 -dt bia \"input_path\" \"output_path\"`\r\n\r\nNote that in all the examples shown above, BatchConvert treats each input file as separate,\r\nstandalone data point, disregarding the possibility that some of the input files might belong to \r\nthe same multidimensional array. Thus, each input file is converted to an independent \r\nOME-TIFF / OME-Zarr and the number of outputs will thus equal the number of selected input files.\r\nAn alternative scenario is discussed below.\r\n\r\n#### Parallel conversion of file groups by stacking multiple files into single OME-TIFFs / OME-Zarrs:\r\n\r\nWhen the flag `--merge_files` is specified, BatchConvert tries to detect which input files might \r\nbelong to the same multidimensional array based on the patterns in the filenames. Then a \"grouped conversion\" \r\nis performed, meaning that the files belonging to the same dataset will be incorporated into \r\na single OME-TIFF / OME-Zarr series, in that files will be concatenated along specific dimension(s) \r\nduring the conversion. Multiple file groups in the input directory can be detected and converted \r\nin parallel. \r\n\r\nThis feature uses Bio-Formats's pattern files as described [here](https://docs.openmicroscopy.org/bio-formats/6.6.0/formats/pattern-file.html).\r\nHowever, BatchConvert generates pattern files automatically, allowing the user to directly use the \r\ninput directory in the conversion command. BatchConvert also has the option of specifying the \r\nconcatenation axes in the command line, which is especially useful in cases where the filenames \r\nmay not contain dimension information.  \r\n\r\nTo be able to use the `--merge files` flag, the input file names must obey certain rules:\r\n1. File names in the same group must be uniform, except for one or more **numeric field(s)**, which\r\nshould show incremental change across the files. These so-called **variable fields** \r\nwill be detected and used as the dimension(s) of concatenation.\r\n2. The length of variable fields must be uniform within the group. For instance, if the\r\nvariable field has values reaching multi-digit numbers, leading \"0\"s should be included where needed \r\nin the file names to make the variable field length uniform within the group.\r\n3. Typically, each variable field should follow a dimension specifier. What patterns can be used as \r\ndimension specifiers are explained [here](https://docs.openmicroscopy.org/bio-formats/6.6.0/formats/pattern-file.html).\r\nHowever, BatchConvert also has the option `--concatenation_order`, which allows the user to\r\nspecify from the command line, the dimension(s), along which the files must be concatenated.\r\n4. File names that are unique and cannot be associated with any group will be assumed as\r\nstandalone images and converted accordingly. \r\n\r\nBelow are some examples of grouped conversion commands in the context of different possible use-case scenarios:\r\n\r\n**Example 1:**\r\n\r\nThis is an example of a folder with non-uniform filename lengths:\r\n```\r\ntime-series/test_img_T2\r\ntime-series/test_img_T4\r\ntime-series/test_img_T6\r\ntime-series/test_img_T8\r\ntime-series/test_img_T10\r\ntime-series/test_img_T12\r\n```\r\nIn this example, leading zeroes are missing in the variable fields of some filenames. \r\nA typical command to convert this folder to a single OME-TIFF would look like: \\\r\n`batchconvert --ometiff --merge_files \"input_dir/time-series\" \"output_path\"`\r\n\r\nHowever, this command would fail to create a single OME-Zarr folder due to the non-uniform \r\nlengths of the filenames. Instead, the files would be split into two groups based on the\r\nfilename length, leading to two separate OME-Zarrs with names:\r\n\r\n`test_img_TRange{2-8-2}.ome.zarr` and `test_img_TRange{10-12-2}.ome.zarr`\r\n\r\nHere is the corrected version of the folder for the above example-\r\n```\r\ntime-series/test_img_T02\r\ntime-series/test_img_T04\r\ntime-series/test_img_T06\r\ntime-series/test_img_T08\r\ntime-series/test_img_T10\r\ntime-series/test_img_T12\r\n```\r\n\r\nExecuting the same command on this folder would result in a single OME-Zarr with the name:\r\n`test_img_TRange{02-12-2}.ome.zarr`\r\n\r\n**Example 2**- \r\n\r\nIn this example, the filename lengths are uniform but the incrementation within the variable field is not.\r\n```\r\ntime-series/test_img_T2\r\ntime-series/test_img_T4\r\ntime-series/test_img_T5\r\ntime-series/test_img_T7\r\n```\r\n\r\nA typical command to convert this folder to a single OME-Zarr would look like: \\\r\n`batchconvert --omezarr --merge_files \"input_dir/time-series\" \"output_path\"`\r\n\r\nHowever, the command would fail to assume these files as a single group due to the\r\nnon-uniform incrementation in the variable field of the filenames. Instead, the dataset \r\nwould be split into two groups, leading to two separate OME-Zarrs with the following names:\r\n`test_img_TRange{2-4-2}.ome.zarr` and `test_img_TRange{5-7-2}.ome.zarr`  \r\n\r\n\r\n**Example 3**\r\n\r\nThis is an example of a case where the conversion attempts to concatenate files along two\r\ndimensions, channel and time.\r\n```\r\nmultichannel_time-series/test_img_C1-T1\r\nmultichannel_time-series/test_img_C1-T2\r\nmultichannel_time-series/test_img_C1-T3\r\nmultichannel_time-series/test_img_C2-T1\r\nmultichannel_time-series/test_img_C2-T2\r\n```\r\nTo convert this folder to a single OME-Zarr, one could try the following command: \\\r\n`batchconvert --omezarr --merge_files \"input_dir/multichannel_time-series\" \"output_path\"`\r\n\r\nHowever, since the channel-2 does not have the same number of timeframes as the channel-1, \r\nBatchConvert will fail to assume these two channels as part of the same series and\r\nwill instead split the two channels into two separate OME-Zarrs. \r\n\r\nThe output would look like: \\\r\n`test_img_C1-TRange{1-3-1}.ome.zarr` \\\r\n`test_img_C2-TRange{1-2-1}.ome.zarr`\r\n\r\nTo be able to really incorporate all files into a single OME-Zarr, the folder should have equal\r\nnumber of images corresponding to both channels, as shown below:\r\n```\r\nmultichannel_time-series/test_img_C1-T1\r\nmultichannel_time-series/test_img_C1-T2\r\nmultichannel_time-series/test_img_C1-T3\r\nmultichannel_time-series/test_img_C2-T1\r\nmultichannel_time-series/test_img_C2-T2\r\nmultichannel_time-series/test_img_C2-T3\r\n```\r\nThe same conversion command on this version of the input folder would result in a single \r\nOME-Zarr with the name: \\\r\n`test_img_CRange{1-2-1}-TRange{1-3-1}.ome.zarr`\r\n\r\n\r\n**Example 4**\r\n\r\nThis is another example of a case, where there are multiple filename patterns in the input folder.\r\n\r\n```\r\nfolder_with_multiple_groups/test_img_C1-T1\r\nfolder_with_multiple_groups/test_img_C1-T2\r\nfolder_with_multiple_groups/test_img_C2-T1\r\nfolder_with_multiple_groups/test_img_C2-T2\r\nfolder_with_multiple_groups/test_img_T1-Z1\r\nfolder_with_multiple_groups/test_img_T1-Z2\r\nfolder_with_multiple_groups/test_img_T1-Z3\r\nfolder_with_multiple_groups/test_img_T2-Z1\r\nfolder_with_multiple_groups/test_img_T2-Z2\r\nfolder_with_multiple_groups/test_img_T2-Z3\r\n```\r\n\r\nOne can convert this folder with- \\\r\n`batchconvert --omezarr --merge_files \"input_dir/folder_with_multiple_groups\" \"output_path\"`\r\n \r\nBatchConvert will detect the two patterns in this folder and perform two grouped conversions. \r\nThe output folders will be named as `test_img_CRange{1-2-1}-TRange{1-2-1}.ome.zarr` and \r\n`test_img_TRange{1-2-1}-ZRange{1-3-1}.ome.zarr`. \r\n\r\n\r\n**Example 5**\r\n\r\nNow imagine that we have the same files as in the example 4 but the filenames of the\r\nfirst group lack any dimension specifier, so we have the following folder:\r\n\r\n```\r\nfolder_with_multiple_groups/test_img_1-1\r\nfolder_with_multiple_groups/test_img_1-2\r\nfolder_with_multiple_groups/test_img_2-1\r\nfolder_with_multiple_groups/test_img_2-2\r\nfolder_with_multiple_groups/test_img_T1-Z1\r\nfolder_with_multiple_groups/test_img_T1-Z2\r\nfolder_with_multiple_groups/test_img_T1-Z3\r\nfolder_with_multiple_groups/test_img_T2-Z1\r\nfolder_with_multiple_groups/test_img_T2-Z2\r\nfolder_with_multiple_groups/test_img_T2-Z3\r\n```\r\n\r\nIn such a scenario, BatchConvert allows the user to specify the concatenation axes \r\nvia `--concatenation_order` option. This option expects comma-separated strings of dimensions \r\nfor each group. In this example, the user must provide a string of 2 characters, such as `ct` for \r\nchannel and time, for group 1, since there are two variable fields for this group. Since group 2 \r\nalready has dimension specifiers (T and Z as specified in the filenames preceding the variable fields),\r\nthe user does not need to specify anything for this group, and can enter `auto` or `aa` for automatic\r\ndetection of the specifiers. \r\n\r\nSo the following line can be used to convert this folder: \\\r\n`batchconvert --omezarr --merge_files --concatenation_order ct,aa \"input_dir/folder_with_multiple_groups\" \"output_path\"`\r\n\r\nThe resulting OME-Zarrs will have the names:\r\n`test_img_CRange{1-2-1}-TRange{1-2-1}.ome.zarr` and\r\n`test_img_TRange{1-2-1}-ZRange{1-3-1}.ome.zarr`\r\n\r\nNote that `--concatenation_order` will override any dimension specifiers already\r\nexisting in the filenames.\r\n\r\n\r\n**Example 6**\r\n\r\nThere can be scenarios where the user may want to have further control over the axes along \r\nwhich to concatenate the images. For example, the filenames might contain the data acquisition\r\ndate, which can be recognised by BatchConvert as a concatenation axis in the automatic \r\ndetection mode. An example of such a fileset might look like:\r\n\r\n```\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ1-T1\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ1-T2\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ1-T3\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ2-T1\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ2-T2\r\nfilenames_with_dates/test_data_date03.03.2023_imageZ2-T3\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ1-T1\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ1-T2\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ1-T3\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ2-T1\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ2-T2\r\nfilenames_with_dates/test_data_date04.03.2023_imageZ2-T3\r\n```\r\n\r\nOne may try the following command to convert this folder:\r\n\r\n`batchconvert --omezarr --merge_files \"input_dir/filenames_with_dates\" \"output_path\"`\r\n\r\nSince the concatenation axes are not specified, this command would try to create\r\na single OME-Zarr with name: `test_data_dateRange{03-04-1}.03.2023_imageZRange{1-2-1}-TRange{1-3-1}`.\r\n\r\nIn order to force BatchConvert to ignore the date field, the user can restrict the concatenation \r\naxes to the last two numeric fields. This can be done by using a command such as: \\\r\n`batchconvert --omezarr --merge_files --concatenation_order aa \"input_dir/filenames_with_dates\" \"output_path\"` \\\r\nThis command will avoid concatenation along the date field, and therefore, there will be two\r\nOME-Zarrs corresponding to the two dates. The number of characters being passed to the \r\n`--concatenation_order` option specifies the number of numeric fields (starting from the right \r\nend of the filename) that are recognised by the BatchConvert as valid concatenation axes. \r\nPassing `aa`, therefore, means that the last two numeric fields must be recognised as \r\nconcatenation axes and the dimension type should be automatically detected (`a` for automatic). \r\nIn the same logic, one could, for example, convert each Z section into a separate OME-Zarr by \r\nspecifying `--concatenation_order a`.\r\n\r\n\r\n\r\n### Conversion on slurm\r\n\r\nAll the examples given above can also be run on slurm by specifying `-pf cluster` option. \r\nNote that this option automatically uses the singularity profile:\\\r\n`batchconvert omezarr -pf cluster -p .oir \"input_path\" \"output_path\"`\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.453.3",
        "edam_operation": [
            "Conversion",
            "Data handling",
            "Format detection"
        ],
        "edam_topic": [
            "Bioimaging",
            "Data management",
            "Imaging",
            "Medical imaging"
        ],
        "filtered_on": "profil.* in description",
        "id": "453",
        "keep": "Reject",
        "latest_version": 3,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/453?version=3",
        "name": "BatchConvert",
        "number_of_steps": 0,
        "projects": [
            "NGFF Tools",
            "Euro-BioImaging"
        ],
        "source": "WorkflowHub",
        "tags": [
            "biostudies",
            "conversion",
            "ngff",
            "nextflow",
            "ome-tiff",
            "ome-zarr",
            "python",
            "s3",
            "bash",
            "bioformats",
            "bioformats2raw",
            "bioimaging",
            "file conversion",
            "image file format",
            "imaging"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-07-11",
        "versions": 3
    },
    {
        "create_time": "2023-06-28",
        "creators": [
            "Peter van Heusden",
            "Bradley W. Langhorst"
        ],
        "description": "SARS-CoV-2 variant prediction using Read It And Keep, fastp, bbmap and iVar",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "519",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/519?version=1",
        "name": "SARS-CoV-2 Illumina Amplicon pipeline - SANBI - v1.2",
        "number_of_steps": 20,
        "projects": [
            "SANBI Pathogen Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "artic",
            "sanbi",
            "sars-cov-2",
            "covid-19"
        ],
        "tools": [
            "fastp",
            "compose_text_param",
            "samtools_stats",
            "samtools_view",
            "ivar_variants",
            "snpeff_sars_cov_2",
            "read_it_and_keep",
            "tp_awk_tool",
            "__FLATTEN__",
            "qualimap_bamqc",
            "tp_sed_tool",
            "collapse_dataset",
            "samtools_ampliconclip",
            "ivar_consensus",
            "bbtools_bbmap",
            "tp_cat",
            "multiqc",
            "deeptools_bam_coverage"
        ],
        "type": "Galaxy",
        "update_time": "2023-06-30",
        "versions": 1
    },
    {
        "create_time": "2023-06-29",
        "creators": [],
        "description": "",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in name",
        "id": "521",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/521?version=1",
        "name": "SARS-CoV-2 ONT Amplicon Sequencing SANBI 1.0",
        "number_of_steps": 5,
        "projects": [
            "SANBI Pathogen Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "sanbi",
            "sars-cov-2",
            "nanopore"
        ],
        "tools": [
            "nanoplot",
            "artic_guppyplex",
            "artic_minion",
            "read_it_and_keep"
        ],
        "type": "Galaxy",
        "update_time": "2023-06-29",
        "versions": 1
    },
    {
        "create_time": "2023-06-21",
        "creators": [],
        "description": "# CWL-assembly\r\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/684724bbc0134960ab41748f4a4b732f)](https://www.codacy.com/app/mb1069/CWL-assembly?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=EBI-Metagenomics/CWL-assembly&amp;utm_campaign=Badge_Grade)\r\n[![Build Status](https://travis-ci.org/EBI-Metagenomics/CWL-assembly.svg?branch=develop)](https://travis-ci.org/EBI-Metagenomics/CWL-assembly)\r\n\r\n## Description\r\n\r\nThis repository contains two workflows for metagenome and metatranscriptome assembly of short read data. MetaSPAdes is used as default for paired-end data, and MEGAHIT for single-end data and co-assemblies. MEGAHIT can be specified as the default assembler in the yaml file if preferred. Steps include:\r\n\r\n  * _QC_: removal of short reads, low quality regions, adapters and host decontamination\r\n  * _Assembly_: with metaSPADES or MEGAHIT\r\n  * _Post-assembly_: Host and PhiX decontamination, contig length filter (500bp), stats generation\r\n\r\n## Requirements - How to install\r\n\r\nThis pipeline requires a conda environment with cwltool, blastn, and metaspades. If created with `requirements.yml`, the environment will be called `cwl_assembly`. \r\n\r\n```\r\nconda env create -f requirements.yml\r\nconda activate cwl_assembly\r\npip install cwltool==3.1.20230601100705\r\n```\r\n\r\n## Databases\r\n\r\nYou will need to pre-download fasta files for host decontamination and generate the following databases accordingly:\r\n  * bwa index\r\n  * blast index\r\n    \r\nSpecify the locations in the yaml file when running the pipeline.\r\n\r\n## Main pipeline executables\r\n\r\n  * `src/workflows/metagenome_pipeline.cwl`\r\n  * `src/workflows/metatranscriptome_pipeline.cwl`\r\n\r\n## Example command\r\n\r\n```cwltool --singularity --outdir ${OUTDIR} ${CWL} ${YML}```\r\n\r\n`$CWL` is going to be one of the executables mentioned above\r\n`$YML` should be a config yaml file including entries among what follows. \r\nYou can find a yml template in the `examples` folder.\r\n\r\n## Example output directory structure\r\n```\r\nRoot directory\r\n    \u251c\u2500\u2500 megahit\r\n    \u2502   \u2514\u2500\u2500 001 -------------------------------- Assembly root directory\r\n    \u2502       \u251c\u2500\u2500 assembly_stats.json ------------ Human-readable assembly stats file\r\n    \u2502       \u251c\u2500\u2500 coverage.tab ------------------- Coverage file\r\n    \u2502       \u251c\u2500\u2500 log ---------------------------- CwlToil+megahit output log\r\n    |       \u251c\u2500\u2500 options.json ------------------- Megahit input options\r\n    \u2502       \u251c\u2500\u2500 SRR6257420.fasta.gz ------------ Archived and trimmed assembly\r\n    \u2502       \u2514\u2500\u2500 SRR6257420.fasta.gz.md5 -------- MD5 hash of above archive\r\n    \u251c\u2500\u2500 metaspades\r\n    \u2502   \u2514\u2500\u2500 001 -------------------------------- Assembly root directory\r\n    \u2502       \u251c\u2500\u2500 assembly_graph.fastg ----------- Assembly graph\r\n    \u2502       \u251c\u2500\u2500 assembly_stats.json ------------ Human-readable assembly stats file\r\n    \u2502       \u251c\u2500\u2500 coverage.tab ------------------- Coverage file\r\n    |       \u251c\u2500\u2500 params.txt --------------------- Metaspades input options\r\n    \u2502       \u251c\u2500\u2500 spades.log --------------------- Metaspades output log\r\n    \u2502       \u251c\u2500\u2500 SRR6257420.fasta.gz ------------ Archived and trimmed assembly\r\n    \u2502       \u2514\u2500\u2500 SRR6257420.fasta.gz.md5 -------- MD5 hash of above archive\r\n    \u2502\u00a0\r\n    \u2514\u2500\u2500 raw ------------------------------------ Raw data directory\r\n        \u251c\u2500\u2500 SRR6257420.fastq.qc_stats.tsv ------ Stats for cleaned fastq\r\n        \u251c\u2500\u2500 SRR6257420_fastp_clean_1.fastq.gz -- Cleaned paired-end file_1\r\n        \u2514\u2500\u2500 SRR6257420_fastp_clean_2.fastq.gz -- Cleaned paired-end file_2\r\n```\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in name",
        "id": "474",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/474?version=2",
        "name": "Metagenome and metatranscriptome assembly in CWL",
        "number_of_steps": 0,
        "projects": [
            "HoloFood at MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-06-21",
        "versions": 2
    },
    {
        "create_time": "2023-05-19",
        "creators": [],
        "description": "# EukRecover\r\nPipeline to recover eukaryotic MAGs using CONCOCT, metaBAT2 and EukCC's merging algorythm.\r\n\r\nNeeds paired end shotgun metagenomic reads.\r\n\r\n## Environment\r\n\r\nEukrecover requires an environment with snakemake and metaWRAP.\r\n\r\n## Quickstart\r\n\r\nDefine your samples in the file `samples.csv`.\r\nThis file needs to have the columns project and run to identify each metagenome. \r\n\r\nThis pipeline does not support co-binning, but feel free to change it. \r\n\r\nClone this repro wherever you want to run the pipeline:\r\n```\r\ngit clone https://github.com/openpaul/eukrecover/\r\n```\r\n\r\n\r\nYou can then run the snakemake like so\r\n\r\n```\r\nsnakemake --use-singularity\r\n```\r\n\r\nThe pipeline used dockerhub to fetch all tools, so make sure you have singularity installed.\r\n\r\n\r\n\r\n## Prepare databases\r\nThe pipeline will setup databases for you, but if you already have a EukCC or a BUSCO 5 database you can use them \r\nby specifying the location in the file `config/config.yaml`\r\n\r\n\r\n## Output:\r\nIn the folder results you will find a folder `MAGs` which will contain a folder\r\n`fa` containing the actual MAG fastas.\r\nIn addition you will find stats for each MAG in the table `QC.csv`.\r\n\r\nThis table contains the following columns:\r\n\r\nname,eukcc_compl,eukcc_cont,BUSCO_C,BUSCO_M,BUSCO_D,BUSCO_F,BUSCO_tax,N50,bp\r\n\r\n\r\n\r\n## Citation:\r\n\r\nIf you use this pipeline please make sure to cite all used software. \r\n\r\nFor this please reffer to the used rules.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "475",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/475?version=1",
        "name": "EukRecover",
        "number_of_steps": 0,
        "projects": [
            "HoloFood at MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-05-19",
        "versions": 1
    },
    {
        "create_time": "2023-05-17",
        "creators": [],
        "description": "The radiation source ELBE (Electron Linac for beams with high Brilliance and low Emittance) at the Helmholtz Centre Dresden Rossendorf (HZDR) can produce several kinds of secondary radiations. THz radiation is one of them and can be used with a typical pulse frequency of 100 kHz as a stimulation source for elementary low-energy degrees of freedom in matter. To sample the whole THz wave the laser path length is modified by moving specific mirrors. The raw data contains for each mirror position a binary file storing the signal spectra and a folder with gray scaled tiff files storing the jitter timing. This Workflow is equivalent to the first part of the standalone jupyter notebook https://github.com/hzdr/TELBE-raw-data-evaluation/blob/main/sorting_binning.ipynb\r\n\r\nIn the job file the folder < FOLDER_BASE> and < FOLDER_SUB> needs to be specified and the parameters as a json string like < PARAMS> = { \"rep\": 100000, \"t_exp\": 1, \"N_sample\": 96, \"offset\": 0, \"pixel_to_ps\": 0.0115, \"Stage_zero\": 0 }\r\n\r\nThe python file which is used is originally published in gitlab https://codebase.helmholtz.cloud/science2workflow/telbe-sorting-binning/-/blob/master/src/ The workflow can automatically be monitored in Heliport if the project number < HELIPORT_PROJECT> is provided.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "binn.* in description",
        "id": "473",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/473?version=1",
        "name": "Sorting and registration of Terahertz ELBE raw data",
        "number_of_steps": 0,
        "projects": [
            "Helmholtz Scientific Project Workflow Platform"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Uniform Interface to Computing Resources",
        "update_time": "2023-06-07",
        "versions": 1
    },
    {
        "create_time": "2023-05-16",
        "creators": [
            "Haris Zafeiropoulos",
            "Martin Beracochea"
        ],
        "description": "# metaGOflow: A workflow for marine Genomic Observatories' data analysis\r\n\r\n![logo](https://raw.githubusercontent.com/hariszaf/metaGOflow-use-case/gh-pages/assets/img/metaGOflow_logo_italics.png)\r\n\r\n\r\n## An EOSC-Life project\r\n\r\nThe workflows developed in the framework of this project are based on `pipeline-v5` of the MGnify resource.\r\n\r\n> This branch is a child of the [`pipeline_5.1`](https://github.com/hariszaf/pipeline-v5/tree/pipeline_5.1) branch\r\n> that contains all CWL descriptions of the MGnify pipeline version 5.1.\r\n\r\n## Dependencies\r\n\r\nTo run metaGOflow you need to make sure you have the following set on your computing environmnet first:\r\n\r\n- python3 [v 3.8+]\r\n- [Docker](https://www.docker.com) [v 19.+] or [Singularity](https://apptainer.org) [v 3.7.+]/[Apptainer](https://apptainer.org) [v 1.+]\r\n- [cwltool](https://github.com/common-workflow-language/cwltool) [v 3.+]\r\n- [rdflib](https://rdflib.readthedocs.io/en/stable/) [v 6.+]\r\n- [rdflib-jsonld](https://pypi.org/project/rdflib-jsonld/) [v 0.6.2]\r\n- [ro-crate-py](https://github.com/ResearchObject/ro-crate-py) [v 0.7.0]\r\n- [pyyaml](https://pypi.org/project/PyYAML/) [v 6.0]\r\n- [Node.js](https://nodejs.org/) [v 10.24.0+]\r\n- Available storage ~235GB for databases\r\n\r\n### Storage while running\r\n\r\nDepending on the analysis you are about to run, disk requirements vary.\r\nIndicatively, you may have a look at the metaGOflow publication for computing resources used in various cases.\r\n\r\n## Installation\r\n\r\n### Get the EOSC-Life marine GOs workflow\r\n\r\n```bash\r\ngit clone https://github.com/emo-bon/MetaGOflow\r\ncd MetaGOflow\r\n```\r\n\r\n### Download necessary databases (~235GB)\r\n\r\nYou can download databases for the EOSC-Life GOs workflow by running the\r\n`download_dbs.sh` script under the `Installation` folder.\r\n\r\n```bash\r\nbash Installation/download_dbs.sh -f [Output Directory e.g. ref-dbs] \r\n```\r\nIf you have one or more already in your system, then create a symbolic link pointing\r\nat the `ref-dbs` folder or at one of its subfolders/files.\r\n\r\nThe final structure of the DB directory should be like the following:\r\n\r\n````bash\r\nuser@server:~/MetaGOflow: ls ref-dbs/\r\ndb_kofam/  diamond/  eggnog/  GO-slim/  interproscan-5.57-90.0/  kegg_pathways/  kofam_ko_desc.tsv  Rfam/  silva_lsu/  silva_ssu/\r\n````\r\n\r\n## How to run\r\n\r\n### Ensure that `Node.js` is installed on your system before running metaGOflow\r\n\r\nIf you have root access on your system, you can run the commands below to install it:\r\n\r\n##### DEBIAN/UBUNTU\r\n```bash\r\nsudo apt-get update -y\r\nsudo apt-get install -y nodejs\r\n```\r\n\r\n##### RH/CentOS\r\n```bash\r\nsudo yum install rh-nodejs<stream version> (e.g. rh-nodejs10)\r\n```\r\n\r\n### Set up the environment\r\n\r\n#### Run once - Setup environment\r\n\r\n- ```bash\r\n  conda create -n EOSC-CWL python=3.8\r\n  ```\r\n\r\n- ```bash\r\n  conda activate EOSC-CWL\r\n  ```\r\n\r\n- ```bash\r\n  pip install cwlref-runner cwltool[all] rdflib-jsonld rocrate pyyaml\r\n\r\n  ```\r\n\r\n#### Run every time\r\n\r\n```bash\r\nconda activate EOSC-CWL\r\n``` \r\n\r\n### Run the workflow\r\n\r\n- Edit the `config.yml` file to set the parameter values of your choice. For selecting all the steps, then set to `true` the variables in lines [2-6].\r\n\r\n#### Using Singularity\r\n\r\n##### Standalone\r\n- run:\r\n   ```bash\r\n   ./run_wf.sh -s -n osd-short -d short-test-case -f test_input/wgs-paired-SRR1620013_1.fastq.gz -r test_input/wgs-paired-SRR1620013_2.fastq.gz\r\n   ``\r\n\r\n##### Using a cluster with a queueing system (e.g. SLURM)\r\n\r\n- Create a job file (e.g., SBATCH file)\r\n\r\n- Enable Singularity, e.g. module load Singularity & all other dependencies \r\n\r\n- Add the run line to the job file\r\n\r\n\r\n#### Using Docker\r\n\r\n##### Standalone\r\n- run:\r\n    ``` bash\r\n    ./run_wf.sh -n osd-short -d short-test-case -f test_input/wgs-paired-SRR1620013_1.fastq.gz -r test_input/wgs-paired-SRR1620013_2.fastq.gz\r\n  ```\r\n  HINT: If you are using Docker, you may need to run the above command without the `-s' flag.\r\n\r\n## Testing samples\r\nThe samples are available in the `test_input` folder.\r\n\r\nWe provide metaGOflow with partial samples from the Human Metagenome Project ([SRR1620013](https://www.ebi.ac.uk/ena/browser/view/SRR1620013) and [SRR1620014](https://www.ebi.ac.uk/ena/browser/view/SRR1620014))\r\nThey are partial as only a small part of their sequences have been kept, in terms for the pipeline to test in a fast way. \r\n\r\n\r\n## Hints and tips\r\n\r\n1. In case you are using Docker, it is strongly recommended to **avoid** installing it through `snap`.\r\n\r\n2. `RuntimeError`: slurm currently does not support shared caching, because it does not support cleaning up a worker\r\n   after the last job finishes.\r\n   Set the `--disableCaching` flag if you want to use this batch system.\r\n\r\n3. In case you are having errors like:\r\n\r\n```\r\ncwltool.errors.WorkflowException: Singularity is not available for this tool\r\n```\r\n\r\nYou may run the following command:\r\n\r\n```\r\nsingularity pull --force --name debian:stable-slim.sif docker://debian:stable-sli\r\n```\r\n\r\n## Contribution\r\n\r\nTo make contribution to the project a bit easier, all the MGnify `conditionals` and `subworkflows` under\r\nthe `workflows/` directory that are not used in the metaGOflow framework, have been removed.   \r\nHowever, all the MGnify `tools/` and `utils/` are available in this repo, even if they are not invoked in the current\r\nversion of metaGOflow.\r\nThis way, we hope we encourage people to implement their own `conditionals` and/or `subworkflows` by exploiting the\r\ncurrently supported `tools` and `utils` as well as by developing new `tools` and/or `utils`.\r\n\r\n\r\n<!-- cwltool --print-dot my-wf.cwl | dot -Tsvg > my-wf.svg -->\r\n",
        "doi": "10.48546/workflowhub.workflow.384.3",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "384",
        "keep": "Keep",
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/384?version=3",
        "name": "A workflow for marine Genomic Observatories data analysis",
        "number_of_steps": 0,
        "projects": [
            "emo-bon"
        ],
        "source": "WorkflowHub",
        "tags": [
            "biodiversity"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-05-16",
        "versions": 3
    },
    {
        "create_time": "2023-03-21",
        "creators": [
            "Zavolan Lab"
        ],
        "description": "[![ci](https://github.com/zavolanlab/zarp/workflows/CI/badge.svg?branch=dev)](https://github.com/zavolanlab/zarp/actions?query=workflow%3Aci)\r\n[![GitHub license](https://img.shields.io/github/license/zavolanlab/zarp?color=orange)](https://github.com/zavolanlab/zarp/blob/dev/LICENSE)\r\n[![DOI:10.1101/2021.11.18.469017](http://img.shields.io/badge/DOI-10.1101/2021.11.18.469017-B31B1B.svg)](https://doi.org/10.1101/2021.11.18.469017)\r\n\r\n\r\n<div align=\"left\">\r\n    <img width=\"20%\" align=\"left\" src=https://raw.githubusercontent.com/zavolanlab/zarp/2bdf65deae5d4ffacc4b1a600d7d9ed425614255/images/zarp_logo.svg>\r\n</div> \r\n\r\n\r\n# **ZARP** ([Zavolan-Lab](https://www.biozentrum.unibas.ch/research/researchgroups/overview/unit/zavolan/research-group-mihaela-zavolan/) Automated RNA-Seq Pipeline) \r\n...is a generic RNA-Seq analysis workflow that allows \r\nusers to process and analyze Illumina short-read sequencing libraries with minimum effort. The workflow relies on \r\npublicly available bioinformatics tools and currently handles single or paired-end stranded bulk RNA-seq data.\r\nThe workflow is developed in [Snakemake](https://snakemake.readthedocs.io/en/stable/), a widely used workflow management system in the bioinformatics\r\ncommunity.\r\n\r\nAccording to the current ZARP implementation, reads are analyzed (pre-processed, aligned, quantified) with state-of-the-art\r\ntools to give meaningful initial insights into the quality and composition of an RNA-Seq library, reducing hands-on time for bioinformaticians and giving experimentalists the possibility to rapidly assess their data. Additional reports summarise the results of the individual steps and provide useful visualisations.\r\n\r\n\r\n> **Note:** For a more detailed description of each step, please refer to the [workflow\r\n> documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md).\r\n\r\n\r\n## Requirements\r\n\r\nThe workflow has been tested on:\r\n- CentOS 7.5\r\n- Debian 10\r\n- Ubuntu 16.04, 18.04\r\n\r\n> **NOTE:**\r\n> Currently, we only support **Linux** execution. \r\n\r\n\r\n# Installation\r\n\r\n## 1. Clone the repository\r\n\r\nGo to the desired directory/folder on your file system, then clone/get the \r\nrepository and move into the respective directory with:\r\n\r\n```bash\r\ngit clone https://github.com/zavolanlab/zarp.git\r\ncd zarp\r\n```\r\n\r\n## 2. Conda and Mamba installation\r\n\r\nWorkflow dependencies can be conveniently installed with the [Conda](http://docs.conda.io/projects/conda/en/latest/index.html)\r\npackage manager. We recommend that you install [Miniconda](https://docs.conda.io/en/latest/miniconda.html) \r\nfor your system (Linux). Be sure to select Python 3 option. \r\nThe workflow was built and tested with `miniconda 4.7.12`.\r\nOther versions are not guaranteed to work as expected.\r\n\r\nGiven that Miniconda has been installed and is available in the current shell the first\r\ndependency for ZARP is the [Mamba](https://github.com/mamba-org/mamba) package manager, which needs to be installed in\r\nthe `base` conda environment with:\r\n\r\n```bash\r\nconda install mamba -n base -c conda-forge\r\n```\r\n\r\n## 3. Dependencies installation\r\n\r\nFor improved reproducibility and reusability of the workflow,\r\neach individual step of the workflow runs either in its own [Singularity](https://sylabs.io/singularity/)\r\ncontainer or in its own [Conda](http://docs.conda.io/projects/conda/en/latest/index.html) virtual environemnt. \r\nAs a consequence, running this workflow has very few individual dependencies. \r\nThe **container execution** requires Singularity to be installed on the system where the workflow is executed. \r\nAs the functional installation of Singularity requires root privileges, and Conda currently only provides Singularity\r\nfor Linux architectures, the installation instructions are slightly different depending on your system/setup:\r\n\r\n### For most users\r\n\r\nIf you do *not* have root privileges on the machine you want\r\nto run the workflow on *or* if you do not have a Linux machine, please [install\r\nSingularity](https://sylabs.io/guides/3.5/admin-guide/installation.html) separately and in privileged mode, depending\r\non your system. You may have to ask an authorized person (e.g., a systems\r\nadministrator) to do that. This will almost certainly be required if you want\r\nto run the workflow on a high-performance computing (HPC) cluster. \r\n\r\n> **NOTE:**\r\n> The workflow has been tested with the following Singularity versions:  \r\n>  * `v2.6.2`\r\n>  * `v3.5.2`\r\n\r\nAfter installing Singularity, install the remaining dependencies with:\r\n```bash\r\nmamba env create -f install/environment.yml\r\n```\r\n\r\n\r\n### As root user on Linux\r\n\r\nIf you have a Linux machine, as well as root privileges, (e.g., if you plan to\r\nrun the workflow on your own computer), you can execute the following command\r\nto include Singularity in the Conda environment:\r\n\r\n```bash\r\nmamba env update -f install/environment.root.yml\r\n```\r\n\r\n## 4. Activate environment\r\n\r\nActivate the Conda environment with:\r\n\r\n```bash\r\nconda activate zarp\r\n```\r\n\r\n# Extra installation steps (optional)\r\n\r\n## 5. Non-essential dependencies installation\r\n\r\nMost tests have additional dependencies. If you are planning to run tests, you\r\nwill need to install these by executing the following command _in your active\r\nConda environment_:\r\n\r\n```bash\r\nmamba env update -f install/environment.dev.yml\r\n```\r\n\r\n## 6. Successful installation tests\r\n\r\nWe have prepared several tests to check the integrity of the workflow and its\r\ncomponents. These can be found in subdirectories of the `tests/` directory. \r\nThe most critical of these tests enable you to execute the entire workflow on a \r\nset of small example input files. Note that for this and other tests to complete\r\nsuccessfully, [additional dependencies](#installing-non-essential-dependencies) \r\nneed to be installed. \r\nExecute one of the following commands to run the test workflow \r\non your local machine:\r\n* Test workflow on local machine with **Singularity**:\r\n```bash\r\nbash tests/test_integration_workflow/test.local.sh\r\n```\r\n* Test workflow on local machine with **Conda**:\r\n```bash\r\nbash tests/test_integration_workflow_with_conda/test.local.sh\r\n```\r\nExecute one of the following commands to run the test workflow \r\non a [Slurm](https://slurm.schedmd.com/documentation.html)-managed high-performance computing (HPC) cluster:\r\n\r\n* Test workflow with **Singularity**:\r\n\r\n```bash\r\nbash tests/test_integration_workflow/test.slurm.sh\r\n```\r\n* Test workflow with **Conda**:\r\n\r\n```bash\r\nbash tests/test_integration_workflow_with_conda/test.slurm.sh\r\n```\r\n\r\n> **NOTE:** Depending on the configuration of your Slurm installation you may\r\n> need to adapt file `slurm-config.json` (located directly under `profiles`\r\n> directory) and the arguments to options `--cores` and `--jobs`\r\n> in the file `config.yaml` of a respective profile.\r\n> Consult the manual of your workload manager as well as the section of the\r\n> Snakemake manual dealing with [profiles].\r\n\r\n# Running the workflow on your own samples\r\n\r\n1. Assuming that your current directory is the repository's root directory,\r\ncreate a directory for your workflow run and move into it with:\r\n\r\n    ```bash\r\n    mkdir config/my_run\r\n    cd config/my_run\r\n    ```\r\n\r\n2. Create an empty sample table and a workflow configuration file:\r\n\r\n    ```bash\r\n    touch samples.tsv\r\n    touch config.yaml\r\n    ```\r\n\r\n3. Use your editor of choice to populate these files with appropriate\r\nvalues. Have a look at the examples in the `tests/` directory to see what the\r\nfiles should look like, specifically:\r\n\r\n    - [samples.tsv](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/samples.tsv)\r\n    - [config.yaml](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/config.yaml)\r\n\r\n    - For more details and explanations, refer to the [pipeline-documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md)\r\n\r\n\r\n4. Create a runner script. Pick one of the following choices for either local\r\nor cluster execution. Before execution of the respective command, you need to\r\nremember to update the argument of the `--singularity-args` option of a\r\nrespective profile (file: `profiles/{profile}/config.yaml`) so that\r\nit contains a comma-separated list of _all_ directories\r\ncontaining input data files (samples and any annotation files etc) required for\r\nyour run.\r\n\r\n    Runner script for _local execution_:\r\n\r\n    ```bash\r\n    cat << \"EOF\" > run.sh\r\n    #!/bin/bash\r\n\r\n    snakemake \\\r\n        --profile=\"../../profiles/local-singularity\" \\\r\n        --configfile=\"config.yaml\"\r\n\r\n    EOF\r\n    ```\r\n\r\n    **OR**\r\n\r\n    Runner script for _Slurm cluster exection_ (note that you may need\r\n    to modify the arguments to `--jobs` and `--cores` in the file:\r\n    `profiles/slurm-singularity/config.yaml` depending on your HPC\r\n    and workload manager configuration):\r\n\r\n    ```bash\r\n    cat << \"EOF\" > run.sh\r\n    #!/bin/bash\r\n    mkdir -p logs/cluster_log\r\n    snakemake \\\r\n        --profile=\"../profiles/slurm-singularity\" \\\r\n        --configfile=\"config.yaml\"\r\n    EOF\r\n    ```\r\n\r\n    When running the pipeline with *conda* you should use `local-conda` and\r\n    `slurm-conda` profiles instead.\r\n\r\n5. Start your workflow run:\r\n\r\n    ```bash\r\n    bash run.sh\r\n    ```\r\n\r\n# Sample downloads from SRA\r\n\r\nAn independent Snakemake workflow `workflow/rules/sra_download.smk` is included\r\nfor the download of SRA samples with [sra-tools].\r\n\r\n> Note: as of Snakemake 7.3.1, only profile conda is supported. \r\n> Singularity fails because the *sra-tools* Docker container only has `sh` \r\nbut `bash` is required.\r\n\r\n> Note: The workflow uses the implicit temporary directory \r\nfrom snakemake, which is called with [resources.tmpdir].\r\n\r\nThe workflow expects the following config:\r\n* `samples`, a sample table (tsv) with column *sample* containing *SRR* identifiers,\r\nsee example [here](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/sra_samples.tsv).\r\n* `outdir`, an output directory\r\n* `samples_out`, a pointer to a modified sample table with location of fastq files\r\n* `cluster_log_dir`, the cluster log directory.\r\n\r\nFor executing the example one can use the following\r\n(with activated *zarp* environment):\r\n\r\n```bash\r\nsnakemake --snakefile=\"workflow/rules/sra_download.smk\" \\\r\n          --profile=\"profiles/local-conda\" \\\r\n          --config samples=\"tests/input_files/sra_samples.tsv\" \\\r\n                   outdir=\"results/sra_downloads\" \\\r\n                   samples_out=\"results/sra_downloads/sra_samples.out.tsv\" \\\r\n                   log_dir=\"logs\" \\\r\n                   cluster_log_dir=\"logs/cluster_log\"\r\n```\r\nAfter successful execution, `results/sra_downloads/sra_samples.out.tsv` should contain:\r\n```tsv\r\nsample\tfq1\tfq2\r\nSRR18552868\tresults/sra_downloads/SRR18552868/SRR18552868.fastq.gz\t\r\nSRR18549672\tresults/sra_downloads/SRR18549672/SRR18549672_1.fastq.gz\tresults/sra_downloads/SRR18549672/SRR18549672_2.fastq.gz\r\n```\r\n\r\n\r\n# Metadata completion with HTSinfer\r\nAn independent Snakemake workflow `workflow/rules/htsinfer.smk` that populates the `samples.tsv` required by ZARP with the sample specific parameters `seqmode`, `f1_3p`, `f2_3p`, `organism`, `libtype` and `index_size`. Those parameters are inferred from the provided `fastq.gz` files by [HTSinfer](https://github.com/zavolanlab/htsinfer).\r\n\r\n> Note: The workflow uses the implicit temporary directory \r\nfrom snakemake, which is called with [resources.tmpdir].\r\n\r\n\r\nThe workflow expects the following config:\r\n* `samples`, a sample table (tsv) with column *sample* containing sample identifiers, as well as columns *fq1* and *fq2* containing the paths to the input fastq files\r\nsee example [here](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/sra_samples.tsv). If the table contains further ZARP compatible columns (see [pipeline documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md#read-sample-table)), the values specified there by the user are given priority over htsinfer's results. \r\n* `outdir`, an output directory\r\n* `samples_out`, path to a modified sample table with inferred parameters\r\n* `records`, set to 100000 per default\r\n  \r\nFor executing the example one can use the following\r\n(with activated *zarp* environment):\r\n```bash\r\ncd tests/test_htsinfer_workflow\r\nsnakemake \\\r\n    --snakefile=\"../../workflow/rules/htsinfer.smk\" \\\r\n    --restart-times=0 \\\r\n    --profile=\"../../profiles/local-singularity\" \\\r\n    --config outdir=\"results\" \\\r\n             samples=\"../input_files/htsinfer_samples.tsv\" \\\r\n             samples_out=\"samples_htsinfer.tsv\" \\\r\n    --notemp \\\r\n    --keep-incomplete\r\n```\r\n\r\nHowever, this call will exit with an error, as not all parameters can be inferred from the example files. The argument `--keep-incomplete` makes sure the `samples_htsinfer.tsv` file can nevertheless be inspected. \r\n\r\nAfter successful execution - if all parameters could be either inferred or were specified by the user - `[OUTDIR]/[SAMPLES_OUT]` should contain a populated table with parameters `seqmode`, `f1_3p`, `f2_3p`, `organism`, `libtype` and `index_size` for all input samples as described in the [pipeline documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md#read-sample-table).\r\n\r\n",
        "doi": "10.48546/workflowhub.workflow.447.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "447",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/447?version=1",
        "name": "ZARP: An automated workflow for processing of RNA-seq data",
        "number_of_steps": 0,
        "projects": [
            "Zavolan Lab"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "ngs",
            "rnaseq",
            "high-throughput",
            "rna",
            "rna-seq"
        ],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-05-12",
        "versions": 1
    },
    {
        "create_time": "2023-04-12",
        "creators": [
            "Andrey Prjibelski",
            "Varsha Kale",
            "Anton Korobeynikov"
        ],
        "description": "**Assembly and quantification metatranscriptome using metagenome data**.\r\n\r\nVersion: see VERSION\r\n\r\n## Introduction\r\n\r\n**MetaGT** is a bioinformatics analysis pipeline used for improving and quantification \r\nmetatranscriptome assembly using metagenome data. The pipeline supports Illumina sequencing \r\ndata and complete metagenome and metatranscriptome assemblies. The pipeline involves the \r\nalignment of metatranscriprome assembly to the metagenome assembly with further extracting CDSs,\r\nwhich are covered by transcripts.\r\n\r\nThe pipeline is built using Nextflow, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible. The Nextflow DSL2 implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies.\r\n\r\n[![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.04.0-brightgreen.svg)](https://www.nextflow.io/)\r\n\r\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg)](https://bioconda.github.io/)\r\n\r\n## Quick Start\r\n\r\n1. Install [`nextflow`](https://nf-co.re/usage/installation)\r\n\r\n2. Install any of [`Conda`](https://conda.io/miniconda.html) for full pipeline reproducibility \r\n\r\n3. Download the pipeline, e.g. by cloning metaGT GitHub repository:\r\n\r\n    ```bash\r\n    git clone git@github.com:ablab/metaGT.git\r\n    ```\r\n   \r\n4. Test it on a minimal dataset by running:\r\n\r\n    ```bash\r\n    nextflow run metaGT -profile test,conda\r\n    ```\r\n   \r\n5. Start running your own analysis!\r\n    > Typical command for analysis using reads:\r\n\r\n    ```bash\r\n    nextflow run metaGT -profile <conda> --dna_reads '*_R{1,2}.fastq.gz' --rna_reads '*_R{1,2}.fastq.gz'\r\n    ```\r\n    > Typical command for analysis using multiple files with reads:\r\n\r\n    ```bash\r\n    nextflow run metaGT -profile <conda> --dna_reads '*.yaml' --rna_reads '*.yaml' --yaml\r\n    ```\r\n    > Typical command for analysis using assemblies:\r\n\r\n    ```bash\r\n    nextflow run metaGT -profile <conda> --genome '*.fasta' --transcriptome '*.fasta'\r\n    ```\r\n## Pipeline Summary\r\nOptionally, if raw reades are used:\r\n\r\n<!-- TODO nf-core: Fill in short bullet-pointed list of default steps of pipeline -->\r\n\r\n* Sequencing quality control (`FastQC`)\r\n* Assembly metagenome or metatranscriptome (`metaSPAdes, rnaSPAdes `)\r\n\r\nBy default, the pipeline currently performs the following:\r\n\r\n* Annotation metagenome (`Prokka`)\r\n* Aligning metatranscriptome on metagenome (`minimap2`)\r\n* Annotation unaligned transcripts (`TransDecoder`)\r\n* Clustering covered CDS and CDS from unaligned transcripts (`MMseqs2`)\r\n* Quantifying abundances of transcripts (`kallisto`)\r\n\r\n## Citation\r\n\r\nMetaGT was developed by Daria Shafranskaya and Andrey Prjibelski.\r\nIf you use it in your research please cite:\r\n\r\n[MetaGT: A pipeline for de novo assembly of metatranscriptomes with the aid of metagenomic data](https://doi.org/10.3389/fmicb.2022.981458)\r\n\r\n## Feedback and bug report\r\n\r\nIf you have any questions, please leave an issue at out [GitHub page](https://github.com/ablab/metaGT/issues).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "454",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/454?version=1",
        "name": "MetaGT: A pipeline for de novo assembly of metatranscriptomes with the aid of metagenomic data",
        "number_of_steps": 0,
        "projects": [
            "HoloFood at MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "multi-omics",
            "expression",
            "metatranscriptomics"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-04-13",
        "versions": 1
    },
    {
        "create_time": "2022-04-21",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst",
            "Germ\u00e1n Royval"
        ],
        "description": "### Workflow for LongRead Quality Control and Filtering\r\n\r\n- NanoPlot  (read quality control) before and after filtering\r\n- Filtlong  (read trimming)\r\n- Kraken2 taxonomic read classification before and after filtering\r\n- Minimap2 read filtering based on given references<br><br>\r\n\r\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br><br>\r\n\r\n**All tool CWL files and other workflows can be found here:**<br>\r\nhttps://gitlab.com/m-unlock/cwl/workflows\r\n\r\n**How to setup and use an UNLOCK workflow:**<br>\r\nhttps://m-unlock.gitlab.io/docs/setup/setup.html<br>\r\n",
        "doi": null,
        "edam_operation": [
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Metagenomic sequencing",
            "Metagenomics",
            "Sequence analysis",
            "Sequencing"
        ],
        "filtered_on": "edam",
        "id": "337",
        "keep": "Keep",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/337?version=1",
        "name": "LongRead Quality Control and Filtering",
        "number_of_steps": 9,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "cwl",
            "genomics",
            "nanopore"
        ],
        "tools": [
            "Converts the file array to a single file object",
            "Removal of contaminated reads using minimap2 mapping",
            "Preparation of fastp output files to a specific output folder",
            "Quality assessment and report of reads before filter",
            "Merge fastq files",
            "Prepare BBMap references to a single fasta file and unique headers",
            "Visualization of Kraken2 classification with Krona",
            "Taxonomic classification of FASTQ reads"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-04-07",
        "versions": 1
    },
    {
        "create_time": "2023-03-30",
        "creators": [
            "Ekaterina Sakharova",
            "Martin Beracochea"
        ],
        "description": "The containerised pipeline for profiling shotgun metagenomic data is derived from the [MGnify](https://www.ebi.ac.uk/metagenomics/) pipeline raw-reads analyses, a well-established resource used for analyzing microbiome data.\r\nKey components:\r\n- Quality control and decontamination\r\n- rRNA and ncRNA detection using Rfam database\r\n- Taxonomic classification of SSU and LSU regions \r\n- Abundance analysis with mOTUs",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "450",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/450?version=1",
        "name": "MGnify raw reads taxonomic profiling pipeline",
        "number_of_steps": 0,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics",
            "nextflow"
        ],
        "tools": [
            "Biopython",
            "Metagenomic operational taxonomic units (mOTUs)",
            "MAPseq",
            "Infernal"
        ],
        "type": "Nextflow",
        "update_time": "2023-03-30",
        "versions": 1
    },
    {
        "create_time": "2023-03-21",
        "creators": [],
        "description": "# SNP-Calling\r\nGATK Variant calling pipeline for genomic data using Nextflow\r\n\r\n[![nextflow](https://img.shields.io/badge/nextflow-%E2%89%A522.04.5-brightgreen.svg)](http://nextflow.io)\r\n\r\n## Quickstart\r\n\r\nInstall Nextflow using the following command: \r\n\r\n    curl -s https://get.nextflow.io | bash\r\n  \r\nIndex reference genome:\r\n\r\n  `$ bwa index /path/to/reference/genome.fa`\r\n \r\n  `$ samtools faidx /path/to/reference/genome.fa`\r\n  \r\n  `$ gatk CreateSequenceDictionary -R /path/to/genome.fa -O genome.dict`\r\n\r\nLaunch the pipeline execution with the following command:\r\n\r\n    nextflow run jdetras/snp-calling -r main -profile docker\r\n  \r\n## Pipeline Description\r\n\r\nThe variant calling pipeline follows the recommended practices from GATK. The input genomic data are aligned to a reference genome using BWA. The alignemnt files are processed using Picard Tools. Variant calling is done using samtools and GATK. \r\n\r\n## Input files\r\n\r\nThe input files required to run the pipeline:\r\n* Genomic sequence paired reads, `*_{1,2}.fq.gz`\r\n* Reference genome, `*.fa`\r\n\r\n## Pipeline parameters\r\n\r\n### Usage\r\nUsage: `nextflow run jdetras/snp-calling -profile docker [options]`\r\n\r\nOptions:\r\n\r\n* `--reads` \r\n* `--genome`\r\n* `--output`\r\n\r\nExample: \r\n  `$ nextflow run jdetras/snp-calling -profile docker --reads '/path/to/reads/*_{1,2}.fq.gz' --genome '/path/to/reference/genome.fa' --output '/path/to/output'`\r\n\r\n#### `--reads`\r\n\r\n* The path to the FASTQ read files.\r\n* Wildcards (*, ?) can be used to declare multiple reads. Use single quotes when wildcards are used. \r\n* Default parameter: `$projectDir/data/reads/*_{1,2}.fq.gz`\r\n\r\nExample: \r\n  `$ nextflow run jdetras/snp-calling -profile docker --reads '/path/to/reads/*_{1,2}.fq.gz'`\r\n  \r\n#### `--genome`\r\n\r\n* The path to the genome file in fasta format.\r\n* The extension is `.fa`.\r\n* Default parameter: `$projectDir/data/reference/genome.fa`\r\n\r\nExample:\r\n  `$ nextflow run jdetras/snp-calling -profile docker --genome /path/to/reference/genome.fa`\r\n    \r\n#### `--output`\r\n\r\n* The path to the directory for the output files.\r\n* Default parameter: `$projectDir/output`\r\n\r\n## Software\r\n\r\n* [BWA 0.7.17](http://bio-bwa.sourceforge.net/)\r\n* [Samtools 1.3.1](http://www.htslib.org/)\r\n* [GATK 4.2.6.1](https://gatk.broadinstitute.org/) \r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "442",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/442?version=1",
        "name": "SNP-Calling Workflow",
        "number_of_steps": 0,
        "projects": [
            "IRRI Bioinformatics Group"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bwa-mem",
            "gatk4",
            "rice",
            "variant calling"
        ],
        "tools": [
            "GATK",
            "BWA"
        ],
        "type": "Nextflow",
        "update_time": "2023-03-21",
        "versions": 1
    },
    {
        "create_time": "2023-02-20",
        "creators": [],
        "description": "# MoP2- DSL2 version of Master of Pores\r\n[![Docker Build Status](https://img.shields.io/docker/automated/biocorecrg/nanopore.svg)](https://cloud.docker.com/u/biocorecrg/repository/docker/biocorecrg/nanopore/builds)\r\n[![mop2-CI](https://github.com/biocorecrg/MoP2/actions/workflows/build.yml/badge.svg)](https://github.com/biocorecrg/MoP2/actions/workflows/build.yml)\r\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\r\n[![Nextflow version](https://img.shields.io/badge/Nextflow-21.04.1-brightgreen)](https://www.nextflow.io/)\r\n[![Nextflow DSL2](https://img.shields.io/badge/Nextflow-DSL2-brightgreen)](https://www.nextflow.io/)\r\n[![Singularity version](https://img.shields.io/badge/Singularity-v3.2.1-green.svg)](https://www.sylabs.io/)\r\n[![Docker version](https://img.shields.io/badge/Docker-v20.10.8-blue)](https://www.docker.com/)\r\n\r\n<br/>\r\n\r\n![MOP2](https://github.com/biocorecrg/MoP2/blob/main/img/master_red.jpg?raw=true)\r\n\r\n\r\nInspired by Metallica's [Master Of Puppets](https://www.youtube.com/watch?v=S7blkui3nQc)\r\n\r\n## Install\r\nPlease install nextflow and singularity or docker before.\r\n\r\nThen download the repo:\r\n\r\n```\r\ngit clone --depth 1 --recurse-submodules git@github.com:biocorecrg/MOP2.git\r\n```\r\n\r\nYou can use INSTALL.sh to download the version 3.4.5 of guppy or you can replace it with the version you prefer. Please consider that the support of VBZ compression of fast5 started with version 3.4.X. \r\n\r\n```\r\ncd MoP2; sh INSTALL.sh\r\n```\r\n\r\n## Testing\r\nYou can replace ```-with-singularity``` with ```-with-docker``` if you want to use the docker engine.\r\n\r\n```\r\ncd mop_preprocess\r\nnextflow run mop_preprocess.nf -with-singularity -bg -profile local > log\r\n\r\n```\r\n\r\n## Reference\r\nIf you use this tool, please cite our papers:\r\n\r\n[\"Nanopore Direct RNA Sequencing Data Processing and Analysis Using MasterOfPores\"\r\nCozzuto L, Delgado-Tejedor A, Hermoso Pulido T, Novoa EM, Ponomarenko J. *N. Methods Mol Biol. 2023*;2624:185-205. doi: 10.1007/978-1-0716-2962-8_13.](https://link.springer.com/protocol/10.1007/978-1-0716-2962-8_13)\r\n\r\n[\"MasterOfPores: A Workflow for the Analysis of Oxford Nanopore Direct RNA Sequencing Datasets\"\r\nLuca Cozzuto, Huanle Liu, Leszek P. Pryszcz, Toni Hermoso Pulido, Anna Delgado-Tejedor, Julia Ponomarenko, Eva Maria Novoa.\r\n*Front. Genet., 17 March 2020.* https://doi.org/10.3389/fgene.2020.00211](https://www.frontiersin.org/articles/10.3389/fgene.2020.00211/full)\r\n\r\n\r\n## Documentation\r\nThe documentation is available at [https://biocorecrg.github.io/MOP2/docs/](https://biocorecrg.github.io/MOP2/docs/about.html)\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Metatranscriptomics",
            "Transcriptomics"
        ],
        "filtered_on": "edam",
        "id": "438",
        "keep": "Reject",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/438?version=1",
        "name": "Master of Pores 2",
        "number_of_steps": 0,
        "projects": [
            "Bioinformatics Unit @ CRG"
        ],
        "source": "WorkflowHub",
        "tags": [
            "ont",
            "transcriptomics",
            "drnaseq",
            "metatranscriptomics",
            "nanopore"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-02-20",
        "versions": 1
    },
    {
        "create_time": "2022-04-20",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst",
            "Germ\u00e1n Royval"
        ],
        "description": "#### - Deprecated -\r\n#### See our updated hybrid assembly workflow: https://workflowhub.eu/workflows/367\r\n#### And other workflows: https://workflowhub.eu/projects/16#workflows\r\n# \r\n**Workflow for sequencing with ONT Nanopore data, from basecalled reads to (meta)assembly and binning**\r\n- Workflow Nanopore Quality\r\n- Kraken2 taxonomic classification of FASTQ reads\r\n- Flye (de-novo assembly)\r\n- Medaka (assembly polishing)\r\n- metaQUAST (assembly quality reports)\r\n\r\n**When Illumina reads are provided:** \r\n  - Workflow Illumina Quality: https://workflowhub.eu/workflows/336?version=1\t\r\n  - Assembly polishing with Pilon<br>\r\n  - Workflow binnning https://workflowhub.eu/workflows/64?version=11\r\n      - Metabat2\r\n      - CheckM\r\n      - BUSCO\r\n      - GTDB-Tk\r\n\r\n**All tool CWL files and other workflows can be found here:**<br>\r\n  Tools: https://git.wur.nl/unlock/cwl/-/tree/master/cwl<br>\r\n  Workflows: https://git.wur.nl/unlock/cwl/-/tree/master/cwl/workflows<br>",
        "doi": null,
        "edam_operation": [
            "Sequence assembly",
            "Sequencing quality control"
        ],
        "edam_topic": [
            "Bioinformatics",
            "Metagenomic sequencing",
            "Metagenomics",
            "Sequence assembly",
            "Sequencing"
        ],
        "filtered_on": "edam",
        "id": "254",
        "keep": "To Curate",
        "latest_version": 3,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/254?version=3",
        "name": "Nanopore Assembly Workflow - Deprecated -",
        "number_of_steps": 22,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "genomics",
            "metagenomics",
            "nanopore"
        ],
        "tools": [
            "Quality and filtering workflow for illumina reads",
            "evaluation of polished assembly with metaQUAST",
            "Preparation of Flye output files to a specific output folder",
            "Illumina reads assembly polishing with Pilon",
            "Binning workflow to create bins",
            "Preparation of pilon output files to a specific output folder",
            "Preparation of Medaka output files to a specific output folder",
            "Sam file conversion to a sorted bam file",
            "Illumina read mapping on pilon assembly for binning",
            "Preparation of Kraken2 output files to a specific output folder",
            "Medaka for polishing of assembled genome",
            "Preparation of metaQUAST output files to a specific output folder",
            "Taxonomic classification of Nanopore reads",
            "Preparation of quast output files to a specific output folder",
            "Preparation of QUAST output files to a specific output folder",
            "De novo assembly of single-molecule reads with Flye",
            "Taxonomic classification of FASTQ reads",
            "Illumina evaluation of pilon polished assembly with metaQUAST",
            "Compress large kraken2 report file",
            "Visualization of kraken2 with Krona",
            "Quality and filtering workflow for nanopore reads"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-02-02",
        "versions": 3
    },
    {
        "create_time": "2023-01-17",
        "creators": [],
        "description": "# TronFlow alignment pipeline\r\n\r\n![GitHub tag (latest SemVer)](https://img.shields.io/github/v/release/tron-bioinformatics/tronflow-bwa?sort=semver)\r\n[![Run tests](https://github.com/TRON-Bioinformatics/tronflow-bwa/actions/workflows/automated_tests.yml/badge.svg?branch=master)](https://github.com/TRON-Bioinformatics/tronflow-bwa/actions/workflows/automated_tests.yml)\r\n[![DOI](https://zenodo.org/badge/327943420.svg)](https://zenodo.org/badge/latestdoi/327943420)\r\n[![License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)\r\n[![Powered by Nextflow](https://img.shields.io/badge/powered%20by-Nextflow-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://www.nextflow.io/)\r\n\r\nThe TronFlow alignment pipeline is part of a collection of computational workflows for tumor-normal pair \r\nsomatic variant calling.\r\n\r\nFind the documentation here [![Documentation Status](https://readthedocs.org/projects/tronflow-docs/badge/?version=latest)](https://tronflow-docs.readthedocs.io/en/latest/?badge=latest)\r\n\r\nThis pipeline aligns paired and single end FASTQ files with BWA aln and mem algorithms and with BWA mem 2.\r\nFor RNA-seq STAR is also supported. To increase sensitivity of novel junctions use `--star_two_pass_mode` (recommended for RNAseq variant calling).\r\nIt also includes an initial step of read trimming using FASTP.\r\n\r\n\r\n## How to run it\r\n\r\nRun it from GitHub as follows:\r\n```\r\nnextflow run tron-bioinformatics/tronflow-alignment -profile conda --input_files $input --output $output --algorithm aln --library paired\r\n```\r\n\r\nOtherwise download the project and run as follows:\r\n```\r\nnextflow main.nf -profile conda --input_files $input --output $output --algorithm aln --library paired\r\n```\r\n\r\nFind the help as follows:\r\n```\r\n$ nextflow run tron-bioinformatics/tronflow-alignment  --help\r\nN E X T F L O W  ~  version 19.07.0\r\nLaunching `main.nf` [intergalactic_shannon] - revision: e707c77d7b\r\n\r\nUsage:\r\n    nextflow main.nf --input_files input_files [--reference reference.fasta]\r\n\r\nInput:\r\n    * input_fastq1: the path to a FASTQ file (incompatible with --input_files)\r\n    * input_files: the path to a tab-separated values file containing in each row the sample name and two paired FASTQs (incompatible with --fastq1 and --fastq2)\r\n    when `--library paired`, or a single FASTQ file when `--library single`\r\n    Example input file:\r\n    name1\tfastq1.1\tfastq1.2\r\n    name2\tfastq2.1\tfastq2.2\r\n    * reference: path to the indexed FASTA genome reference or the star reference folder in case of using star\r\n\r\nOptional input:\r\n    * input_fastq2: the path to a second FASTQ file (incompatible with --input_files, incompatible with --library paired)\r\n    * output: the folder where to publish output (default: output)\r\n    * algorithm: determines the BWA algorithm, either `aln`, `mem`, `mem2` or `star` (default `aln`)\r\n    * library: determines whether the sequencing library is paired or single end, either `paired` or `single` (default `paired`)\r\n    * cpus: determines the number of CPUs for each job, with the exception of bwa sampe and samse steps which are not parallelized (default: 8)\r\n    * memory: determines the memory required by each job (default: 32g)\r\n    * inception: if enabled it uses an inception, only valid for BWA aln, it requires a fast file system such as flash (default: false)\r\n    * skip_trimming: skips the read trimming step\r\n    * star_two_pass_mode: activates STAR two-pass mode, increasing sensitivity of novel junction discovery, recommended for RNA variant calling (default: false)\r\n    * additional_args: additional alignment arguments, only effective in BWA mem, BWA mem 2 and STAR (default: none) \r\n\r\nOutput:\r\n    * A BAM file \\${name}.bam and its index\r\n    * FASTP read trimming stats report in HTML format \\${name.fastp_stats.html}\r\n    * FASTP read trimming stats report in JSON format \\${name.fastp_stats.json}\r\n```\r\n\r\n### Input tables\r\n\r\nThe table with FASTQ files expects two tab-separated columns without a header\r\n\r\n| Sample name          | FASTQ 1                      | FASTQ 2                  |\r\n|----------------------|---------------------------------|------------------------------|\r\n| sample_1             | /path/to/sample_1.1.fastq      |    /path/to/sample_1.2.fastq   |\r\n| sample_2             | /path/to/sample_2.1.fastq      |    /path/to/sample_2.2.fastq   |\r\n\r\n\r\n### Reference genome\r\n\r\nThe reference genome has to be provided in FASTA format and it requires two set of indexes:\r\n* FAI index. Create with `samtools faidx your.fasta`\r\n* BWA indexes. Create with `bwa index your.fasta`\r\n\r\nFor bwa-mem2 a specific index is needed:\r\n```\r\nbwa-mem2 index your.fasta\r\n```\r\n\r\nFor star a reference folder prepared with star has to be provided. In order to prepare it will need the reference\r\ngenome in FASTA format and the gene annotations in GTF format. Run a command as follows:\r\n```\r\nSTAR --runMode genomeGenerate --genomeDir $YOUR_FOLDER --genomeFastaFiles $YOUR_FASTA --sjdbGTFfile $YOUR_GTF\r\n```\r\n\r\n## References\r\n\r\n* Li H. and Durbin R. (2010) Fast and accurate long-read alignment with Burrows-Wheeler Transform. Bioinformatics, Epub. https://doi.org/10.1093/bioinformatics/btp698 \r\n* Shifu Chen, Yanqing Zhou, Yaru Chen, Jia Gu; fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics, Volume 34, Issue 17, 1 September 2018, Pages i884\u2013i890, https://doi.org/10.1093/bioinformatics/bty560\r\n* Vasimuddin Md, Sanchit Misra, Heng Li, Srinivas Aluru. Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems. IEEE Parallel and Distributed Processing Symposium (IPDPS), 2019.\r\n* Dobin A, Davis CA, Schlesinger F, Drenkow J, Zaleski C, Jha S, Batut P, Chaisson M, Gingeras TR. STAR: ultrafast universal RNA-seq aligner. Bioinformatics. 2013 Jan 1;29(1):15-21. doi: 10.1093/bioinformatics/bts635. Epub 2012 Oct 25. PMID: 23104886; PMCID: PMC3530905.\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "418",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/418?version=1",
        "name": "TronFlow alignment pipeline",
        "number_of_steps": 0,
        "projects": [
            "TRON gGmbH"
        ],
        "source": "WorkflowHub",
        "tags": [
            "alignment",
            "bwa",
            "bioinformatics",
            "star",
            "fastp"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-17",
        "versions": 1
    },
    {
        "create_time": "2023-01-17",
        "creators": [
            "Pablo Riesgo Ferreiro",
            "Thomas Bukur",
            "Patrick Sorn"
        ],
        "description": "![CoVigator logo](images/CoVigator_logo_txt_nobg.png \"CoVigator logo\")\r\n\r\n# CoVigator pipeline: variant detection pipeline for Sars-CoV-2\r\n\r\n[![DOI](https://zenodo.org/badge/374669617.svg)](https://zenodo.org/badge/latestdoi/374669617)\r\n[![Run tests](https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline/actions/workflows/automated_tests.yml/badge.svg?branch=master)](https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline/actions/workflows/automated_tests.yml)\r\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-Nextflow-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://www.nextflow.io/)\r\n[![License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)\r\n\r\n\r\n\r\nThe Covigator pipeline processes SARS-CoV-2 FASTQ or FASTA files into annotated and normalized analysis ready VCF files.\r\nIt also classifies samples into lineages using pangolin.\r\nThe pipeline is implemented in the Nextflow framework (Di Tommaso, 2017), it is a stand-alone pipeline that can be\r\nused independently of the CoVigator dashboard and knowledge base.\r\n\r\nAlthough it is configured by default for SARS-CoV-2 it can be employed for the analysis of other microbial organisms \r\nif the required references are provided.\r\n\r\nThe result of the pipeline is one or more annotated VCFs with the list of SNVs and indels ready for analysis.\r\n\r\nThe results from the CoVigator pipeline populate our CoVigator dashboard [https://covigator.tron-mainz.de](https://covigator.tron-mainz.de) \r\n\r\n**Table of Contents**\r\n\r\n1. [Two pipelines in one](#id1)\r\n2. [Implementation](#id2)\r\n3. [How to run](#id3)\r\n4. [Understanding the output](#id4)\r\n6. [Annotation resources](#id5)\r\n7. [Future work](#id6)\r\n8. [Bibliography](#id7)\r\n\r\n\r\n## Two pipelines in one\r\n\r\nIn CoVigator we analyse samples from two different formats, FASTQ files (e.g.: as provided by the European Nucleotide \r\nArchive) and FASTA files containing a consensus assembly. While from the first we get the raw reads, \r\nfrom the second we obtain already assembled genomes. Each of these formats has to be \r\nanalysed differently. Also, the output data that we can obtain from each of these is different.\r\n\r\n![CoVigator pipeline](images/pipeline.drawio.png)\r\n\r\n### Pipeline for FASTQ files\r\n\r\nWhen FASTQ files are provided the pipeline includes the following steps:\r\n- **Trimming**. `fastp` is used to trim reads with default values. This step also includes QC filtering.\r\n- **Alignment**. `BWA mem 2` is used for the alignment of single or paired end samples.\r\n- **BAM preprocessing**. BAM files are prepared and duplicate reads are marked using GATK and Sambamba tools.\r\n- **Primer trimming**. When a BED with primers is provided, these are trimmed from the reads using iVar. This is applicable to the results from all variant callers.\r\n- **Coverage analysis**. `samtools coverage` and `samtools depth` are used to compute the horizontal and vertical \r\n  coverage respectively.\r\n- **Variant calling**. Four different variant callers are employed: BCFtools, LoFreq, iVar and GATK. \r\n  Subsequent processing of resulting VCF files is independent for each caller.\r\n- **Variant normalization**. `bcftools norm` is employed to left align indels, trim variant calls and remove variant duplicates.\r\n- **Technical annotation**. `VAFator` is employed to add VAF and coverage annotations from the reads pileup.\r\n- **Phasing**. Clonal mutations (ie: VAF >= 0.8) occurring in the same amino acid are merged for its correct functional annotation.\r\n- **Biological annotation**. `SnpEff` is employed to annotate the variant consequences of variants and\r\n  `bcftools annotate` is employed to add additional SARS-CoV-2 annotations.\r\n- **Lineage determination**. `pangolin` is used for this purpose, this runs over the results from each of the variant callers separately.\r\n\r\nBoth single end and paired end FASTQ files are supported.\r\n\r\n### Pipeline for FASTA files\r\n\r\nWhen a FASTA file is provided with a single assembly sequence the pipeline includes the following steps:\r\n- **Variant calling**. A Smith-Waterman global alignment is performed against the reference sequence to call SNVs and \r\n  indels. Indels longer than 50 bp and at the beginning or end of the assembly sequence are excluded. Any mutation where\r\n  either reference or assembly contain an N is excluded.\r\n- **Variant normalization**. Same as described above.\r\n- **Phasing**. mutations occurring in the same amino acid are merged for its correct annotation.\r\n- **Biological annotation**. Same as described above.\r\n- **Lineage determination**. `pangolin` is used for this purpose.\r\n\r\nThe FASTA file is expected to contain a single assembly sequence. \r\nBear in mind that only clonal variants can be called on the assembly.\r\n\r\n### Pipeline for VCF files\r\n\r\nWhen a VCF file is provided the pipeline includes the following steps:\r\n- **Variant normalization**. Same as described above.\r\n- **Technical annotation**. Same as described above (optional if BAM is provided)\r\n- **Phasing**. mutations occurring in the same amino acid are merged for its correct annotation.\r\n- **Biological annotation**. Same as described above\r\n- **Lineage determination**. `pangolin` is used for this purpose.\r\n\r\n## Implementation\r\n\r\nThe pipeline is implemented as a Nextflow workflow with its DSL2 syntax.\r\nThe dependencies are managed through a conda environment to ensure version traceability and reproducibility.\r\nThe references for SARS-CoV-2 are embedded in the pipeline.\r\nThe pipeline is based on a number of third-party tools, plus a custom implementation based on biopython (Cock, 2009) \r\nfor the alignment and subsequent variant calling over a FASTA file.\r\n\r\nAll code is open sourced in GitHub [https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline](https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline)\r\nand made available under the MIT license. We welcome any contribution. \r\nIf you have troubles using the CoVigator pipeline or you find an issue, we will be thankful if you would report a ticket \r\nin GitHub.\r\n\r\nThe alignment, BAM preprocessing and variant normalization pipelines are based on the implementations in additional \r\nNextflow pipelines within the TronFlow initiative [https://tronflow-docs.readthedocs.io/](https://tronflow-docs.readthedocs.io/). \r\n\r\n\r\n### Variant annotations\r\n\r\nThe variants derived from a FASTQ file are annotated on the `FILTER` column using the VAFator \r\n(https://github.com/TRON-Bioinformatics/vafator) variant allele frequency \r\n(VAF) into `LOW_FREQUENCY`, `SUBCLONAL`, `LOW_QUALITY_CLONAL` and finally `PASS` variants correspond to clonal variants. \r\nBy default, variants with a VAF < 2 % are considered `LOW_FREQUENCY`, variants with a VAF >= 2 % and < 50 % are \r\nconsidered `SUBCLONAL` and variants with a VAF >= 50 % and < 80 % are considered `LOW_QUALITY_CLONAL`. \r\nThis thresholds can be changed with the parameters `--low_frequency_variant_threshold`,\r\n`--subclonal_variant_threshold` and `--low_quality_clonal_variant_threshold` respectively.\r\n\r\nVAFator technical annotations:\r\n\r\n- `INFO/vafator_af`: variant allele frequency of the mutation \r\n- `INFO/vafator_ac`: number of reads supporting the mutation \r\n- `INFO/vafator_dp`: total number of reads at the position, in the case of indels this represents the number of reads in the previous position\r\n\r\nSnpEff provides the functional annotations. And all mutations are additionally annotated with the following SARS-CoV-2 specific annotations:\r\n- ConsHMM conservation scores as reported in (Kwon, 2021)\r\n- Pfam domains as reported in Ensemble annotations.\r\n\r\nBiological annotations: \r\n\r\n- `INFO/ANN` are the SnpEff consequence annotations (eg: overlapping gene, effect of the mutation). \r\nThis are described in detail here [http://pcingola.github.io/SnpEff/se_inputoutput/](http://pcingola.github.io/SnpEff/se_inputoutput/) \r\n- `INFO/CONS_HMM_SARS_COV_2` is the ConsHMM conservation score in SARS-CoV-2\r\n- `INFO/CONS_HMM_SARBECOVIRUS` is the ConsHMM conservation score among Sarbecovirus\r\n- `INFO/CONS_HMM_VERTEBRATE_COV` is the ConsHMM conservation score among vertebrate Corona virus\r\n- `INFO/PFAM_NAME` is the Interpro name for the overlapping Pfam domains\r\n- `INFO/PFAM_DESCRIPTION` is the Interpro description for the overlapping Pfam domains\r\n- `INFO/problematic` contains the filter provided in DeMaio et al. (2020) for problematic mutations\r\n\r\nAccording to DeMaio et al. (2020), mutations at the beginning (ie: POS <= 50) and end (ie: POS >= 29,804) of the \r\ngenome are filtered out\r\n\r\nThis is an example of biological annotations of a missense mutation in the spike protein on the N-terminal subunit 1 domain.\r\n```\r\nANN=A|missense_variant|MODERATE|S|gene-GU280_gp02|transcript|TRANSCRIPT_gene-GU280_gp02|protein_coding|1/1|c.118G>A|\r\np.D40N|118/3822|118/3822|40/1273||;CONS_HMM_SARS_COV_2=0.57215;CONS_HMM_SARBECOVIRUS=0.57215;CONS_HMM_VERTEBRATE_COV=0;\r\nPFAM_NAME=bCoV_S1_N;PFAM_DESCRIPTION=Betacoronavirus-like spike glycoprotein S1, N-terminal\r\n```\r\n\r\n\r\n### Phasing limitations\r\n\r\nThe phasing implementation is applicable only to clonal mutations. It assumes all clonal mutations are in phase and \r\nhence it merges those occurring in the same amino acid.\r\nIn order to phase intrahost mutations we would need to implement a read-backed phasing approach such as in WhatsHap \r\nor GATK's ReadBackedPhasing. Unfortunately these tools do not support the scenario of a haploid organism with an\r\nundefined number of subclones.\r\nFor this reason, phasing is implemented with custom Python code at `bin/phasing.py`.\r\n\r\n### Primers trimming\r\n\r\nWith some library preparation protocols such as ARTIC it is recommended to trim the primers from the reads.\r\nWe have observed that if primers are not trimmed spurious mutations are being called specially SNVs with lower frequencies and long deletions.\r\nAlso the variant allele frequencies of clonal mutations are underestimated.\r\n\r\nThe BED files containing the primers for each ARTIC version can be found at https://github.com/artic-network/artic-ncov2019/tree/master/primer_schemes/nCoV-2019.\r\n\r\nIf the adequate BED file is provided to the CoVigator pipeline with `--primers` the primers will be trimmed with iVar. \r\nThis affects the output of every variant caller, not only iVar.\r\n\r\n### Reference data\r\n\r\nThe default SARS-CoV-2 reference files correspond to Sars_cov_2.ASM985889v3 and were downloaded from Ensembl servers.\r\nNo additional parameter needs to be provided to use the default SARS-CoV-2 reference genome.\r\n\r\n#### Using a custom reference genome\r\n\r\nThese references can be customised to use a different SARS-CoV-2 reference or to analyse a different virus.\r\nTwo files need to be provided:\r\n- Use a custom reference genome by providing the parameter `--reference your.fasta`.\r\n- Gene annotation file in GFFv3 format `--gff your.gff`. This is only required to run iVar\r\n\r\nAdditionally, the FASTA needs bwa indexes, .fai index and a .dict index.\r\nThese indexes can be generated with the following two commands:\r\n```\r\nbwa index reference.fasta\r\nsamtools faidx reference.fasta\r\ngatk CreateSequenceDictionary --REFERENCE your.fasta\r\n```\r\n\r\n**NOTE**: beware that for Nextflow to find these indices the reference needs to be passed as an absolute path.\r\n\r\nThe SARS-CoV-2 specific annotations will be skipped when using a custom genome.\r\n\r\nIn order to have SnpEff functional annotations available you will also need to provide three parameters:\r\n- `--snpeff_organism`: organism to annotate with SnpEff (ie: as registered in SnpEff)\r\n- `--snpeff_data`: path to the SnpEff data folder\r\n- `--snpeff_config`: path to the SnpEff config file\r\n\r\n### Intrahost mutations\r\n\r\nSome mutations may be observed in a subset of the virus sample, this may arise through intrahost virus evolution or\r\nco-infection. Intrahost mutations can only be detected when analysing the raw reads (ie: the FASTQs) \r\nas in the assembly (ie: the FASTA file) a single virus consensus sequence is represented. \r\nBCFtools and GATK do not normally capture intrahost mutations; on the other hand LoFreq and iVar both capture\r\nmutations that deviate from a clonal-like VAF. \r\nNevertheless, mutations with lower variant allele frequency (VAF) are challenging to distinguish from sequencing and\r\nanalytical errors.  \r\n\r\nMutations are annotated on the `FILTER` column using the VAF into three categories: \r\n- `LOW_FREQUENCY`: subset of intrahost mutations with lowest frequencies, potentially enriched with false positive calls (VAF < 2 %).\r\n- `SUBCLONAL`: subset of intrahost mutations with higher frequencies (2 % <= VAF < 50 %).\r\n- `LOW_QUALITY_CLONAL`: subset of clonal mutations with lower frequencies (50 % <= VAF < 80 %).\r\n- `PASS` clonal mutations (VAF >= 80 %)\r\n\r\nOther low quality mutations are removed from the output.\r\n\r\nThe VAF thresholds can be changed with the parameters `--low_frequency_variant_threshold`,\r\n`--subclonal_variant_threshold` and `--low_quality_clonal_variant_threshold`.\r\n\r\n## How to run\r\n\r\n### Requirements\r\n\r\n- Nextflow >= 19.10.0\r\n- Java >= 8\r\n- Conda >=4.9\r\n\r\n### Testing\r\n\r\nTo run the workflow on a test assembly dataset run:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda,test_fasta\r\n```\r\n\r\nFind the output in the folder `covigator_test_fasta`.\r\n\r\nTo run the workflow on a test raw reads dataset run:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda,test_fastq\r\n```\r\n\r\nFind the output in the folder `covigator_test_fastq`.\r\n\r\nThe above commands are useful to create the conda environments beforehand.\r\n\r\n**NOTE**: pangolin is the most time-consuming step of the whole pipeline. To make it faster, locate the conda \r\nenvironment that Nextflow created with pangolin (eg: `find $YOUR_NEXTFOW_CONDA_ENVS_FOLDER -name pangolin`) and run\r\n`pangolin --decompress-model`.\r\n\r\n### Running\r\n\r\nFor paired end reads:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\r\n[-r v0.10.0] \\\r\n[-profile conda] \\\r\n--fastq1 <FASTQ_FILE> \\\r\n--fastq2 <FASTQ_FILE> \\\r\n--name example_run \\\r\n--output <OUTPUT_FOLDER> \\\r\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\n\r\nFor single end reads:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\r\n[-r v0.10.0] \\\r\n[-profile conda] \\\r\n--fastq1 <FASTQ_FILE> \\\r\n--name example_run \\\r\n--output <OUTPUT_FOLDER> \\\r\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\n\r\nFor assembly:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\r\n[-r v0.10.0] \\\r\n[-profile conda] \\\r\n--fasta <FASTA_FILE> \\\r\n--name example_run \\\r\n--output <OUTPUT_FOLDER> \\\r\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\n\r\nFor VCF:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\r\n[-r v0.10.0] \\\r\n[-profile conda] \\\r\n--vcf <VCF_FILE> \\\r\n--name example_run \\\r\n--output <OUTPUT_FOLDER> \\\r\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\n\r\nAs an optional input when processing directly VCF files you can provide BAM files to annotate VAFs:\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\r\n[-r v0.10.0] \\\r\n[-profile conda] \\\r\n--vcf <VCF_FILE> \\\r\n--bam <BAM_FILE> \\\r\n--bai <BAI_FILE> \\\r\n--name example_run \\\r\n--output <OUTPUT_FOLDER> \\\r\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\n\r\nFor batch processing of reads use `--input_fastqs_list` and `--name`.\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] --input_fastqs_list <TSV_FILE> --library <paired|single> --output <OUTPUT_FOLDER> [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\nwhere the TSV file contains two or three columns tab-separated columns **without header**. Columns: sample name, path to FASTQ 1 and optionally path to FASTQ 2. \r\n\r\n| Sample    | FASTQ 1                       | FASTQ 2 (optional column)     |\r\n|-----------|-------------------------------|-------------------------------|\r\n| sample1   | /path/to/sample1_fastq1.fastq | /path/to/sample1_fastq2.fastq |\r\n| sample2   | /path/to/sample2_fastq1.fastq | /path/to/sample2_fastq2.fastq |\r\n| ...       | ...                           | ...                           |\r\n\r\n\r\nFor batch processing of assemblies use `--input_fastas_list`.\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] --input_fastas_list <TSV_FILE> --library <paired|single> --output <OUTPUT_FOLDER> [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\nwhere the TSV file contains two columns tab-separated columns **without header**. Columns: sample name and path to FASTA.\r\n\r\n| Sample    | FASTA                  | \r\n|-----------|------------------------|\r\n| sample1   | /path/to/sample1.fasta |\r\n| sample2   | /path/to/sample2.fasta |\r\n| ...       | ...                    |\r\n\r\nFor batch processing of VCFs use `--input_vcfs_list`.\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] --input_vcfs_list <TSV_FILE> --output <OUTPUT_FOLDER> [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\nwhere the TSV file contains two columns tab-separated columns **without header**. Columns: sample name and path to VCF.\r\n\r\n| Sample    | FASTA                  |\r\n|-----------|------------------------|\r\n| sample1   | /path/to/sample1.vcf |\r\n| sample2   | /path/to/sample2.vcf |\r\n| ...       | ...                    |\r\n\r\nOptionally, provide BAM files for batch processing of VCFs using `--input_bams_list`.\r\n```\r\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] \\\r\n  --input_vcfs_list <TSV_FILE> \\\r\n  --input_bams_list <TSV_FILE> \\\r\n  --output <OUTPUT_FOLDER> \\\r\n  [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\r\n  [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\r\n```\r\nwhere the BAMs TSV file contains three columns tab-separated columns **without header**. Columns: sample name, \r\npath to BAM and path to BAI.\r\n\r\n| Sample    | BAM                  | BAI                  |\r\n|-----------|----------------------|----------------------|\r\n| sample1   | /path/to/sample1.bam | /path/to/sample1.bai |\r\n| sample2   | /path/to/sample2.bam | /path/to/sample2.bai |\r\n| ...       | ...                  | ...                  |\r\n\r\n\r\n\r\n### Getting help\r\n\r\nYou can always contact us directly or create a GitHub issue, otherwise see all available options using `--help`:\r\n```\r\n$ nextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda --help\r\n\r\nUsage:\r\n    nextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda --help\r\n\r\nInput:\r\n    * --fastq1: the first input FASTQ file (not compatible with --fasta, nor --vcf)\r\n    * --fasta: the FASTA file containing the assembly sequence (not compatible with --fastq1, nor --vcf)\r\n    * --vcf: the VCF file containing mutations to analyze (not compatible with --fastq1, nor --fasta)\r\n    * --bam: the BAM file containing reads to annotate VAFs on a VCF (not compatible with --fastq1, nor --fasta)\r\n    * --bai: the BAI index for a BAM file (not compatible with --fastq1, nor --fasta)\r\n    * --name: the sample name, output files will be named after this name\r\n    * --output: the folder where to publish output\r\n    * --input_fastqs_list: alternative to --name and --fastq1 for batch processing\r\n    * --library: required only when using --input_fastqs\r\n    * --input_fastas_list: alternative to --name and --fasta for batch processing\r\n    * --input_vcfs_list: alternative to --name and --vcf for batch processing\r\n    * --input_bams_list: alternative to --name, --vcf, --bam and --bai for batch processing\r\n\r\nOptional input only required to use a custom reference:\r\n    * --reference: the reference genome FASTA file, *.fai, *.dict and bwa indexes are required.\r\n    * --gff: the GFFv3 gene annotations file (required to run iVar and to phase mutations from all variant callers)    \r\n    * --snpeff_data: path to the SnpEff data folder, it will be useful to use the pipeline on other virus than SARS-CoV-2\r\n    * --snpeff_config: path to the SnpEff config file, it will be useful to use the pipeline on other virus than SARS-CoV-2\r\n    * --snpeff_organism: organism to annotate with SnpEff, it will be useful to use the pipeline on other virus than SARS-CoV-2\r\n\r\nOptional input:\r\n    * --fastq2: the second input FASTQ file\r\n    * --primers: a BED file containing the primers used during library preparation. If provided primers are trimmed from the reads.\r\n    * --min_base_quality: minimum base call quality to take a base into account for variant calling (default: 20)\r\n    * --min_mapping_quality: minimum mapping quality to take a read into account for variant calling (default: 20)\r\n    * --vafator_min_base_quality: minimum base call quality to take a base into account for VAF annotation (default: 0)\r\n    * --vafator_min_mapping_quality: minimum mapping quality to take a read into account for VAF annotation (default: 0)\r\n    * --low_frequency_variant_threshold: VAF threshold to mark a variant as low frequency (default: 0.02)\r\n    * --subclonal_variant_threshold: VAF superior threshold to mark a variant as subclonal  (default: 0.5)\r\n    * --lq_clonal_variant_threshold: VAF superior threshold to mark a variant as loq quality clonal (default: 0.8)\r\n    * --memory: the ammount of memory used by each job (default: 3g)\r\n    * --cpus: the number of CPUs used by each job (default: 1)\r\n    * --skip_lofreq: skips calling variants with LoFreq\r\n    * --skip_gatk: skips calling variants with GATK\r\n    * --skip_bcftools: skips calling variants with BCFTools\r\n    * --skip_ivar: skips calling variants with iVar\r\n    * --skip_pangolin: skips lineage determination with pangolin\r\n    * --match_score: global alignment match score, only applicable for assemblies (default: 2)\r\n    * --mismatch_score: global alignment mismatch score, only applicable for assemblies (default: -1)\r\n    * --open_gap_score: global alignment open gap score, only applicable for assemblies (default: -3)\r\n    * --extend_gap_score: global alignment extend gap score, only applicable for assemblies (default: -0.1)\r\n    * --skip_sarscov2_annotations: skip some of the SARS-CoV-2 specific annotations (default: false)\r\n    * --keep_intermediate: keep intermediate files (ie: BAM files and intermediate VCF files)\r\n    * --args_bcftools_mpileup: additional arguments for bcftools mpileup command (eg: --args_bcftools_mpileup='--ignore-overlaps')\r\n    * --args_bcftools_call: additional arguments for bcftools call command (eg: --args_bcftools_call='--something')\r\n    * --args_lofreq: additional arguments for lofreq command (eg: --args_lofreq='--something')\r\n    * --args_gatk: additional arguments for gatk command (eg: --args_gatk='--something')\r\n    * --args_ivar_samtools: additional arguments for ivar samtools mpileup command (eg: --args_ivar_samtools='--ignore-overlaps')\r\n    * --args_ivar: additional arguments for ivar command (eg: --args_ivar='--something')\r\n\r\nOutput:\r\n    * Output a VCF file for each of BCFtools, GATK, LoFreq and iVar when FASTQ files are\r\n    provided or a single VCF obtained from a global alignment when a FASTA file is provided.\r\n    * A pangolin results file for each of the VCF files.\r\n    * Only when FASTQs are provided:\r\n      * FASTP statistics\r\n      * Depth and breadth of coverage analysis results\r\n      \r\n```\r\n\r\n## Understanding the output\r\n\r\nAlthough the VCFs are normalized for both pipelines, the FASTQ pipeline runs four variant callers, while the FASTA\r\npipeline runs a single variant caller. Also, there are several metrics in the FASTQ pipeline that are not present\r\nin the output of the FASTA pipeline. Here we will describe these outputs.\r\n\r\n### FASTQ pipeline output\r\n\r\nFind in the table below a description of each of the expected files and a link to a sample file for the FASTQ pipeline.\r\nThe VCF files will be described in more detail later.\r\n\r\n| Name                            | Description                                                    | Sample file                                                                                                                                       |\r\n|---------------------------------|----------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\r\n| $NAME.fastp_stats.json          | Output metrics of the fastp trimming process in JSON format    | [ERR4145453.fastp_stats.json](_static/covigator_pipeline_sample_output_reads/ERR4145453.fastp_stats.json)                                         |\r\n| $NAME.fastp_stats.html          | Output metrics of the fastp trimming process in HTML format    | [ERR4145453.fastp_stats.html](_static/covigator_pipeline_sample_output_reads/ERR4145453.fastp_stats.html)                                         |\r\n| $NAME.deduplication_metrics.txt | Deduplication metrics                                          | [ERR4145453.deduplication_metrics.txt](_static/covigator_pipeline_sample_output_reads/ERR4145453.deduplication_metrics.txt)                       |\r\n| $NAME.coverage.tsv              | Coverage metrics (eg: mean depth, % horizontal coverage)       | [ERR4145453.coverage.tsv](_static/covigator_pipeline_sample_output_reads/ERR4145453.coverage.tsv)                                                 |\r\n| $NAME.depth.tsv                 | Depth of coverage per position                                 | [ERR4145453.depth.tsv](_static/covigator_pipeline_sample_output_reads/ERR4145453.depth.tsv)                                                       |\r\n| $NAME.bcftools.vcf.gz           | Bgzipped, tabix-indexed and annotated output VCF from BCFtools | [ERR4145453.bcftools.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_reads/ERR4145453.bcftools.normalized.annotated.vcf.gz) |\r\n| $NAME.gatk.vcf.gz               | Bgzipped, tabix-indexed and annotated output VCF from GATK     | [ERR4145453.gatk.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_reads/ERR4145453.gatk.normalized.annotated.vcf.gz)         |\r\n| $NAME.lofreq.vcf.gz             | Bgzipped, tabix-indexed and annotated output VCF from LoFreq   | [ERR4145453.lofreq.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_reads/ERR4145453.lofreq.normalized.annotated.vcf.gz)     |\r\n| $NAME.ivar.vcf.gz               | Bgzipped, tabix-indexed and annotated output VCF from LoFreq   | [ERR4145453.ivar.tsv](_static/covigator_pipeline_sample_output_reads/ERR4145453.ivar.tsv)                                                         |\r\n| $NAME.lofreq.pangolin.csv       | Pangolin CSV output file derived from LoFreq mutations         | [ERR4145453.lofreq.pangolin.csv](_static/covigator_pipeline_sample_output_reads/ERR4145453.lofreq.pangolin.csv)                                              |\r\n\r\n\r\n### FASTA pipeline output\r\n\r\nThe FASTA pipeline returns a single VCF file. The VCF files will be described in more detail later.\r\n\r\n| Name                        | Description                                                  | Sample file                                                                                          |\r\n|-----------------------------|--------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\r\n| $NAME.assembly.vcf.gz | Bgzipped, tabix-indexed and annotated output VCF | [ERR4145453.assembly.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_assembly/hCoV-19_NTXX.assembly.normalized.annotated.vcf.gz) |\r\n\r\n\r\n## Annotations resources\r\n\r\nSARS-CoV-2 ASM985889v3 references were downloaded from Ensembl on 6th of October 2020:\r\n- ftp://ftp.ensemblgenomes.org/pub/viruses/fasta/sars_cov_2/dna/Sars_cov_2.ASM985889v3.dna.toplevel.fa.gz\r\n- ftp://ftp.ensemblgenomes.org/pub/viruses/gff3/sars_cov_2/Sars_cov_2.ASM985889v3.101.gff3.gz\r\n\r\nConsHMM mutation depletion scores downloaded on 1st of July 2021:\r\n- https://github.com/ernstlab/ConsHMM_CoV/blob/master/wuhCor1.mutDepletionConsHMM.bed\r\n- https://github.com/ernstlab/ConsHMM_CoV/blob/master/wuhCor1.mutDepletionSarbecovirusConsHMM.bed\r\n- https://github.com/ernstlab/ConsHMM_CoV/blob/master/wuhCor1.mutDepletionVertebrateCoVConsHMM.bed\r\n\r\nGene annotations including Pfam domains downloaded from Ensembl on 25th of February 2021 from:\r\n- ftp://ftp.ensemblgenomes.org/pub/viruses/json/sars_cov_2/sars_cov_2.json\r\n\r\n\r\n## Future work\r\n\r\n- Primer trimming on an arbitrary sequencing library.\r\n- Pipeline for Oxford Nanopore technology.\r\n- Variant calls from assemblies contain an abnormally high number of deletions of size greater than 3 bp. This\r\nis a technical artifact that would need to be avoided.\r\n\r\n## Bibliography\r\n\r\n- Di Tommaso, P., Chatzou, M., Floden, E. W., Barja, P. P., Palumbo, E., & Notredame, C. (2017). Nextflow enables reproducible computational workflows. Nature Biotechnology, 35(4), 316\u2013319. https://doi.org/10.1038/nbt.3820\r\n- Vasimuddin Md, Sanchit Misra, Heng Li, Srinivas Aluru. Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems. IEEE Parallel and Distributed Processing Symposium (IPDPS), 2019.\r\n- Adrian Tan, Gon\u00e7alo R. Abecasis and Hyun Min Kang. Unified Representation of Genetic Variants. Bioinformatics (2015) 31(13): 2202-2204](http://bioinformatics.oxfordjournals.org/content/31/13/2202) and uses bcftools [Li, H. (2011). A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data. Bioinformatics (Oxford, England), 27(21), 2987\u20132993. 10.1093/bioinformatics/btr509\r\n- Danecek P, Bonfield JK, Liddle J, Marshall J, Ohan V, Pollard MO, Whitwham A, Keane T, McCarthy SA, Davies RM, Li H. Twelve years of SAMtools and BCFtools. Gigascience. 2021 Feb 16;10(2):giab008. doi: 10.1093/gigascience/giab008. PMID: 33590861; PMCID: PMC7931819.\r\n- Van der Auwera GA, Carneiro M, Hartl C, Poplin R, del Angel G, Levy-Moonshine A, Jordan T, Shakir K, Roazen D, Thibault J, Banks E, Garimella K, Altshuler D, Gabriel S, DePristo M. (2013). From FastQ Data to High-Confidence Variant Calls: The Genome Analysis Toolkit Best Practices Pipeline. Curr Protoc Bioinformatics, 43:11.10.1-11.10.33. DOI: 10.1002/0471250953.bi1110s43.\r\n- Martin, M., Patterson, M., Garg, S., O Fischer, S., Pisanti, N., Klau, G., Sch\u00f6enhuth, A., & Marschall, T. (2016). WhatsHap: fast and accurate read-based phasing. BioRxiv, 085050. https://doi.org/10.1101/085050\r\n- Danecek, P., & McCarthy, S. A. (2017). BCFtools/csq: haplotype-aware variant consequences. Bioinformatics, 33(13), 2037\u20132039. https://doi.org/10.1093/bioinformatics/btx100\r\n- Wilm, A., Aw, P. P. K., Bertrand, D., Yeo, G. H. T., Ong, S. H., Wong, C. H., Khor, C. C., Petric, R., Hibberd, M. L., & Nagarajan, N. (2012). LoFreq: A sequence-quality aware, ultra-sensitive variant caller for uncovering cell-population heterogeneity from high-throughput sequencing datasets. Nucleic Acids Research, 40(22), 11189\u201311201. https://doi.org/10.1093/nar/gks918\r\n- Grubaugh, N. D., Gangavarapu, K., Quick, J., Matteson, N. L., De Jesus, J. G., Main, B. J., Tan, A. L., Paul, L. M., Brackney, D. E., Grewal, S., Gurfield, N., Van Rompay, K. K. A., Isern, S., Michael, S. F., Coffey, L. L., Loman, N. J., & Andersen, K. G. (2019). An amplicon-based sequencing framework for accurately measuring intrahost virus diversity using PrimalSeq and iVar. Genome Biology, 20(1), 8. https://doi.org/10.1186/s13059-018-1618-7\r\n- Shifu Chen, Yanqing Zhou, Yaru Chen, Jia Gu; fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics, Volume 34, Issue 17, 1 September 2018, Pages i884\u2013i890, https://doi.org/10.1093/bioinformatics/bty560\r\n- Kwon, S. Bin, & Ernst, J. (2021). Single-nucleotide conservation state annotation of the SARS-CoV-2 genome. Communications Biology, 4(1), 1\u201311. https://doi.org/10.1038/s42003-021-02231-w\r\n- Cock, P. J., Antao, T., Chang, J. T., Chapman, B. A., Cox, C. J., Dalke, A., et al. (2009). Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics, 25(11), 1422\u20131423.\r\n- Artem Tarasov, Albert J. Vilella, Edwin Cuppen, Isaac J. Nijman, Pjotr Prins, Sambamba: fast processing of NGS alignment formats, Bioinformatics, Volume 31, Issue 12, 15 June 2015, Pages 2032\u20132034, https://doi.org/10.1093/bioinformatics/btv098\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in description",
        "id": "417",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/417?version=1",
        "name": "CoVigator pipeline: variant detection pipeline for Sars-CoV-2 (and other viruses...)",
        "number_of_steps": 0,
        "projects": [
            "TRON gGmbH"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bioinformatics",
            "nextflow",
            "sars-cov-2",
            "covid-19",
            "variant calling"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-17",
        "versions": 1
    },
    {
        "create_time": "2022-11-24",
        "creators": [
            "Saskia Hiltemann",
            "Willem de Koning"
        ],
        "description": "Workflow for the GTN training \"Antibiotic resistance detection\"",
        "doi": null,
        "edam_operation": [
            "Antimicrobial resistance prediction"
        ],
        "edam_topic": [
            "Microbiology"
        ],
        "filtered_on": "metage.* in tags",
        "id": "406",
        "keep": "Keep",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/406?version=1",
        "name": "GTN Training - Antibiotic Resistance Detection",
        "number_of_steps": 12,
        "projects": [
            "Galaxy Training Network"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metagenomics"
        ],
        "tools": [
            "nanoplot",
            "unicycler",
            "racon",
            "gfa_to_fa",
            "minimap2",
            "miniasm",
            "bandage_image",
            "PlasFlow",
            "staramr_search"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2022-01-27",
        "creators": [],
        "description": "## Introduction\r\n\r\n**vibbits/rnaseq-editing** is a bioinformatics pipeline that can be used to analyse RNA sequencing data obtained from organisms with a reference genome and annotation followed by a prediction step of editing sites using RDDpred.\r\n\r\nThe pipeline is largely based on the [nf-core RNAseq pipeline](https://nf-co.re/rnaseq/).\r\n\r\nThe initial nf-core pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\r\n\r\n## Pipeline summary\r\n\r\n1. Merge re-sequenced FastQ files ([`cat`](http://www.linfo.org/cat.html))\r\n2. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\r\n3. Adapter and quality trimming ([`Trimmomatics`](https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/))\r\n4. Use of STAR for multiple alignment and quantification: [`STAR`](https://github.com/alexdobin/STAR)\r\n5. Sort and index alignments ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\r\n6. Prediction of editing sites using RDDpred ([`RDDpred`](https://github.com/vibbits/RDDpred))\r\n7. Extensive quality control:\r\n    1. [`RSeQC`](http://rseqc.sourceforge.net/)\r\n    2. [`Qualimap`](http://qualimap.bioinfo.cipf.es/)\r\n    3. [`dupRadar`](https://bioconductor.org/packages/release/bioc/html/dupRadar.html)\r\n8. Present QC for raw read, alignment, gene biotype, sample similarity, and strand-specificity checks ([`MultiQC`](http://multiqc.info/), [`R`](https://www.r-project.org/))\r\n\r\n## Quick Start\r\n\r\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=21.04.0`)\r\n\r\n2. Install [`Docker`](https://docs.docker.com/engine/installation/) on a Linux operating system.\r\n   Note: This pipeline does not currently support running with macOS.\r\n\r\n3. Download the pipeline via git clone, download the associated training data files for RDDpred into the assets folder, download the reference data to \r\n\r\n    ```console\r\n    git clone https://github.com/vibbits/rnaseq-editing.git\r\n    cd $(pwd)/rnaseq-editing/assets\r\n    # download training data file for RDDpred\r\n    wget -c \r\n    # download reference data for your genome, we provide genome and indexed genome for STAR 2.7.3a\r\n    \r\n    ```\r\n\r\n    > * Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\r\n\r\n4. Start running your own analysis using Docker locally!\r\n\r\n    ```console\r\n    nextflow run vibbits/rnaseq-editing \\\r\n        --input samplesheet.csv \\\r\n        --genome hg19 \\\r\n        -profile docker\r\n    ```\r\n\r\n    * An executable Python script called [`fastq_dir_to_samplesheet.py`](https://github.com/nf-core/rnaseq/blob/master/bin/fastq_dir_to_samplesheet.py) has been provided if you would like to auto-create an input samplesheet based on a directory containing FastQ files **before** you run the pipeline (requires Python 3 installed locally) e.g.\r\n\r\n        ```console\r\n        wget -L https://raw.githubusercontent.com/nf-core/rnaseq/master/bin/fastq_dir_to_samplesheet.py\r\n        ./fastq_dir_to_samplesheet.py <FASTQ_DIR> samplesheet.csv --strandedness reverse\r\n        ```\r\n\r\n    * The final analysis has been executed on the Azure platform using Azure Kubernetes Services (AKS). AKS has to be set up on the Azure platform by defining a standard node pool called sys next to the scalable node pool cpumem using Standard_E8ds_v4 as node size for calculation.\r\n      Furthermore, persistent volume claims (PVCs) have been setup for input and work folders of the nextflow runs. In the PVC `input` the reference data as well as the fastqc files have been stored where the PVC `work`, the temporary nextflow files for the individual runs as well as the output files have been stored.\r\n    * The config file for the final execution run for [RNAseq editing for the human samples and reference genome hg19](https://github.com/vibbits/rnaseq-editing/blob/master/nextflow.config.as-executed).    \r\n\r\n## Documentation\r\n\r\nThe nf-core/rnaseq pipeline comes with documentation about the pipeline [usage](https://nf-co.re/rnaseq/usage), [parameters](https://nf-co.re/rnaseq/parameters) and [output](https://nf-co.re/rnaseq/output).\r\n\r\n## Credits\r\nThese scripts were written to provide a reproducible data analysis pipeline until the downstream processing using dedicated R scripts for exploratory analysis and plotting. The general structure of pipeline is based on the data analysis steps of the our recent paper [ADAR1 interaction with Z-RNA promotes editing of endogenous double-stranded RNA and prevents MDA5-dependent immune activation](https://pubmed.ncbi.nlm.nih.gov/34380029/).\r\n\r\nNote: The nf-core scripts this pipeline is based on were originally written for use at the [National Genomics Infrastructure](https://ngisweden.scilifelab.se), part of [SciLifeLab](http://www.scilifelab.se/) in Stockholm, Sweden, by Phil Ewels ([@ewels](https://github.com/ewels)) and Rickard Hammar\u00e9n ([@Hammarn](https://github.com/Hammarn)).\r\n\r\nThe RNAseq pipeline was re-written in Nextflow DSL2 by Harshil Patel ([@drpatelh](https://github.com/drpatelh)) from [The Bioinformatics & Biostatistics Group](https://www.crick.ac.uk/research/science-technology-platforms/bioinformatics-and-biostatistics/) at [The Francis Crick Institute](https://www.crick.ac.uk/), London.\r\n\r\n## Citations\r\n\r\nThe `nf-core` publication is cited here as follows:\r\n\r\n> **The nf-core framework for community-curated bioinformatics pipelines.**\r\n>\r\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\r\n>\r\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "264",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/264?version=1",
        "name": "RNA sequencing data obtained from organisms with a reference genome and annotation followed by a prediction step of editing sites using RDDpred",
        "number_of_steps": 0,
        "projects": [
            "VIB Bioinformatics Core"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-07-07",
        "creators": [
            "Bart Nijsse",
            "Jasper Koehorst"
        ],
        "description": "### Workflow for Metagenomics from bins to metabolic models (GEMs)\r\n\r\n**Summary**\r\n  - Prodigal gene prediction\r\n  - CarveMe genome scale metabolic model reconstruction\r\n  - MEMOTE for metabolic model testing\r\n  - SMETANA Species METabolic interaction ANAlysis\r\n\r\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br>\r\n\r\n**All tool CWL files and other workflows can be found here:**<br>\r\nTools: https://gitlab.com/m-unlock/cwl<br>\r\nWorkflows: https://gitlab.com/m-unlock/cwl/workflows\r\n\r\n**How to setup and use an UNLOCK workflow:**<br>\r\nhttps://m-unlock.gitlab.io/docs/setup/setup.html<br>\r\n",
        "doi": null,
        "edam_operation": [
            "Metabolic pathway prediction"
        ],
        "edam_topic": [
            "Metagenomics",
            "Sequence analysis"
        ],
        "filtered_on": "edam",
        "id": "372",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/372?version=1",
        "name": "Metagenomic GEMs from Assembly",
        "number_of_steps": 11,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "gem",
            "genomics",
            "metagenomics",
            "carveme",
            "memote"
        ],
        "tools": [
            "Take a snapshot of a model's state and generate a report.",
            "Preparation of workflow output files to a specific output folder",
            "Compress CarveMe GEM",
            "Species METabolic interaction ANAlysis",
            "Compress prodigal protein files",
            "Genome-scale metabolic models reconstruction with CarveMe",
            "prodigal gene/protein prediction",
            "CarveMe GEM statistics",
            "MEMOTE run analsis"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-10-12",
        "creators": [
            "Georgina Samaha"
        ],
        "description": "# IndexReferenceFasta-nf\r\n===========\r\n\r\n  - [Description](#description)\r\n  - [Diagram](#diagram)\r\n  - [User guide](#user-guide)\r\n  - [Benchmarking](#benchmarking)\r\n  - [Workflow summaries](#workflow-summaries)\r\n      - [Metadata](#metadata)\r\n      - [Component tools](#component-tools)\r\n      - [Required (minimum)\r\n        inputs/parameters](#required-minimum-inputsparameters)\r\n  - [Additional notes](#additional-notes)\r\n  - [Help/FAQ/Troubleshooting](#helpfaqtroubleshooting)\r\n  - [Acknowledgements/citations/credits](#acknowledgementscitationscredits)\r\n\r\n---\r\n\r\n## Description\r\nThis is a flexible pipeline for generating common reference genome index files for WGS data analysis. IndexReferenceFasta-nf is a Nextflow (DSL2) pipeline that runs the following tools using Singularity containers:\r\n* Samtools faidx\r\n* BWA index\r\n* GATK CreateSequenceDictionary \r\n\r\n## Diagram\r\n<p align=\"center\"> \r\n<img src=\"https://user-images.githubusercontent.com/73086054/189310509-375fea4f-11fb-41ca-ba52-90760e9a5aa3.png\" width=\"80%\">\r\n</p> \r\n\r\n## User guide\r\n**1. Set up**\r\n\r\nClone this repository by running:\r\n```\r\ngit clone https://github.com/Sydney-Informatics-Hub/IndexReferenceFasta-nf.git\r\ncd IndexReferenceFasta-nf\r\n``` \r\n\r\n**2. Generate indexes**  \r\n\r\nUsers can specify which index files to create by using the `--samtools`, `--bwa`, and/or `--gatk` flags. All are optional. Run the pipeline with:\r\n\r\n```\r\nnextflow run main.nf /path/to/ref.fasta --bwa --samtools --gatk \r\n```\r\n\r\n## Benchmarking\r\n\r\n### Human hg38 reference assembly @ Pawsey's Nimbus (NCPU/task = 1)\r\n|task_id|hash     |native_id|name          |status   |exit|submit |duration  |realtime  |%cpu   |peak_rss|peak_vmem|rchar  |wchar  |\r\n|-------|---------|---------|--------------|---------|----|-------|----------|----------|-------|--------|---------|-------|-------|\r\n|3      |27/33fffc|131621   |samtools_index|COMPLETED|0   |55:44.9|12.2s     |12s       |99.20% |6.3 MB  |11.8 MB  |3 GB   |19.1 KB|\r\n|1      |80/f03e46|131999   |gatk_index    |COMPLETED|0   |55:46.7|22.6s     |22.3s     |231.90%|3.8 GB  |37.1 GB  |3.1 GB |726 KB |\r\n|2      |ea/e29535|131594   |bwa_index     |COMPLETED|0   |55:44.9|1h 50m 16s|1h 50m 15s|99.50% |4.5 GB  |4.5 GB   |12.1 GB|8.2 GB |\r\n\r\n## Workflow summaries\r\n\r\n### Metadata\r\n|metadata field     | workflow_name / workflow_version  |\r\n|-------------------|:---------------------------------:|\r\n|Version            | workflow_version                  |\r\n|Maturity           | under development                 |\r\n|Creators           | Georgie Samaha                    |\r\n|Source             | NA                                |\r\n|License            | GPL-3.0 license                   |\r\n|Workflow manager   | NextFlow                          |\r\n|Container          | None                              |\r\n|Install method     | Manual                            |\r\n|GitHub             | Sydney-Informatics-Hub/IndexReferenceFasta-nf                                |\r\n|bio.tools          | NA                                |\r\n|BioContainers      | NA                                | \r\n|bioconda           | NA                                |\r\n\r\n### Component tools\r\n\r\n* samtools/1.15.1\r\n* gatk/4.2.6.1 \r\n* bwa/0.7.17\r\n\r\n### Required (minimum) inputs/parameters\r\n\r\n* A reference genome file in fasta format.\r\n\r\n## Additional notes\r\n\r\n### Help/FAQ/Troubleshooting\r\n\r\n## Acknowledgements/citations/credits\r\n### Authors \r\n- Georgie Samaha (Sydney Informatics Hub, University of Sydney)   \r\n\r\n### Acknowledgements \r\n\r\n- This pipeline was built using the [Nextflow DSL2 template](https://github.com/Sydney-Informatics-Hub/Nextflow_DSL2_template).  \r\n- Documentation was created following the [Australian BioCommons documentation guidelines](https://github.com/AustralianBioCommons/doc_guidelines).  \r\n\r\n### Cite us to support us! \r\nAcknowledgements (and co-authorship, where appropriate) are an important way for us to demonstrate the value we bring to your research. Your research outcomes are vital for ongoing funding of the Sydney Informatics Hub and national compute facilities. We suggest including the following acknowledgement in any publications that follow from this work:  \r\n\r\nThe authors acknowledge the technical assistance provided by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney and the Australian BioCommons which is enabled by NCRIS via Bioplatforms Australia. \r\n",
        "doi": "10.48546/workflowhub.workflow.393.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "16S in description",
        "id": "393",
        "keep": "Reject",
        "latest_version": 1,
        "license": "LGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/393?version=1",
        "name": "IndexReferenceFasta-nf",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "bwa",
            "bioinformatics",
            "gatk",
            "genomics",
            "nextflow",
            "samtools",
            "wgs",
            "index",
            "referencegenome"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-06-07",
        "creators": [
            "Ekaterina Sakharova",
            "Varsha Kale",
            "Martin Beracochea"
        ],
        "description": "MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.\r\n\r\nDocumentation: https://docs.mgnify.org/en/latest/analysis.html#raw-reads-analysis-pipeline",
        "doi": "10.48546/workflowhub.workflow.362.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "362",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/362?version=1",
        "name": "MGnify - raw-reads analysis pipeline",
        "number_of_steps": 4,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cwl",
            "metagenomics",
            "workflows"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-06-07",
        "creators": [
            "Alex L Mitchell",
            " Alexandre Almeida",
            " Martin Beracochea",
            " Miguel Boland",
            " Josephine Burgin",
            " Guy Cochrane",
            " Michael R Crusoe",
            " Varsha Kale",
            " Simon C Potter",
            " Lorna J Richardson",
            " Ekaterina Sakharova",
            " Maxim Scheremetjew",
            " Anton Korobeynikov",
            " Alex Shlemov",
            " Olga Kunyavskaya",
            " Alla Lapidus",
            " Robert D Finn"
        ],
        "description": "MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.\r\n\r\nDocumentation: https://docs.mgnify.org/en/latest/analysis.html#amplicon-analysis-pipeline\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "361",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/361?version=1",
        "name": "MGnify - amplicon analysis pipeline",
        "number_of_steps": 3,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cwl",
            "metagenomics",
            "rna",
            "workflow"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-06-07",
        "creators": [
            " Alex L Mitchell",
            " Alexandre Almeida",
            " Martin Beracochea",
            " Miguel Boland",
            " Josephine Burgin",
            " Guy Cochrane",
            " Michael R Crusoe",
            " Varsha Kale",
            " Simon C Potter",
            " Lorna J Richardson",
            " Ekaterina Sakharova",
            " Maxim Scheremetjew",
            " Anton Korobeynikov",
            " Alex Shlemov",
            " Olga Kunyavskaya",
            " Alla Lapidus",
            " Robert D Finn"
        ],
        "description": "MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.\r\n\r\nDocumentation: https://docs.mgnify.org/en/latest/analysis.html#assembly-analysis-pipeline\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in tags",
        "id": "360",
        "keep": "To Curate",
        "latest_version": 2,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/360?version=2",
        "name": "MGnify - assembly analysis pipeline",
        "number_of_steps": 4,
        "projects": [
            "MGnify",
            "HoloFood at MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "annotation",
            "cwl",
            "metagenomics",
            "workflow"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-04-28",
        "versions": 2
    },
    {
        "create_time": "2022-05-10",
        "creators": [],
        "description": "# HiFi *de novo* genome assembly workflow\r\n\r\nHiFi-assembly-workflow is a bioinformatics pipeline that can be used to analyse Pacbio CCS reads for *de novo* genome assembly using PacBio Circular Consensus Sequencing (CCS)  reads. This workflow is implemented in Nextflow and has 3 major sections. \r\n \r\nPlease refer to the following documentation for detailed description of each workflow section:\r\n \r\n- [Pre-assembly quality control (QC)](https://github.com/AusARG/hifi-assembly-workflow/blob/master/recommendations.md#stage-1-pre-assembly-quality-control)\r\n- [Assembly](https://github.com/AusARG/hifi-assembly-workflow/blob/master/recommendations.md#stage-2-assembly)\r\n- [Post-assembly QC](https://github.com/AusARG/hifi-assembly-workflow/blob/master/recommendations.md#stage-3-post-assembly-quality-control)\r\n\r\n## HiFi assembly workflow flowchart\r\n\r\n![](https://github.com/AusARG/hifi-assembly-workflow/blob/master/workflow.png?raw=true)\r\n\r\n# Quick Usage:\r\nThe pipeline has been tested  on NCI Gadi and AGRF balder cluster. If needed to run on AGRF cluster, please contact us at bioinformatics@agrf.org.au.\r\nPlease note for running this on NCI Gadi you need access. Please refer to Gadi guidelines for account creation and usage: these can be found at https://opus.nci.org.au/display/Help/Access.\r\n\r\nHere is an example that can be used to run a phased assembly on Gadi:\r\n\r\n```\r\nModule load nextflow/21.04.3\r\nnextflow run Hifi_assembly.nf \u2013bam_folder <PATH TO THE BAM FOLDER> -profile gadi \r\n\r\nThe workflow accepts 2 mandatory arguments:\r\n--bam_folder     --    Full Path to the CCS bam files\r\n-profile         --    gadi/balder/local\r\n```\r\n\r\nPlease note that you can either run jobs interactively or submit jobs to the cluster. This is determined by the -profile flag. By passing the gadi tag to the profile argument, the jobs are submitted and run on the cluster.\r\n\r\n# General recommendations for using the HiFi *de novo* genome assembly workflow\r\n\r\n## Example local profile usage\r\n\r\n```\r\nStart a screen, submit a job, and run the workflow \r\nScreen -S \u2018name\u2019\r\n\r\nqsub -I -qnormal -Pwz54 -lwalltime=48:00:00,ncpus=4,mem=200GB,storage=scratch/wz54+gdata/wz54,wd\r\nexport MODULEPATH=/apps/Modules/modulefiles:/g/data/wz54/groupResources/modules\r\n\r\nmodule load nextflow/21.04.3\r\nnextflow run /g/data/wz54/groupResources/scripts/pl/hifi_assembly.nf  --bam_folder  <bam-folder_path> -profile local\r\n\r\n#This load the scripts directory to the environmental PATH and load nextflow module\r\nmodule load hifi_assembly/1.0.0 \r\n```\r\n\r\n# Outputs\r\n\r\nPipeline generates various files and folders here is a brief description: \r\nThe pipeline creates a folder called `secondary_analysis` that contains two sub folders named:\r\n\r\n- `exeReport`     \r\n- `Results`       -- Contains preQC, assembly and postQC analysis files\r\n\r\n## exeReport\r\nThis folder contains a computation resource usage summary in various charts and a text file. \r\n`report.html` provides a comprehensive summary.\r\n\r\n## Results\r\nThe `Results` folder contains three sub-directories preQC, assembly and postqc. As the name suggests, outputs from the respective workflow sections are placed in each of these folders.\r\n\r\n### preQC\r\nThe following table contains list of files and folder from preQC results\r\n\r\n| Output folder/file | File             | Description                                                                    |\r\n| ------------------ | ---------------- | ------------------------------------------------------------------------------ |\r\n| <sample>.fa        |                  | Bam files converted to fasta format                                            |\r\n| kmer\\_analysis     |                  | Folder containing kmer analysis outputs                                        |\r\n|                    | <sample>.jf      | k-mer counts from each sample                                                  |\r\n|                    | <sample>.histo   | histogram of k-mer occurrence                                                  |\r\n| genome\\_profiling  |                  | genomescope profiling outputs                                                  |\r\n|                    | summary.txt      | Summary metrics of genome scope outputs                                        |\r\n|                    | linear\\_plot.png | Plot showing no. of times a k-mer observed by no. of k-mers with that coverage |\r\n\r\n\r\n### Assembly\r\nThis folder contains final assembly results in <FASTA> format.\r\n\r\n- `<sample>_primary.fa` - Fasta file containing primary contigs\r\n- `<sample>_associate.fa` - Fasta file containing associated contigs\r\n\r\n### postqc\r\n \r\nThe postqc folder contains two sub folders \r\n\r\n- `assembly_completeness`\r\n- `assembly_evaluation`\r\n\r\n#### assembly_completeness\r\nThis contains BUSCO evaluation results for primary and associate contig.\r\n\r\n#### assembly_evaluation\r\nAssembly evaluation folder contains various file formats, here is a brief description for each of the outputs.\r\n\r\n| File        | Description                                                                               |\r\n| ----------- | ----------------------------------------------------------------------------------------- |\r\n| report.txt  | Assessment summary in plain text format                                                   |\r\n| report.tsv  | Tab-separated version of the summary, suitable for spreadsheets (Google Docs, Excel, etc) |\r\n| report.tex  | LaTeX version of the summary                                                              |\r\n| icarus.html | Icarus main menu with links to interactive viewers                                        |\r\n| report.html | HTML version of the report with interactive plots inside                                  |\r\n\r\n\r\n# Infrastructure usage and recommendations\r\n\r\n### NCI facility access\r\nOne should have a user account set with NCI to access gadi high performance computational facility. Setting up a NCI account is mentioned in detail at the following URL: https://opus.nci.org.au/display/Help/Setting+up+your+NCI+Account \r\n  \r\nDocumentation for a specific infrastructure should go into a infrastructure documentation template\r\nhttps://github.com/AustralianBioCommons/doc_guidelines/blob/master/infrastructure_optimisation.md\r\n\r\n\r\n## Compute resource usage across tested infrastructures\r\n\r\n|                                       | Computational resource for plant case study |\r\n| ------------------------------------- | ------------------------------------------- |\r\n|                                       | Time                                        | CPU | Memory | I/O |\r\n| Process                               | duration                                    | realtime | %cpu | peak\\_rss | peak\\_vmem | rchar | wchar |\r\n| Converting bam to fasta for sample    | 12m 54s                                     | 12m 48s | 99.80% | 5.2 MB | 197.7 MB | 43.3 GB | 50.1 GB |\r\n| Generating k-mer counts and histogram | 26m 43s                                     | 26m 36s | 1725.30% | 19.5 GB | 21 GB | 77.2 GB | 27.1 GB |\r\n| Profiling genome characteristics      | 34.7s                                       | 13.2s | 89.00% | 135 MB | 601.2 MB | 8.5 MB | 845.9 KB |\r\n| Denovo assembly                       | 6h 51m 15s                                  | 6h 51m 11s | 4744.40% | 84.7 GB | 225.6 GB | 1.4 TB | 456 GB |\r\n| evaluate\\_assemblies                  | 5m 18s                                      | 4m 54s | 98.20% | 1.6 GB | 1.9 GB | 13.6 GB | 2.8 GB |\r\n| assemblies\\_completeness              | 25m 57s                                     | 25m 53s | 2624.20% | 22 GB | 25.2 GB | 624.9 GB | 2.9 GB |\r\n\r\n\r\n|                                       | Computational resource for bird case study |\r\n| ------------------------------------- | ------------------------------------------ |\r\n|                                       | Time                                       | CPU | Memory | I/O |\r\n| Process                               | duration                                   | realtime | %cpu | peak\\_rss | peak\\_vmem | rchar | wchar |\r\n| Converting bam to fasta for sample    | 12m 54s                                    | 7m 9s | 86.40% | 5.2 MB | 197.8 MB | 21.5 GB | 27.4 GB |\r\n| Generating k-mer counts and histogram | 26m 43s                                    | 15m 34s | 1687.70% | 10.1 GB | 11.7 GB | 44 GB | 16.6 GB |\r\n| Profiling genome characteristics      | 34.7s                                      | 1m 15s | 15.30% | 181.7 MB | 562.2 MB | 8.5 MB | 819.1 KB |\r\n| De novo assembly                      | 6h 51m 15s                                 | 9h 2m 47s | 1853.50% | 67.3 GB | 98.4 GB | 1 TB | 395.6 GB |\r\n| evaluate assemblies                   | 5m 18s                                     | 2m 48s | 97.50% | 1.1 GB | 1.4 GB | 8.7 GB | 1.8 GB |\r\n| assemblies completeness               | 25m 57s                                    | 22m 36s | 2144.00% | 22.2 GB | 25 GB | 389.7 GB | 1.4 GB |\r\n\r\n\r\n# Workflow summaries\r\n\r\n## Metadata\r\n\r\n| Metadata field   | Pre-assembly quality control                                                      | Primary assembly   | Post-assembly quality control |\r\n| ---------------- | --------------------------------------------------------------------------------- | ------------------ | ----------------------------- |\r\n| Version          | 1.0                                                                               | 1.0                | 1.0                           |\r\n| Maturity         | Production                                                                        | Production         | production                    |\r\n| Creators         | Naga, Kenneth                                                                     | Naga, Kenneth      | Naga, Kenneth                 |\r\n| Source           | [AusARG/hifi-assembly-workflow](https://github.com/AusARG/hifi-assembly-workflow) |\r\n| License          |  MIT License                                                                       | MIT License         | MIT License                     |\r\n| Workflow manager | NextFlow                                                                          | NextFlow           | NextFlow                      |\r\n| Container        | No containers used                                                                | No containers used | No containers used            |\r\n| Install method   | Manual                                                                            | Manual             | Manual                        |\r\n\r\n\r\n## Component tools\r\n\u200b\r\n| Workflow element                  | Workflow element version | Workflow title                |\r\n| --------------------------------- | ------------------------ | ----------------------------- |\r\n| Samtools, jellyfish, genomescope  | 1.0                      | Pre-assembly quality control  |\r\n| Improved phased assembler (pbipa) | 1.0                      | Primary assembly              |\r\n| Quast and busco                   | 1.0                      | Post-assembly quality control |\r\n\r\n\r\n## Required (minimum) inputs/parameters\r\n \r\nPATH to HIFI bam folder is the minimum requirement for the processing the pipeline.\r\n\r\n## Third party tools / dependencies\r\n\r\nThe following packages are used by the pipeline.\r\n\r\n- `nextflow/21.04.3`\r\n- `samtools/1.12`\r\n- `jellyfish/2.3.0`\r\n- `genomescope/2.0`\r\n- `ipa/1.3.1`\r\n- `quast/5.0.2`\r\n- `busco/5.2.2`\r\n\r\nThe following paths contain all modules required for the pipeline.\r\n\r\n- `/apps/Modules/modulefiles`\r\n- `/g/data/wz54/groupResources/modules`\r\n\r\n---\r\n\r\n# Help/FAQ/Troubleshooting\r\n\r\nDirect training and help is available if you are new to HPC and/or new to NCI/Gadi.\r\n\r\n- Basic information to get started with the NCI Gadi for bioinformatics can be found at https://github.com/AusARG/ABLeS/wiki/temppage.\r\n- For NCI support, contact the NCI helpdesk directly at https://www.nci.org.au/users/nci-helpdesk\r\n- Queue limits and structure explained at https://opus.nci.org.au/display/Help/4.+PBS+Jobs\r\n\r\n---\r\n\r\n# 3rd party Tutorials \r\n\r\nA tutorial by Andrew Severin on running GenomeScope 1.0 is available here:\r\nhttps://github.com/AusARG/hifi-assembly-workflow.git\r\n\r\nImproved Phased Assembler tutorial is available at \r\nhttps://github.com/PacificBiosciences/pbbioconda/wiki/Improved-Phased-Assembler\r\n\r\nBusco tutorial\r\nhttps://wurmlab.com/genomicscourse/2016-SIB/practicals/busco/busco_tutorial\r\n\r\n---\r\n\r\n# Licence(s)\r\n\r\nMIT License\r\n\r\nCopyright (c) 2022 AusARG\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining a copy\r\nof this software and associated documentation files (the \"Software\"), to deal\r\nin the Software without restriction, including without limitation the rights\r\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\ncopies of the Software, and to permit persons to whom the Software is\r\nfurnished to do so, subject to the following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be included in all\r\ncopies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\nSOFTWARE.\r\n\r\n---\r\n\r\n# Acknowledgements/citations/credits\r\n\r\n> Jung, H. et al. Twelve quick steps for genome assembly and annotation in the classroom. PLoS Comput. Biol. 16, 1\u201325 (2020).\r\n\r\n> 2020, G. A. W. No Title. https://ucdavis-bioinformatics-training.github.io/2020-Genome_Assembly_Workshop/kmers/kmers.\r\n\r\n> Sovi\u0107, I. et al. Improved Phased Assembly using HiFi Data. (2020).\r\n\r\n> Gurevich, A., Saveliev, V., Vyahhi, N. & Tesler, G. QUAST: Quality assessment tool for genome assemblies. Bioinformatics 29, 1072\u20131075 (2013).\r\n\r\n> Waterhouse, R. M. et al. BUSCO applications from quality assessments to gene prediction and phylogenomics. Mol. Biol. Evol. 35, 543\u2013548 (2018).\r\n\r\n---\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "340",
        "keep": "Reject",
        "latest_version": 1,
        "license": "CC-BY-4.0",
        "link": "https:/workflowhub.eu/workflows/340?version=1",
        "name": "HiFi de novo genome assembly workflow",
        "number_of_steps": 0,
        "projects": [
            "AGRF BIO"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-04-07",
        "creators": [
            "Cali Willet",
            "Rosemarie Sadsad",
            "Tracy Chew"
        ],
        "description": "# Shotgun Metagenomics Analysis\r\nAnalysis of metagenomic shotgun sequences including assembly, speciation, ARG discovery and more\r\n\r\n## Description\r\nThe input for this analysis is paired end next generation sequencing data from metagenomic samples. The workflow is designed to be modular, so that individual modules can be run depending on the nature of the metagenomics project at hand. More modules will be added as we develop them - this repo is a work in progress!\r\n\r\nThese scripts have been written specifically for NCI Gadi HPC, wich runs PBS Pro, however feel free to use and modify for anothre system if you are not a Gadi user. \r\n\r\n### Part 1. Setup and QC\r\nDownload the repo. You will see directories for `Fastq`, `Inputs`, `Reference` and `Logs`. You will need to copy or symlink your fastq to `Fastq`, sample configuration file (see below) to `Inputs` and the reference genome sequence of your host species (if applicable) to `Reference` for host contamination removal.\r\n \r\n\r\n#### Fastq inputs\r\nThe scripts assume all fastq files are paired, gzipped, and all in the one directory named 'Fastq'. If your fastq are within a convoluted directory structure (eg per-sample directories) or you would simply like to link them from an alternate location, please use the script `setup_fastq.sh`.\r\n\r\nTo use this script, parse the path name of your fastq as first argument on the command line, and run the script from the base working directory (<your_path>/Shotgun-Metagenomics-Analysis) which will from here on be referred to as `workdir`. Note that this script looks for `f*q.gz` files (ie fastq.gz or fq.gz) - if yours differ in suffix, please adjust the script accordingly.\r\n\r\n```\r\nbash ./Scripts/setup_fastq.sh </path/to/your/parent/fastq/directory>\r\n```\r\n\r\n#### Configuration/sample info\r\nThe only required input configuration file should be named <cohort>.config, where <cohort> is the name of the current batch of samples you are processing, or some other meaningful name to your project; it will be used to name output files. The config file should be placed inside the $workdir/Inputs directory, and include the following columns, in this order:\r\n\r\n```\r\n1. Sample ID - used to identify the sample, eg if you have 3 lanes of sequencing per sample, erach of those 6 fastq files should contain this ID that si in column 1\r\n2. Lab Sample ID - can be the same as column 1, or different if you have reason to change the IDs eg if the seq centre applies an in-house ID. Please make sure IDs are unique within column 1 and unique within column 2\r\n3. Group - eg different time points or treatment groups. If no specific group structure is relevant, please set this to 1 (do not leave blank!) \r\n3. Platform - should be Illumina; other sequencing platforms are not tested on this workflow\r\n4. Sequencing centre name\r\n5. Library - eg if you have 2 sequencing libraries for the same sample. Can be left blank, or assigned to 1. Blank will be assigned libray ID of 1 during processing.\r\n```\r\n\r\nPlease do not have spaces in any of the values for the config file. \r\n\r\n\r\n#### General setup\r\n\r\nAll scripts will need to be edited to reflect your NCI project code at the `-P <project>` and `-l <storage> directive. Please run the script create_project.sh and follow the prompts to complete some of the setup for you. \r\n\r\nNote that you will need to manually edit the PDS resource requests for each PBS script; guidelines/example resources will be given at each step to help you do this. As the 'sed' commands within this script operate on .sh and .pbs files, this setup script has been intentionally named .bash (easiest solution).\r\n\r\nRemember to submit all scripts from your `workdir`. \r\n\r\n`bash ./Scripts/create_project.sh`\r\n\r\nFor jobs that execute in parallel, there are 3 scripts: one to make the 'inputs' file listing hte details of each parallel task, one job execution shell script that is run over each task in parallel, and one PBS launcher script. The process is to submit the make input script, check it to make sure your job details are correct, edit the resources directives depending on the number and size of your parallel tasks, then submit the PBS launcher script with `qsub`. \r\n\r\n#### QC\r\n\r\nRun fastQC over each fastq file in parallel. Adjust the resources as per your project. To run all files in parallel, set the number of NCPUS requested equal to the number of fastq files (remember that Gadi can only request <1 node or multiples of whole nodes). The make input script sorts the fastq files largest to smallest, so if you have a discrpeancy in file size, optimal efficiency can be achieved by requested less nodes than the total required to run all your fastq in parallel.\r\n\r\nFastQC does not multithread on a single file, so CPUs per parallel task is set to 1. Example walltimes on Gadi 'normal' queue:  one 1.8 GB fastq = 4 minutes; one 52 GB fastq file = 69.5 minutes.\r\n\r\nMake the fastqc parallel inputs file by running (from `workdir`):\r\n`bash ./Scripts/fastqc_make_inputs.sh`\r\n\r\nEdit the resource requests in `fastqc_run_parallel.pbs` according to your number of fastq files and their size, then submit:\r\n`qsub fastqc_run_parallel.pbs`\r\n\r\nTo ease manual inspection of the fastQC output, running `multiqc` is recommended. This will collate the individual fastQC reports into one report. This can be done on the login node for small sample numbers, or using the below script for larger cohorts. Edit the PBS directives, then run:\r\n\r\n`qsub multiqc.pbs`\r\n\r\nSave a copy of ./MultiQC/multiqc_report.html to your local disk then open in a web browser to inspect the results. \r\n\r\n#### Quality filtering and trimming\r\n\r\nWill be added at a later date. This is highly dependent on the quality of your data and your individual project needs so will be a guide only. \r\n\r\n### Part 2. Removal of host contamination. \r\n\r\nIf you have metagenomic data extracted from a host, you will need a copy of the host reference genome sequence in order to remove any DNA sequences belonging to the host. Even if your wetlab protocol included a host removal step, it is still important to run bioinformatic host removal.\r\n\r\n\r\n#### Prepare the reference\r\nEnsure you have a copy of the reference genome (or symlink) in ./Fasta. This workflow requires BBtools(tested with version 37.98). As of writing, BBtools is not available as a global app on Gadi. Please install locally and make \"module loadable\", or else edit the scripts to point directly to your local BBtools installation.\r\n\r\nBBtools repeat masking will use all available threads on machine and 85% of available mem by default. For a mammalian genome, 2 hours on one Gadi 'normal' node is sufficient for repeat masking. \r\n\r\nUpdate the name of your reference fastq in the `bbmap_prep.pbs` script (and BBtools, see note above), then run:\r\n`qsub ./Scripts/bbmap_prep.pbs`\r\n\r\n#### Host contamination removal\r\n\r\nTBC 1/4/22... \r\n",
        "doi": "10.48546/workflowhub.workflow.327.1",
        "edam_operation": [],
        "edam_topic": [
            "Metagenomic sequencing",
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "327",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/327?version=1",
        "name": "Shotgun-Metagenomics-Analysis",
        "number_of_steps": 0,
        "projects": [
            "Sydney Informatics Hub",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "diamond",
            "metagenomics",
            "abricate",
            "antimicrobial resistance",
            "bbmap",
            "braken",
            "humann2",
            "kraken",
            "prokka",
            "shotgun",
            "whole genome sequencing"
        ],
        "tools": [],
        "type": "Shell Script",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-04-05",
        "creators": [
            "Delphine Lariviere"
        ],
        "description": "Create Meryl Database used for the estimation of assembly parameters and quality control with Merqury. Part of the VGP pipeline.",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in name",
        "id": "309",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/309?version=1",
        "name": "VGP genome profile analysis",
        "number_of_steps": 6,
        "projects": [
            "usegalaxy-eu"
        ],
        "source": "WorkflowHub",
        "tags": [
            "assembly",
            "galaxy",
            "vgp"
        ],
        "tools": [],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-09-24",
        "creators": [
            "Valentin Tilloy",
            "Pierre Cuzin",
            "Laura Leroi",
            "Patrick Durand"
        ],
        "description": "ASPICov was developed to provide a rapid, reliable and complete analysis of NGS SARS-Cov2 samples to the biologist. This broad application tool allows to process samples from either capture or amplicon strategy and Illumina or Ion Torrent technology. To ensure FAIR data analysis, this Nextflow pipeline follows nf-core guidelines and use Singularity containers. \r\n\r\nAvailability and Implementation:\u00a0https://gitlab.com/vtilloy/aspicov\r\n\r\nCitation: Valentin Tilloy, Pierre Cuzin, Laura Leroi, Emilie Gu\u00e9rin, Patrick Durand, Sophie Alain\r\n\t\t\t\t\t\t\tASPICov: An automated pipeline for identification of SARS-Cov2 nucleotidic variants\r\n\t\t\t\t\t\t\tPLoS One 2022 Jan 26;17(1):e0262953: https://pubmed.ncbi.nlm.nih.gov/35081137/",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "Amplicon in description",
        "id": "192",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/192?version=1",
        "name": "ASPICov",
        "number_of_steps": 0,
        "projects": [
            "CHU Limoges - UF9481 Bioinformatique / CNR Herpesvirus"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2022-01-27",
        "creators": [
            "Luca Pireddu"
        ],
        "description": "# Snakemake workflow: FAIR CRCC - send data\r\n\r\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22656.3.0-brightgreen.svg)](https://snakemake.github.io)\r\n[![GitHub actions status](https://github.com/crs4/fair-crcc-send-data/workflows/Tests/badge.svg?branch=main)](https://github.com/crs4/fair-crcc-send-data/actions?query=branch%3Amain+workflow%3ATests)\r\n\r\n\r\nA Snakemake workflow for securely sharing Crypt4GH-encrypted sensitive data from\r\nthe [CRC\r\nCohort](https://www.bbmri-eric.eu/scientific-collaboration/colorectal-cancer-cohort/)\r\nto a destination approved through a successful [access\r\nrequest](https://www.bbmri-eric.eu/services/access-policies/).\r\n\r\nThe recommendation is to create a directory for the request that has been\r\napproved;  it will be used as the working directory for the run.  Copy there the\r\nrecipient's crypt4gh key and prepare the run configuration.  The configuration\r\nwill specify the repository, the destination of the data, and the list of\r\nfiles/directories to transfer.\r\n\r\n\r\n## What's the CRC Cohort?\r\n\r\nThe CRC Cohort is a collection of clinical data and digital high-resolution\r\ndigital pathology images pertaining to tumor cases.  The collection has been\r\nassembled from a number of participating biobanks and other partners through the\r\n[ADOPT BBMRI-ERIC](https://www.bbmri-eric.eu/scientific-collaboration/adopt-bbmri-eric/) project.\r\n\r\nResearchers interested in using the data for science can file an application for\r\naccess.  If approved, the part of the dataset required for the planned and\r\napproved work can be copied to the requester's selected secure storage location\r\n(using this workflow).\r\n\r\n\r\n## Usage\r\n\r\n### Example\r\n\r\n    mkdir request_1234 && cd request_1234\r\n    # Now write the configuration, specifying crypt4gh keys, destination and files to send.\r\n    # Finally, execute workflow.\r\n    snakemake --snakefile ../fair-crcc-send-data/workflow/Snakefile --profile ../profile/ --configfile config.yml --use-singularity --cores\r\n\r\n\r\n#### Run configuration example\r\n\r\n```\r\nrecipient_key: ./recipient_key\r\nrepository:\r\n  path: \"/mnt/rbd/data/sftp/fair-crcc/\"\r\n  private_key: bbmri-key\r\n  public_key: bbmri-key.pub\r\nsources:\r\n  glob_extension: \".tiff.c4gh\"\r\n  items:\r\n  - some/directory/to/glob\r\n  - another/individual/file.tiff.c4gh\r\ndestination:\r\n  type: \"S3\"\r\n  root_path: \"my-bucket/prefix/\"\r\n  connection:  # all elements will be passed to the selected snakemake remote provider\r\n    access_key_id: \"MYACCESSKEY\"\r\n    secret_access_key: \"MYSECRET\"\r\n    host: http://localhost:9000\r\n    verify: false # don't verify ssl certificates\r\n```\r\n\r\n\r\nTODO\r\n\r\nThe usage of this workflow is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?usage=crs4%2Ffair-crcc-send-data).\r\n\r\nIf you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this (original) fair-crcc-send-datasitory and its DOI (see above).\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "265",
        "keep": "Reject",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/265?version=1",
        "name": "FAIR CRCC - send data",
        "number_of_steps": 0,
        "projects": [
            "CRC Cohort"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2024-09-15",
        "versions": 1
    },
    {
        "create_time": "2021-11-29",
        "creators": [
            "Jos\u00e9 M\u00aa Fern\u00e1ndez",
            "Asier Gonzalez-Uriarte"
        ],
        "description": "## Description\r\n\r\nThe workflow takes an input file with Cancer Driver Genes predictions (i.e. the results provided by a participant), computes a set of metrics, and compares them against the data currently stored in OpenEBench within the TCGA community. Two assessment metrics are provided for that predictions. Also, some plots (which are optional) that allow to visualize the performance of the tool are generated. The workflow consists in three standard steps, defined by OpenEBench. The tools needed to run these steps are containerised in three Docker images, whose recipes are available in the [TCGA_benchmarking_dockers](https://github.com/inab/TCGA_benchmarking_dockers ) repository and the images are stored in the [INB GitLab container registry](https://gitlab.bsc.es/inb/elixir/openebench/workflows/tcga_benchmarking_dockers/container_registry) . Separated instances are spawned from these images for each step:\r\n1. **Validation**: the input file format is checked and, if required, the content of the file is validated (e.g check whether the submitted gene IDs exist)\r\n2. **Metrics Generation**: the predictions are compared with the 'Gold Standards' provided by the community, which results in two performance metrics - precision (Positive Predictive Value) and recall(True Positive Rate).\r\n3. **Consolidation**: the benchmark itself is performed by merging the tool metrics with the rest of TCGA data. The results are provided in JSON format and SVG format (scatter plot).\r\n\r\n![OpenEBench benchmarking workflow](https://raw.githubusercontent.com/inab/TCGA_benchmarking_workflow/1.0.8/workflow_schema.jpg)\r\n\r\n## Data\r\n\r\n* [TCGA_sample_data](./TCGA_sample_data) folder contains all the reference data required by the steps. It is derived from the manuscript:\r\n[Comprehensive Characterization of Cancer Driver Genes and Mutations](https://www.cell.com/cell/fulltext/S0092-8674%2818%2930237-X?code=cell-site), Bailey et al, 2018, Cell [![doi:10.1016/j.cell.2018.02.060](https://img.shields.io/badge/doi-10.1016%2Fj.cell.2018.02.060-green.svg)](https://doi.org/10.1016/j.cell.2018.02.060) \r\n* [TCGA_sample_out](./TCGA_sample_out) folder contains an example output for a worklow run, with two cancer types / challenges selected (ACC, BRCA). Results obtained from the default execution should be similar to those ones available in this directory. Results found in [TCGA_sample_out/results](./TCGA_sample_out/results) can be visualized in the browser using [`benchmarking_workflows_results_visualizer` javascript library](https://github.com/inab/benchmarking_workflows_results_visualizer).\r\n\r\n## Requirements\r\nThis workflow depends on three tools that have to be installed before you can run it:\r\n* [Git](https://git-scm.com/downloads): Used to download the workflow from GitHub.\r\n* [Docker](https://docs.docker.com/get-docker/): The Docker Engine is used under the hood to execute the containerised steps of the benchmarking workflow.\r\n* [Nextflow](https://www.nextflow.io/): Is the technology used to write and execute the benchmarking workflow. Note that it depends on Bash (>=3.2) and Java (>=8 , <=17). We provide the script [run_local_nextflow.bash](run_local_nextflow.bash) that automates their installation for local testing.\r\n\r\nCheck that these tools are available in your environment:\r\n```\r\n# Git\r\n> which git\r\n/usr/bin/git\r\n> git --version\r\ngit version 2.26.2\r\n\r\n# Docker\r\n> which docker\r\n/usr/bin/docker\r\n> docker --version\r\nDocker version 20.10.9-ce, build 79ea9d308018\r\n\r\n# Nextflow\r\n> which nextflow\r\n/home/myuser/bin/nextflow\r\n> nextflow -version\r\n\r\n      N E X T F L O W\r\n      version 21.04.1 build 5556\r\n      created 14-05-2021 15:20 UTC (17:20 CEST)\r\n      cite doi:10.1038/nbt.3820\r\n      http://nextflow.io\r\n```\r\nIn the case of docker, apart from being installed the daemon has to be running. On Linux distributions that use `Systemd` for service management, which includes the most popular ones as of 2021 (Ubuntu, Debian, CentOs, Red Hat, OpenSuse), the `systemctl` command can be used to check its status and manage it:\r\n\r\n```\r\n# Check status of docker daemon\r\n> sudo systemctl status docker\r\n\u25cf docker.service - Docker Application Container Engine\r\n   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)\r\n   Active: inactive (dead)\r\n     Docs: http://docs.docker.com\r\n\r\n# Start docker daemon\r\n> sudo systemctl start docker\r\n```\r\n\r\n### Download workflow\r\nSimply clone the repository and check out the latest tag (currently `1.0.8`):\r\n\r\n```\r\n# Clone repository\r\n> git clone https://github.com/inab/TCGA_benchmarking_dockers.git\r\n\r\n# Move to new directory\r\ncd TCGA_benchmarking_workflow/\r\n\r\n# Checkout version 1.0.8\r\n> git checkout 1.0.8 -b 1.0.8\r\n```\r\n\r\n## Usage\r\nThe workflow can be run workflow in two different ways:\r\n* Standard: `nextflow run main.nf -profile docker`\r\n* Using the bash script that installs Java and Nextflow:`./run_local_nextflow.bash run main.nf -profile docker`.\r\n\r\nArguments specifications:\r\n```\r\nUsage:\r\nRun the pipeline with default parameters:\r\nnextflow run main.nf -profile docker\r\n\r\nRun with user parameters:\r\nnextflow run main.nf -profile docker --predictionsFile {driver.genes.file} --public_ref_dir {validation.reference.file} --participant_name {tool.name} --metrics_ref_dir {gold.standards.dir} --cancer_types {analyzed.cancer.types} --assess_dir {benchmark.data.dir} --results_dir {output.dir}\r\n\r\nMandatory arguments:\r\n\t--input                 List of cancer genes prediction\r\n\t--community_id          Name or OEB permanent ID for the benchmarking community\r\n\t--public_ref_dir        Directory with list of cancer genes used to validate the predictions\r\n\t--participant_id        Name of the tool used for prediction\r\n\t--goldstandard_dir      Dir that contains metrics reference datasets for all cancer types\r\n\t--challenges_ids        List of types of cancer selected by the user, separated by spaces\r\n\t--assess_dir            Dir where the data for the benchmark are stored\r\n\r\nOther options:\r\n\t--validation_result     The output directory where the results from validation step will be saved\r\n\t--augmented_assess_dir  Dir where the augmented data for the benchmark are stored\r\n\t--assessment_results    The output directory where the results from the computed metrics step will be saved\r\n\t--outdir                The output directory where the consolidation of the benchmark will be saved\r\n\t--statsdir              The output directory with nextflow statistics\r\n\t--data_model_export_dir The output dir where json file with benchmarking data model contents will be saved\r\n\t--otherdir              The output directory where custom results will be saved (no directory inside)\r\nFlags:\r\n\t--help                  Display this message\r\n```\r\n\r\nDefault input parameters and Docker images to use for each step can be specified in the [config](./nextflow.config) file.\r\n\r\n**NOTE: In order to make your workflow compatible with the [OpenEBench VRE Nextflow Executor](https://github.com/inab/vre-process_nextflow-executor), please make sure to use the same parameter names in your workflow.**\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "profil.* in description",
        "id": "244",
        "keep": "To Curate",
        "latest_version": 4,
        "license": "LGPL-3.0",
        "link": "https:/workflowhub.eu/workflows/244?version=4",
        "name": "OpenEBench TCGA Cancer Driver Genes benchmarking workflow",
        "number_of_steps": 0,
        "projects": [
            "OpenEBench"
        ],
        "source": "WorkflowHub",
        "tags": [
            "benchmarking",
            "openebench",
            "tcga"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 4
    },
    {
        "create_time": "2021-11-21",
        "creators": [],
        "description": "`atavide` is a complete workflow for metagenomics data analysis, including QC/QA, optional host removal, assembly and cross-assembly, and individual read based annotations. We have also built in some advanced analytics including tools to assign annotations from reads to contigs, and to generate metagenome-assembled genomes in several different ways, giving you the power to explore your data!\r\n\r\n`atavide` is 100% snakemake and conda, so you only need to install the snakemake workflow, and then everything else will be installed with conda.\r\n\r\nSteps:\r\n1. QC/QA with [prinseq++](https://github.com/Adrian-Cantu/PRINSEQ-plus-plus)\r\n2. optional host removal using bowtie2 and samtools, [as described previously](https://edwards.flinders.edu.au/command-line-deconseq/). To enable this, you need to provide a path to the host db and a host db.\r\n\r\nMetagenome assembly\r\n1. pairwise assembly of each sample using [megahit](https://github.com/voutcn/megahit)\r\n2. extraction of all reads that do not assemble using samtools flags\r\n3. assembly of all unassembled reads using [megahit](https://github.com/voutcn/megahit)\r\n4. compilation of _all_ contigs into a single unified set using [Flye](https://github.com/fenderglass/Flye)\r\n5. comparison of reads -> contigs to generate coverage\r\n\r\nMAG creation\r\n1. [metabat](https://bitbucket.org/berkeleylab/metabat/src/master/)\r\n2. [concoct](https://github.com/BinPro/CONCOCT)\r\n3. Pairwise comparisons using [turbocor](https://github.com/dcjones/turbocor) followed by clustering\r\n\r\nRead-based annotations\r\n1. [Kraken2](https://ccb.jhu.edu/software/kraken2/)\r\n2. [singlem](https://github.com/wwood/singlem)\r\n3. [SUPER-focus](https://github.com/metageni/SUPER-FOCUS)\r\n4. [FOCUS](https://github.com/metageni/FOCUS)\r\n\r\nWant something else added to the suite? File an issue on github and we'll add it ASAP!\r\n\r\n### Installation\r\n\r\nYou will need to install\r\n1. The NCBI taxonomy database somewhere\r\n2. The superfocus databases somewhere, and set the SUPERFOCUS_DB environmental variable\r\n\r\nEverything else should install automatically.",
        "doi": "10.48546/workflowhub.workflow.241.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "241",
        "keep": "Keep",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/241?version=1",
        "name": "atavide",
        "number_of_steps": 0,
        "projects": [
            "FAME"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [],
        "type": "Snakemake",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-11-10",
        "creators": [],
        "description": "",
        "doi": null,
        "edam_operation": [
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "233",
        "keep": "Keep",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/233?version=1",
        "name": "16S_biodiversity_for_nonoverlap_paired_end",
        "number_of_steps": 25,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metadegalaxy"
        ],
        "tools": [
            "\n addValue",
            "vsearch_chimera_detection",
            "uclust2otutable",
            "phyloseq_abundance",
            "samtools_fastx",
            "vsearch_search",
            "bwa_mem",
            "vsearch_clustering",
            "phyloseq_taxonomy",
            "biom_add_metadata",
            "symmetricPlot",
            "phyloseq_net",
            "\n Cut1",
            "vsearch_dereplication",
            "fastqc",
            "phyloseq_DESeq2",
            "picard_FilterSamReads",
            "phyloseq_richness",
            "cat_multi_datasets",
            "\n cat1",
            "biom_convert",
            "trimmomatic"
        ],
        "type": "Galaxy",
        "update_time": "2024-04-17",
        "versions": 1
    },
    {
        "create_time": "2021-11-10",
        "creators": [],
        "description": "MetaDEGalaxy: Galaxy workflow for differential abundance analysis of 16s metagenomic data",
        "doi": null,
        "edam_operation": [
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "232",
        "keep": "Keep",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/232?version=1",
        "name": "16S_biodiversity_for_overlap_paired_end",
        "number_of_steps": 27,
        "projects": [
            "QCIF Bioinformatics"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metadegalaxy"
        ],
        "tools": [
            "\n addValue",
            "vsearch_chimera_detection",
            "uclust2otutable",
            "phyloseq_abundance",
            "samtools_fastx",
            "vsearch_search",
            "bwa_mem",
            "vsearch_clustering",
            "phyloseq_taxonomy",
            "picard_MergeSamFiles",
            "biom_add_metadata",
            "symmetricPlot",
            "phyloseq_net",
            "\n Cut1",
            "iuc_pear",
            "vsearch_dereplication",
            "fastqc",
            "phyloseq_DESeq2",
            "picard_FilterSamReads",
            "phyloseq_richness",
            "\n cat1",
            "biom_convert",
            "trimmomatic"
        ],
        "type": "Galaxy",
        "update_time": "2024-04-17",
        "versions": 1
    },
    {
        "create_time": "2021-11-08",
        "creators": [
            "Anna Syme"
        ],
        "description": "Assembly polishing; can run alone or as part of a combined workflow for large genome assembly. \r\n\r\n* What it does: Polishes (corrects) an assembly, using long reads (with the tools Racon and Medaka) and short reads (with the tool Racon). (Note: medaka is only for nanopore reads, not PacBio reads). \r\n* Inputs:  assembly to be polished:  assembly.fasta; long reads - the same set used in the assembly (e.g. may be raw or filtered) fastq.gz format; short reads, R1 only, in fastq.gz format\r\n* Outputs: Racon+Medaka+Racon polished_assembly. fasta; Fasta statistics after each polishing tool\r\n* Tools used: Minimap2, Racon, Fasta statistics, Medaka\r\n* Input parameters:  None required, but recommended to set the Medaka model correctly (default = r941_min_high_g360). See drop down list for options. \r\n\r\nWorkflow steps:\r\n\r\n-1-  Polish with long reads: using Racon\r\n* Long reads and assembly contigs => Racon polishing (subworkflow): \r\n* minimap2 : long reads are mapped to assembly => overlaps.paf. \r\n* overaps, long reads, assembly => Racon => polished assembly 1\r\n* using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\r\n* using polished assembly 2 as input, repeat minimap2 + racon => polished assembly 3\r\n* using polished assembly 3 as input, repeat minimap2 + racon => polished assembly 4\r\n* Racon long-read polished assembly => Fasta statistics\r\n* Note: The Racon tool panel can be a bit confusing and is under review for improvement. Presently it requires sequences (= long reads), overlaps (= the paf file created by minimap2), and target sequences (= the contigs to be polished) as per \"usage\" described here https://github.com/isovic/racon/blob/master/README.md\r\n* Note: Racon: the default setting for \"output unpolished target sequences?\" is No. This has been changed to Yes for all Racon steps in these polishing workflows.  This means that even if no polishes are made in some contigs, they will be part of the output fasta file. \r\n* Note: the contigs output by Racon have new tags in their headers. For more on this see https://github.com/isovic/racon/issues/85.\r\n\r\n-2-  Polish with long reads: using Medaka\r\n* Racon polished assembly + long reads => medaka polishing X1 => medaka polished assembly\r\n* Medaka polished assembly => Fasta statistics\r\n\r\n-3-  Polish with short reads: using Racon\r\n* Short reads and Medaka polished assembly =>Racon polish (subworkflow):\r\n* minimap2: short reads (R1 only) are mapped to the assembly => overlaps.paf. Minimap2 setting is for short reads.\r\n* overlaps + short reads + assembly => Racon => polished assembly 1\r\n* using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\r\n* Racon short-read polished assembly => Fasta statistics\r\n\r\nOptions\r\n* Change settings for Racon long read polishing if using PacBio reads:  The default profile setting for Racon long read polishing: minimap2 read mapping is \"Oxford Nanopore read to reference mapping\", which is specified as an input parameter to the whole Assembly polishing workflow, as text: map-ont. If you are not using nanopore reads and/or need a different setting, change this input. To see the other available settings, open the minimap2 tool, find \"Select a profile of preset options\", and click on the drop down menu. For each described option, there is a short text in brackets at the end (e.g. map-pb). This is the text to enter into the assembly polishing workflow at runtime instead of the default (map-ont).\r\n* Other options: change the number of polishes (in Racon and/or Medaka). There are ways to assess how much improvement in assembly quality has occurred per polishing round (for example, the number of corrections made; the change in Busco score - see section Genome quality assessment for more on Busco).\r\n* Option: change polishing settings for any of these tools. Note: for Racon - these will have to be changed within those subworkflows first. Then, in the main workflow, update the subworkflows, and re-save. \r\n\r\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)",
        "doi": "10.48546/workflowhub.workflow.226.1",
        "edam_operation": [
            "Sequence assembly"
        ],
        "edam_topic": [
            "Sequence assembly"
        ],
        "filtered_on": "profil.* in description",
        "id": "226",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/226?version=1",
        "name": "Assembly polishing",
        "number_of_steps": 6,
        "projects": [
            "Galaxy Australia",
            "Australian BioCommons"
        ],
        "source": "WorkflowHub",
        "tags": [
            "large-genome-assembly"
        ],
        "tools": [
            "medaka_consensus_pipeline",
            "\n d5a2cb013d9747c0",
            "\n 01041e6e0464607c",
            "fasta-stats"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-30",
        "versions": 1
    },
    {
        "create_time": "2021-08-11",
        "creators": [
            "Mike Thang"
        ],
        "description": "This is a Galaxy workflow that uses to convert the16S BIOM file to table and figures. It is part of the metaDEGalaxy workflow MetaDEGalaxy: Galaxy workflow for differential abundance analysis of 16s metagenomic data. ",
        "doi": null,
        "edam_operation": [
            "Taxonomic classification"
        ],
        "edam_topic": [
            "Metagenomics"
        ],
        "filtered_on": "edam",
        "id": "142",
        "keep": "Keep",
        "latest_version": 1,
        "license": "GPL-3.0",
        "link": "https:/workflowhub.eu/workflows/142?version=1",
        "name": "16S_biodiversity_BIOM",
        "number_of_steps": 8,
        "projects": [
            "Galaxy Australia"
        ],
        "source": "WorkflowHub",
        "tags": [
            "metadegalaxy"
        ],
        "tools": [
            "biom_convert",
            "phyloseq_DESeq2",
            "phyloseq_abundance",
            "phyloseq_taxonomy",
            "biom_add_metadata",
            "symmetricPlot",
            "phyloseq_richness",
            "phyloseq_net"
        ],
        "type": "Galaxy",
        "update_time": "2024-04-17",
        "versions": 1
    },
    {
        "create_time": "2021-06-17",
        "creators": [],
        "description": "Metagenomic dataset taxonomic classification using kraken2",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "124",
        "keep": "Keep",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/124?version=1",
        "name": "1: Plant virus detection with kraken2 (SE)",
        "number_of_steps": 3,
        "projects": [
            "Integrated and Urban Plant Pathology Laboratory"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virology",
            "kraken"
        ],
        "tools": [
            "taxonomy_krona_chart",
            "kraken2",
            "Kraken2Tax"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2021-02-04",
        "creators": [],
        "description": "Metagenomic dataset taxonomic classification using kraken2",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "101",
        "keep": "Keep",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/101?version=1",
        "name": "1: Plant virus detection with kraken2 (PE)",
        "number_of_steps": 3,
        "projects": [
            "Integrated and Urban Plant Pathology Laboratory"
        ],
        "source": "WorkflowHub",
        "tags": [
            "virology",
            "kraken"
        ],
        "tools": [
            "taxonomy_krona_chart",
            "kraken2",
            "Kraken2Tax"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-10-28",
        "creators": [
            "Jasper Koehorst",
            "Bart Nijsse"
        ],
        "description": "Amplicon analysis workflow using NG-Tax\r\n\r\n**Steps:**\r\n\r\n* Quality control on the reads\r\n* Execute NGTax for ASV detection and classification\r\n\r\nFor more information about NG-Tax 2.0 have a look at https://doi.org/10.3389/fgene.2019.01366",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "16S in tags",
        "id": "45",
        "keep": "To Curate",
        "latest_version": 7,
        "license": "CC0-1.0",
        "link": "https:/workflowhub.eu/workflows/45?version=7",
        "name": "NGTax",
        "number_of_steps": 5,
        "projects": [
            "UNLOCK"
        ],
        "source": "WorkflowHub",
        "tags": [
            "16s",
            "amplicon",
            "its"
        ],
        "tools": [],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-03-21",
        "creators": [],
        "description": "Workflow for tracking objects in Cell Profiler:\r\nhttps://training.galaxyproject.org/training-material/topics/imaging/tutorials/object-tracking-using-cell-profiler/tutorial.html",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [
            "Bioimaging",
            "Imaging"
        ],
        "filtered_on": "profil.* in description",
        "id": "115",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/115?version=1",
        "name": "Object tracking using CellProfiler",
        "number_of_steps": 12,
        "projects": [
            "IBISBA Workflows",
            "Euro-BioImaging"
        ],
        "source": "WorkflowHub",
        "tags": [
            "cellprofiler",
            "galaxy",
            "image processing",
            "imaging"
        ],
        "tools": [
            "cp_color_to_gray",
            "cp_identify_primary_objects",
            "cp_common",
            "cp_measure_object_size_shape",
            "cp_tile",
            "cp_overlay_outlines",
            "cp_export_to_spreadsheet",
            "cp_measure_object_intensity",
            "cp_save_images",
            "cp_cellprofiler",
            "unzip",
            "cp_track_objects"
        ],
        "type": "Galaxy",
        "update_time": "2023-07-03",
        "versions": 1
    },
    {
        "create_time": "2020-06-08",
        "creators": [
            "Martin Beracochea"
        ],
        "description": "<p class=\"has-line-data\" data-line-start=\"0\" data-line-end=\"1\"><img src=\"https://img.shields.io/badge/CWL-1.2.0--dev2-green\" alt=\"\"> <img src=\"https://img.shields.io/badge/nextflow-20.01.0-brightgreen\" alt=\"\"> <img src=\"https://img.shields.io/badge/uses-docker-blue.svg\" alt=\"\"> <img src=\"https://img.shields.io/badge/uses-conda-yellow.svg\" alt=\"\"> <img src=\"https://api.travis-ci.org/EBI-Metagenomics/emg-viral-pipeline.svg\" alt=\"\"></p>\r\n<h1 class=\"code-line\" data-line-start=2 data-line-end=3 ><a id=\"VIRify_2\"></a>VIRify</h1>\r\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><img width=\"500px\" src=\"https://raw.githubusercontent.com/EBI-Metagenomics/emg-viral-pipeline/master/nextflow/figures/sankey.png\" alt=\"Sankey plot\"></p>\r\n<p class=\"has-line-data\" data-line-start=\"6\" data-line-end=\"7\">VIRify is a recently developed pipeline for the detection, annotation, and taxonomic classification of viral contigs in metagenomic and metatranscriptomic assemblies. The pipeline is part of the repertoire of analysis services offered by <a href=\"https://www.ebi.ac.uk/metagenomics/\">MGnify</a>. VIRify\u2019s taxonomic classification relies on the detection of taxon-specific profile hidden Markov models (HMMs), built upon a set of 22,014 orthologous protein domains and referred to as ViPhOGs.</p>\r\n<p class=\"has-line-data\" data-line-start=\"8\" data-line-end=\"9\">VIRify was implemented in CWL.</p>\r\n<h2 class=\"code-line\" data-line-start=10 data-line-end=11 ><a id=\"What_do_I_need_10\"></a>What do I need?</h2>\r\n<p class=\"has-line-data\" data-line-start=\"12\" data-line-end=\"13\">The current implementation uses CWL version 1.2 dev+2. It was tested using Toil version 4.10 as the workflow engine and conda to manage the software dependencies.</p>\r\n<h3 class=\"code-line\" data-line-start=14 data-line-end=15 ><a id=\"Docker__Singularity_support_14\"></a>Docker - Singularity support</h3>\r\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\">Soon\u2026</p>\r\n<h2 class=\"code-line\" data-line-start=18 data-line-end=19 ><a id=\"Setup_environment_18\"></a>Setup environment</h2>\r\n<pre><code class=\"has-line-data\" data-line-start=\"21\" data-line-end=\"24\" class=\"language-bash\">conda env create <span class=\"hljs-operator\">-f</span> cwl/requirements/conda_env.yml\r\nconda activate viral_pipeline\r\n</code></pre>\r\n<h2 class=\"code-line\" data-line-start=25 data-line-end=26 ><a id=\"Basic_execution_25\"></a>Basic execution</h2>\r\n<pre><code class=\"has-line-data\" data-line-start=\"28\" data-line-end=\"31\" class=\"language-bash\"><span class=\"hljs-built_in\">cd</span> cwl/\r\nvirify.sh -h\r\n</code></pre>\r\n<h1 class=\"code-line\" data-line-start=32 data-line-end=33 ><a id=\"A_note_about_metatranscriptomes_32\"></a>A note about metatranscriptomes</h1>\r\n<p class=\"has-line-data\" data-line-start=\"34\" data-line-end=\"36\">Although VIRify has been benchmarked and validated with metagenomic data in mind, it is also possible to use this tool to detect RNA viruses in metatranscriptome assemblies (e.g. SARS-CoV-2). However, some additional considerations for this purpose are outlined below:<br>\r\n<strong>1. Quality control:</strong> As for metagenomic data, a thorough quality control of the FASTQ sequence reads to remove low-quality bases, adapters and host contamination (if appropriate) is required prior to assembly. This is especially important for metatranscriptomes as small errors can further decrease the quality and contiguity of the assembly obtained. We have used <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/\">TrimGalore</a> for this purpose.</p>\r\n<p class=\"has-line-data\" data-line-start=\"37\" data-line-end=\"38\"><strong>2. Assembly:</strong> There are many assemblers available that are appropriate for either metagenomic or single-species transcriptomic data. However, to our knowledge, there is no assembler currently available specifically for metatranscriptomic data. From our preliminary investigations, we have found that transcriptome-specific assemblers (e.g. <a href=\"http://cab.spbu.ru/software/spades/\">rnaSPAdes</a>) generate more contiguous and complete metatranscriptome assemblies compared to metagenomic alternatives (e.g. <a href=\"https://github.com/voutcn/megahit/releases\">MEGAHIT</a> and <a href=\"http://cab.spbu.ru/software/spades/\">metaSPAdes</a>).</p>\r\n<p class=\"has-line-data\" data-line-start=\"39\" data-line-end=\"40\"><strong>3. Post-processing:</strong> Metatranscriptomes generate highly fragmented assemblies. Therefore, filtering contigs based on a set minimum length has a substantial impact in the number of contigs processed in VIRify. It has also been observed that the number of false-positive detections of <a href=\"https://github.com/jessieren/VirFinder/releases\">VirFinder</a> (one of the tools included in VIRify) is lower among larger contigs. The choice of a length threshold will depend on the complexity of the sample and the sequencing technology used, but in our experience any contigs &lt;2 kb should be analysed with caution.</p>\r\n<p class=\"has-line-data\" data-line-start=\"41\" data-line-end=\"42\"><strong>4. Classification:</strong> The classification module of VIRify depends on the presence of a minimum number and proportion of phylogenetically-informative genes within each contig in order to confidently assign a taxonomic lineage. Therefore, short contigs typically obtained from metatranscriptome assemblies remain generally unclassified. For targeted classification of RNA viruses (for instance, to search for Coronavirus-related sequences), alternative DNA- or protein-based classification methods can be used. Two of the possible options are: (i) using <a href=\"https://github.com/marbl/MashMap/releases\">MashMap</a> to screen the VIRify contigs against a database of RNA viruses (e.g. Coronaviridae) or (ii) using <a href=\"http://hmmer.org/download.html\">hmmsearch</a> to screen the proteins obtained in the VIRify contigs against marker genes of the taxon of interest.</p>\r\n<h2>Contact us</h2>\r\n<a href=\"https://www.ebi.ac.uk/support/metagenomics\">MGnify helpdesk</a>",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "27",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/27?version=1",
        "name": "VIRify",
        "number_of_steps": 0,
        "projects": [
            "MGnify",
            "HoloFood at MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-03-24",
        "versions": 1
    },
    {
        "create_time": "2020-06-08",
        "creators": [
            "Martin Beracochea"
        ],
        "description": "<p class=\"has-line-data\" data-line-start=\"0\" data-line-end=\"1\"><img src=\"https://img.shields.io/badge/CWL-1.2.0--dev2-green\" alt=\"\"> <img src=\"https://img.shields.io/badge/nextflow-20.01.0-brightgreen\" alt=\"\"> <img src=\"https://img.shields.io/badge/uses-docker-blue.svg\" alt=\"\"> <img src=\"https://img.shields.io/badge/uses-conda-yellow.svg\" alt=\"\"> <img src=\"https://api.travis-ci.org/EBI-Metagenomics/emg-viral-pipeline.svg\" alt=\"\"></p>\r\n<h1 class=\"code-line\" data-line-start=2 data-line-end=3 ><a id=\"VIRify_2\"></a>VIRify</h1>\r\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><img width=\"500px\" src=\"https://raw.githubusercontent.com/EBI-Metagenomics/emg-viral-pipeline/master/nextflow/figures/sankey.png\" alt=\"Sankey plot\"></p>\r\n<p class=\"has-line-data\" data-line-start=\"6\" data-line-end=\"7\">VIRify is a recently developed pipeline for the detection, annotation, and taxonomic classification of viral contigs in metagenomic and metatranscriptomic assemblies. The pipeline is part of the repertoire of analysis services offered by <a href=\"https://www.ebi.ac.uk/metagenomics/\">MGnify</a>. VIRify\u2019s taxonomic classification relies on the detection of taxon-specific profile hidden Markov models (HMMs), built upon a set of 22,014 orthologous protein domains and referred to as ViPhOGs.</p>\r\n<p class=\"has-line-data\" data-line-start=\"8\" data-line-end=\"9\">VIRify was implemented in CWL.</p>\r\n<h2 class=\"code-line\" data-line-start=10 data-line-end=11 ><a id=\"What_do_I_need_10\"></a>What do I need?</h2>\r\n<p class=\"has-line-data\" data-line-start=\"12\" data-line-end=\"13\">The current implementation uses CWL version 1.2 dev+2. It was tested using Toil version 4.10 as the workflow engine and conda to manage the software dependencies.</p>\r\n<h3 class=\"code-line\" data-line-start=14 data-line-end=15 ><a id=\"Docker__Singularity_support_14\"></a>Docker - Singularity support</h3>\r\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\">Soon\u2026</p>\r\n<h2 class=\"code-line\" data-line-start=18 data-line-end=19 ><a id=\"Setup_environment_18\"></a>Setup environment</h2>\r\n<pre><code class=\"has-line-data\" data-line-start=\"21\" data-line-end=\"24\" class=\"language-bash\">conda env create <span class=\"hljs-operator\">-f</span> cwl/requirements/conda_env.yml\r\nconda activate viral_pipeline\r\n</code></pre>\r\n<h2 class=\"code-line\" data-line-start=25 data-line-end=26 ><a id=\"Basic_execution_25\"></a>Basic execution</h2>\r\n<pre><code class=\"has-line-data\" data-line-start=\"28\" data-line-end=\"31\" class=\"language-bash\"><span class=\"hljs-built_in\">cd</span> cwl/\r\nvirify.sh -h\r\n</code></pre>\r\n<h1 class=\"code-line\" data-line-start=32 data-line-end=33 ><a id=\"A_note_about_metatranscriptomes_32\"></a>A note about metatranscriptomes</h1>\r\n<p class=\"has-line-data\" data-line-start=\"34\" data-line-end=\"36\">Although VIRify has been benchmarked and validated with metagenomic data in mind, it is also possible to use this tool to detect RNA viruses in metatranscriptome assemblies (e.g. SARS-CoV-2). However, some additional considerations for this purpose are outlined below:<br>\r\n<strong>1. Quality control:</strong> As for metagenomic data, a thorough quality control of the FASTQ sequence reads to remove low-quality bases, adapters and host contamination (if appropriate) is required prior to assembly. This is especially important for metatranscriptomes as small errors can further decrease the quality and contiguity of the assembly obtained. We have used <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/\">TrimGalore</a> for this purpose.</p>\r\n<p class=\"has-line-data\" data-line-start=\"37\" data-line-end=\"38\"><strong>2. Assembly:</strong> There are many assemblers available that are appropriate for either metagenomic or single-species transcriptomic data. However, to our knowledge, there is no assembler currently available specifically for metatranscriptomic data. From our preliminary investigations, we have found that transcriptome-specific assemblers (e.g. <a href=\"http://cab.spbu.ru/software/spades/\">rnaSPAdes</a>) generate more contiguous and complete metatranscriptome assemblies compared to metagenomic alternatives (e.g. <a href=\"https://github.com/voutcn/megahit/releases\">MEGAHIT</a> and <a href=\"http://cab.spbu.ru/software/spades/\">metaSPAdes</a>).</p>\r\n<p class=\"has-line-data\" data-line-start=\"39\" data-line-end=\"40\"><strong>3. Post-processing:</strong> Metatranscriptomes generate highly fragmented assemblies. Therefore, filtering contigs based on a set minimum length has a substantial impact in the number of contigs processed in VIRify. It has also been observed that the number of false-positive detections of <a href=\"https://github.com/jessieren/VirFinder/releases\">VirFinder</a> (one of the tools included in VIRify) is lower among larger contigs. The choice of a length threshold will depend on the complexity of the sample and the sequencing technology used, but in our experience any contigs &lt;2 kb should be analysed with caution.</p>\r\n<p class=\"has-line-data\" data-line-start=\"41\" data-line-end=\"42\"><strong>4. Classification:</strong> The classification module of VIRify depends on the presence of a minimum number and proportion of phylogenetically-informative genes within each contig in order to confidently assign a taxonomic lineage. Therefore, short contigs typically obtained from metatranscriptome assemblies remain generally unclassified. For targeted classification of RNA viruses (for instance, to search for Coronavirus-related sequences), alternative DNA- or protein-based classification methods can be used. Two of the possible options are: (i) using <a href=\"https://github.com/marbl/MashMap/releases\">MashMap</a> to screen the VIRify contigs against a database of RNA viruses (e.g. Coronaviridae) or (ii) using <a href=\"http://hmmer.org/download.html\">hmmsearch</a> to screen the proteins obtained in the VIRify contigs against marker genes of the taxon of interest.</p>\r\n<h2>Contact us</h2>\r\n<a href=\"https://www.ebi.ac.uk/support/metagenomics\">MGnify helpdesk</a>",
        "doi": "10.48546/workflowhub.workflow.26.1",
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "26",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/26?version=1",
        "name": "VIRify",
        "number_of_steps": 17,
        "projects": [
            "MGnify"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [
            "krona plots",
            "ViPhOG annotations",
            "Default lenght 1kb https://github.com/EBI-Metagenomics/emg-virify-scripts/issues/6",
            "hmmscan",
            "MashMap",
            "PPR-Meta",
            "Restore fasta names",
            "Blast in a database of viral sequences including metagenomes",
            "Prodigal",
            "VirFinder",
            "Filter contigs",
            "Combine",
            "ratio evalue ViPhOG",
            "VirSorter",
            "Taxonomic assign"
        ],
        "type": "Common Workflow Language",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2021-02-02",
        "creators": [],
        "description": "Galaxy version of pre-processing of reads from COVID-19 samples. \r\nQC + human read cleaning\r\nBased on https://github.com/Finn-Lab/Metagen-FastQC/blob/master/metagen-fastqc.sh",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "99",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/99?version=1",
        "name": "COVID-19: read pre-processing",
        "number_of_steps": 4,
        "projects": [
            "IBISBA Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [],
        "tools": [
            "samtool_filter2",
            "trim_galore",
            "bwa_mem",
            "samtools_fastx"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-11-04",
        "creators": [],
        "description": "This WF is based on the official Covid19-Galaxy assembly workflow as available from https://covid19.galaxyproject.org/genomics/2-assembly/ . It has been adapted to suit the needs of the analysis of metagenomics sequencing data. Prior to be submitted to INDSC databases, these data need to be cleaned from contaminant reads, including reads of possible human origin. \r\n\r\nThe assembly of the SARS-CoV-2 genome is performed using both the Unicycler and the SPAdes assemblers, similar to the original WV.\r\n\r\nTo facilitate the deposition of raw sequencing reads in INDSC databases, different fastq files are saved during the different steps of the WV. Which reflect different levels of stringency/filtration:\r\n\r\n(1) Initially fastq are filtered to remove human reads. \r\n(2) Subsequently, a similarity search is performed against the reference assembly of the SARS-CoV-2 genome, to retain only SARS-CoV-2 like reads. \r\n(3) Finally, SARS-CoV-2 reads are assembled, and the bowtie2 program is used to identify (and save in the corresponding fastq files) only reads that are completely identical to the final assembly of the genome.\r\n\r\nAny of the fastq files produced in (1), (2) or (3) are suitable for being submitted in  raw reads repositories. While the files filtered according to (1) are richer and contain more data, including for example genomic sequences of different microbes living in the oral cavity; files filtered according to (3) contain only the reads that are completely identical to the final assembly. This should guarantee that any re-analysis/re-assembly of these always produce consistent and identical results. File obtained at (2) include all the reads in the sequencing reaction that had some degree of similarity with the reference SARS-CoV-2 genome, these may include subgenomic RNAs, but also polymorphic regions/variants in the case of a coinfection by multiple SARS-CoV-2 strains. Consequently, reanalysis of these data is not guarateed to produce identical and consistent results, depending on the parameters used during the assembly. However, these data contain more information.\r\n\r\nPlease feel free to comment,  ask questions and/or add suggestions\r\n\r\n",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "68",
        "keep": "Reject",
        "latest_version": 1,
        "license": "Apache-2.0",
        "link": "https:/workflowhub.eu/workflows/68?version=1",
        "name": "MC_COVID19like_Assembly_Reads",
        "number_of_steps": 7,
        "projects": [
            "Italy-Covid-data-Portal"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [
            "bowtie2",
            "unicycler",
            "spades",
            "fastp"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-08-05",
        "creators": [
            "Milad Miladi"
        ],
        "description": "Metagenomics: taxa classification",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in name",
        "id": "53",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/53?version=1",
        "name": "ONT -- Metagenomics-Kraken2-Krona",
        "number_of_steps": 4,
        "projects": [
            "NanoGalaxy"
        ],
        "source": "WorkflowHub",
        "tags": [
            "ont"
        ],
        "tools": [
            "taxonomy_krona_chart",
            "tp_replace_in_line",
            "kraken2",
            "datamash_reverse"
        ],
        "type": "Galaxy",
        "update_time": "2023-02-13",
        "versions": 1
    },
    {
        "create_time": "2020-05-14",
        "creators": [
            "Andreas Wilm and October SESSIONS and Paola Florez DE SESSIONS and ZHU Yuan and Shuzhen SIM and CHU Wenhan Collins"
        ],
        "description": "<!DOCTYPE html><html><head><meta charset=\"utf-8\"><style></style></head><body id=\"preview\">\r\n<h1 class=\"code-line\" data-line-start=0 data-line-end=1><a id=\"nfcoreviprhttpsrawgithubusercontentcomnfcoreviprmasterdocsimagesvipr_logosvg_0\"></a><img src=\"https://raw.githubusercontent.com/nf-core/vipr/master/docs/images/vipr_logo.png\" alt=\"nf-core/vipr\" height=\"200\" width=\"500\"></h1>\r\n<p class=\"has-line-data\" data-line-start=\"2\" data-line-end=\"3\"><a href=\"https://travis-ci.org/nf-core/vipr\"><img src=\"https://travis-ci.org/nf-core/vipr.svg?branch=master\" alt=\"Build Status\"></a> <a href=\"https://www.nextflow.io/\"><img src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.31.1-brightgreen.svg\" alt=\"Nextflow\"></a> <a href=\"https://gitter.im/nf-core/Lobby\"><img src=\"https://img.shields.io/badge/gitter-%20join%20chat%20%E2%86%92-4fb99a.svg\" alt=\"Gitter\"></a></p>\r\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><a href=\"http://bioconda.github.io/\"><img src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" alt=\"install with bioconda\"></a> <a href=\"https://hub.docker.com/r/nfcore/vipr/\"><img src=\"https://img.shields.io/docker/automated/nfcore/vipr.svg\" alt=\"Docker Container available\"></a> <a href=\"https://singularity-hub.org/collections/1405\"><img src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"></a></p>\r\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\"><strong>nf-core/vipr</strong> is a bioinformatics best-practice analysis pipeline for assembly and intrahost / low-frequency variant calling for viral samples.</p>\r\n<p class=\"has-line-data\" data-line-start=\"18\" data-line-end=\"19\">The pipeline is built using <a href=\"https://www.nextflow.io\">Nextflow</a>, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.</p>\r\n<h3 class=\"code-line\" data-line-start=20 data-line-end=21><a id=\"Pipeline_Steps_20\"></a>Pipeline Steps</h3>\r\n<table class=\"table table-striped table-bordered\">\r\n<thead>\r\n<tr>\r\n<th>Step</th>\r\n<th>Main program/s</th>\r\n</tr>\r\n</thead>\r\n<tbody>\r\n<tr>\r\n<td>Trimming, combining of read-pairs per sample and QC</td>\r\n<td>Skewer, FastQC</td>\r\n</tr>\r\n<tr>\r\n<td>Decontamination</td>\r\n<td>decont</td>\r\n</tr>\r\n<tr>\r\n<td>Metagenomics classification / Sample purity</td>\r\n<td>Kraken</td>\r\n</tr>\r\n<tr>\r\n<td>Assembly to contigs</td>\r\n<td>BBtools\u2019 Tadpole</td>\r\n</tr>\r\n<tr>\r\n<td>Assembly polishing</td>\r\n<td>ViPR Tools</td>\r\n</tr>\r\n<tr>\r\n<td>Mapping to assembly</td>\r\n<td>BWA, LoFreq</td>\r\n</tr>\r\n<tr>\r\n<td>Low frequency variant calling</td>\r\n<td>LoFreq</td>\r\n</tr>\r\n<tr>\r\n<td>Coverage and variant AF plots (two processes)</td>\r\n<td>Bedtools, ViPR Tools</td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n<h3 class=\"code-line\" data-line-start=33 data-line-end=34><a id=\"Documentation_33\"></a>Documentation</h3>\r\n<p class=\"has-line-data\" data-line-start=\"35\" data-line-end=\"36\">Documentation about the pipeline can be found in the <code>docs/</code> directory:</p>\r\n<ol>\r\n<li class=\"has-line-data\" data-line-start=\"37\" data-line-end=\"38\"><a href=\"docs/installation.md\">Installation and configuration</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"38\" data-line-end=\"39\"><a href=\"docs/usage.md\">Running the pipeline</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"39\" data-line-end=\"41\"><a href=\"docs/output.md\">Output and how to interpret the results</a></li>\r\n</ol>\r\n<h3 class=\"code-line\" data-line-start=41 data-line-end=42><a id=\"Credits_41\"></a>Credits</h3>\r\n<p class=\"has-line-data\" data-line-start=\"43\" data-line-end=\"46\">This pipeline was originally developed by Andreas Wilm (<a href=\"https://github.com/andreas-wilm\">andreas-wilm</a>) at <a href=\"https://www.a-star.edu.sg/gis/\">Genome Institute of Singapore</a>.<br>\r\nIt started out as an ecosystem around LoFreq and went through a couple of iterations.<br>\r\nThe current version had three predecessors <a href=\"https://github.com/CSB5/vipr\">ViPR 1</a>, <a href=\"https://github.com/CSB5/vipr2\">ViPR 2</a> and <a href=\"https://github.com/gis-rpd/pipelines/tree/master/germs/vipr\">ViPR 3</a>.</p>\r\n<p class=\"has-line-data\" data-line-start=\"47\" data-line-end=\"48\">An incomplete list of publications using (previous versions of) ViPR:</p>\r\n<ul>\r\n<li class=\"has-line-data\" data-line-start=\"49\" data-line-end=\"50\"><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/26327586\">Sessions et. al., PLoS Negl Trop Dis., 2015</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"50\" data-line-end=\"52\"><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/26325059\">Sim et al., PLoS Negl Trop Dis., 2015</a></li>\r\n</ul>\r\n<p class=\"has-line-data\" data-line-start=\"52\" data-line-end=\"53\">Plenty of people provided essential feedback, including:</p>\r\n<ul>\r\n<li class=\"has-line-data\" data-line-start=\"54\" data-line-end=\"55\"><a href=\"https://www.duke-nus.edu.sg/content/sessions-october\">October SESSIONS</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"55\" data-line-end=\"56\"><a href=\"https://www.a-star.edu.sg/gis/Our-People/Platform-Leaders\">Paola Florez DE SESSIONS</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"56\" data-line-end=\"57\">ZHU Yuan</li>\r\n<li class=\"has-line-data\" data-line-start=\"57\" data-line-end=\"58\">Shuzhen SIM</li>\r\n<li class=\"has-line-data\" data-line-start=\"58\" data-line-end=\"59\">CHU Wenhan Collins</li>\r\n</ul>\r\n</body></html>",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "20",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/20?version=1",
        "name": "nf-core/vipr",
        "number_of_steps": 0,
        "projects": [
            "nf-core viralrecon"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-05-14",
        "creators": [
            "Sarai Varona and Miguel Juli\u00e1 and Sara Monzon and Alexander Peltzer and Alison Meynert and Edgar Garriga Nogales and Erik Garrison and Gisela Gabernet and Harshil Patel and Joao Curado and Jose Espinosa-Carrasco and Katrin Sameith and Marta Pozuelo and Maxime Garcia and Michael Heuer and Phil Ewels and Simon Heumos and Stephen Kelly and Thanh Le Viet and Isabel Cuesta"
        ],
        "description": "<!DOCTYPE html><html><head><meta charset=\"utf-8\"><style></style></head><body id=\"preview\">\r\n<h1 class=\"code-line\" data-line-start=0 data-line-end=1><a id=\"nfcoreviralreconhttpsrawgithubusercontentcomnfcoreviralreconmasterdocsimagesnfcoreviralrecon_logopng_0\"></a><img src=\"https://raw.githubusercontent.com/nf-core/viralrecon/master/docs/images/nf-core-viralrecon_logo.png\" alt=\"nf-core/viralrecon\"></h1>\r\n<p class=\"has-line-data\" data-line-start=\"2\" data-line-end=\"3\"><a href=\"https://github.com/nf-core/viralrecon/actions\"><img src=\"https://github.com/nf-core/viralrecon/workflows/nf-core%20CI/badge.svg\" alt=\"GitHub Actions CI Status\"></a> <a href=\"https://github.com/nf-core/viralrecon/actions\"><img src=\"https://github.com/nf-core/viralrecon/workflows/nf-core%20linting/badge.svg\" alt=\"GitHub Actions Linting Status\"></a> <a href=\"https://www.nextflow.io/\"><img src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\" alt=\"Nextflow\"></a> <a href=\"https://bioconda.github.io/\"><img src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" alt=\"install with bioconda\"></a></p>\r\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><a href=\"https://hub.docker.com/r/nfcore/viralrecon\"><img src=\"https://img.shields.io/docker/automated/nfcore/viralrecon.svg\" alt=\"Docker\"></a> <a href=\"https://doi.org/10.5281/zenodo.3872730\"><img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3872730.svg\" alt=\"DOI\"></a></p>\r\n<p class=\"has-line-data\" data-line-start=\"8\" data-line-end=\"9\"><strong>nfcore/viralrecon</strong> is a bioinformatics analysis pipeline used to perform assembly and intrahost/low-frequency variant calling for viral samples. The pipeline currently supports metagenomics and amplicon sequencing data derived from the Illumina sequencing platform.</p>\r\n<p class=\"has-line-data\" data-line-start=\"10\" data-line-end=\"11\">This pipeline is a re-implementation of the <a href=\"https://github.com/BU-ISCIII/SARS_Cov2_consensus-nf\">SARS_Cov2_consensus-nf</a> and <a href=\"https://github.com/BU-ISCIII/SARS_Cov2_assembly-nf\">SARS_Cov2_assembly-nf</a> pipelines initially developed by <a href=\"https://github.com/svarona\">Sarai Varona</a> and <a href=\"https://github.com/saramonzon\">Sara Monzon</a> from <a href=\"https://github.com/BU-ISCIII\">BU-ISCIII</a>. Porting both of these pipelines to nf-core was an international collaboration between numerous contributors and developers, led by <a href=\"https://github.com/drpatelh\">Harshil Patel</a> from the <a href=\"https://www.crick.ac.uk/research/science-technology-platforms/bioinformatics-and-biostatistics/\">The Bioinformatics &amp; Biostatistics Group</a> at <a href=\"https://www.crick.ac.uk/\">The Francis Crick Institute</a>, London. We appreciated the need to have a portable, reproducible and scalable pipeline for the analysis of COVID-19 sequencing samples and so the Avengers Assembled! Please come and join us and add yourself to the contributor list :)</p>\r\n<p class=\"has-line-data\" data-line-start=\"12\" data-line-end=\"13\">We have integrated a number of options in the pipeline to allow you to run specific aspects of the workflow if you so wish. For example, you can skip all of the assembly steps with the <code>--skip_assembly</code> parameter. See <a href=\"docs/usage.md\">usage docs</a> for all of the available options when running the pipeline.</p>\r\n<p class=\"has-line-data\" data-line-start=\"14\" data-line-end=\"15\">Please click <a href=\"https://raw.githack.com/nf-core/viralrecon/master/docs/html/multiqc_report.html\">here</a> to see an example MultiQC report generated using the parameters defined in <a href=\"https://github.com/nf-core/viralrecon/blob/master/conf/test_full.config\">this configuration file</a> to run the pipeline on <a href=\"https://zenodo.org/record/3735111\">samples</a> which were prepared from the <a href=\"https://artic.network/ncov-2019\">ncov-2019 ARTIC Network V1 amplicon set</a> and sequenced on the Illumina MiSeq platform in 301bp paired-end format.</p>\r\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\">The pipeline is built using <a href=\"https://www.nextflow.io\">Nextflow</a>, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible. Furthermore, automated continuous integration tests to run the pipeline on a full-sized dataset are passing on AWS cloud.</p>\r\n<h2 class=\"code-line\" data-line-start=18 data-line-end=19><a id=\"Pipeline_summary_18\"></a>Pipeline summary</h2>\r\n<ol>\r\n<li class=\"has-line-data\" data-line-start=\"20\" data-line-end=\"21\">Download samples via SRA, ENA or GEO ids (<a href=\"https://ena-docs.readthedocs.io/en/latest/retrieval/file-download.html\"><code>ENA FTP</code></a>, <a href=\"https://github.com/rvalieris/parallel-fastq-dump\"><code>parallel-fastq-dump</code></a>; <em>if required</em>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"21\" data-line-end=\"22\">Merge re-sequenced FastQ files (<a href=\"http://www.linfo.org/cat.html\"><code>cat</code></a>; <em>if required</em>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"22\" data-line-end=\"23\">Read QC (<a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\"><code>FastQC</code></a>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"23\" data-line-end=\"24\">Adapter trimming (<a href=\"https://github.com/OpenGene/fastp\"><code>fastp</code></a>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"24\" data-line-end=\"33\">Variant calling<br>\r\ni. Read alignment (<a href=\"http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\"><code>Bowtie 2</code></a>)<br>\r\nii. Sort and index alignments (<a href=\"https://sourceforge.net/projects/samtools/files/samtools/\"><code>SAMtools</code></a>)<br>\r\niii. Primer sequence removal (<a href=\"https://github.com/andersen-lab/ivar\"><code>iVar</code></a>; <em>amplicon data only</em>)<br>\r\niv. Duplicate read marking (<a href=\"https://broadinstitute.github.io/picard/\"><code>picard</code></a>; <em>removal optional</em>)<br>\r\nv. Alignment-level QC (<a href=\"https://broadinstitute.github.io/picard/\"><code>picard</code></a>, <a href=\"https://sourceforge.net/projects/samtools/files/samtools/\"><code>SAMtools</code></a>)<br>\r\nvi. Choice of multiple variant calling and consensus sequence generation routes (<a href=\"https://dkoboldt.github.io/varscan/\"><code>VarScan 2</code></a>, <a href=\"https://samtools.github.io/bcftools/bcftools.html\"><code>BCFTools</code></a>, <a href=\"https://github.com/arq5x/bedtools2/\"><code>BEDTools</code></a> <em>||</em> <a href=\"https://github.com/andersen-lab/ivar\"><code>iVar variants and consensus</code></a> <em>||</em> <a href=\"https://samtools.github.io/bcftools/bcftools.html\"><code>BCFTools</code></a>, <a href=\"https://github.com/arq5x/bedtools2/\"><code>BEDTools</code></a>)<br>\r\n- Variant annotation (<a href=\"http://snpeff.sourceforge.net/SnpEff.html\"><code>SnpEff</code></a>, <a href=\"http://snpeff.sourceforge.net/SnpSift.html\"><code>SnpSift</code></a>)<br>\r\n- Consensus assessment report (<a href=\"http://quast.sourceforge.net/quast\"><code>QUAST</code></a>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"33\" data-line-end=\"43\"><em>De novo</em> assembly<br>\r\ni. Primer trimming (<a href=\"https://cutadapt.readthedocs.io/en/stable/guide.html\"><code>Cutadapt</code></a>; <em>amplicon data only</em>)<br>\r\nii. Removal of host reads (<a href=\"http://ccb.jhu.edu/software/kraken2/\"><code>Kraken 2</code></a>)<br>\r\niii. Choice of multiple assembly tools (<a href=\"http://cab.spbu.ru/software/spades/\"><code>SPAdes</code></a> <em>||</em> <a href=\"http://cab.spbu.ru/software/meta-spades/\"><code>metaSPAdes</code></a> <em>||</em> <a href=\"https://github.com/rrwick/Unicycler\"><code>Unicycler</code></a> <em>||</em> <a href=\"https://github.com/GATB/minia\"><code>minia</code></a>)<br>\r\n- Blast to reference genome (<a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastSearch\"><code>blastn</code></a>)<br>\r\n- Contiguate assembly (<a href=\"https://www.sanger.ac.uk/science/tools/pagit\"><code>ABACAS</code></a>)<br>\r\n- Assembly report (<a href=\"https://github.com/BU-ISCIII/plasmidID\"><code>PlasmidID</code></a>)<br>\r\n- Assembly assessment report (<a href=\"http://quast.sourceforge.net/quast\"><code>QUAST</code></a>)<br>\r\n- Call variants relative to reference (<a href=\"https://github.com/lh3/minimap2\"><code>Minimap2</code></a>, <a href=\"https://github.com/ekg/seqwish\"><code>seqwish</code></a>, <a href=\"https://github.com/vgteam/vg\"><code>vg</code></a>, <a href=\"https://github.com/rrwick/Bandage\"><code>Bandage</code></a>)<br>\r\n- Variant annotation (<a href=\"http://snpeff.sourceforge.net/SnpEff.html\"><code>SnpEff</code></a>, <a href=\"http://snpeff.sourceforge.net/SnpSift.html\"><code>SnpSift</code></a>)</li>\r\n<li class=\"has-line-data\" data-line-start=\"43\" data-line-end=\"45\">Present QC and visualisation for raw read, alignment, assembly and variant calling results (<a href=\"http://multiqc.info/\"><code>MultiQC</code></a>)</li>\r\n</ol>\r\n<h2 class=\"code-line\" data-line-start=45 data-line-end=46><a id=\"Quick_Start_45\"></a>Quick Start</h2>\r\n<p class=\"has-line-data\" data-line-start=\"47\" data-line-end=\"48\">i. Install <a href=\"https://nf-co.re/usage/installation\"><code>nextflow</code></a></p>\r\n<p class=\"has-line-data\" data-line-start=\"49\" data-line-end=\"50\">ii. Install either <a href=\"https://docs.docker.com/engine/installation/\"><code>Docker</code></a> or <a href=\"https://www.sylabs.io/guides/3.0/user-guide/\"><code>Singularity</code></a> for full pipeline reproducibility (please only use <a href=\"https://conda.io/miniconda.html\"><code>Conda</code></a> as a last resort; see <a href=\"https://nf-co.re/usage/configuration#basic-configuration-profiles\">docs</a>)</p>\r\n<p class=\"has-line-data\" data-line-start=\"51\" data-line-end=\"52\">iii. Download the pipeline and test it on a minimal dataset with a single command</p>\r\n<pre><code class=\"has-line-data\" data-line-start=\"54\" data-line-end=\"56\" class=\"language-bash\">nextflow run nf-core/viralrecon -profile <span class=\"hljs-built_in\">test</span>,&lt;docker/singularity/conda/institute&gt;\r\n</code></pre>\r\n<blockquote>\r\n<p class=\"has-line-data\" data-line-start=\"57\" data-line-end=\"58\">Please check <a href=\"https://github.com/nf-core/configs#documentation\">nf-core/configs</a> to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use <code>-profile &lt;institute&gt;</code> in your command. This will enable either <code>docker</code> or <code>singularity</code> and set the appropriate execution settings for your local compute environment.</p>\r\n</blockquote>\r\n<p class=\"has-line-data\" data-line-start=\"59\" data-line-end=\"60\">iv. Start running your own analysis!</p>\r\n<pre><code class=\"has-line-data\" data-line-start=\"62\" data-line-end=\"64\" class=\"language-bash\">nextflow run nf-core/viralrecon -profile &lt;docker/singularity/conda/institute&gt; --input samplesheet.csv --genome <span class=\"hljs-string\">'NC_045512.2'</span> -profile docker\r\n</code></pre>\r\n<p class=\"has-line-data\" data-line-start=\"65\" data-line-end=\"66\">See <a href=\"docs/usage.md\">usage docs</a> for all of the available options when running the pipeline.</p>\r\n<h2 class=\"code-line\" data-line-start=67 data-line-end=68><a id=\"Documentation_67\"></a>Documentation</h2>\r\n<p class=\"has-line-data\" data-line-start=\"69\" data-line-end=\"70\">The nf-core/viralrecon pipeline comes with documentation about the pipeline, found in the <code>docs/</code> directory:</p>\r\n<ol>\r\n<li class=\"has-line-data\" data-line-start=\"71\" data-line-end=\"72\"><a href=\"https://nf-co.re/usage/installation\">Installation</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"72\" data-line-end=\"76\">Pipeline configuration\r\n<ul>\r\n<li class=\"has-line-data\" data-line-start=\"73\" data-line-end=\"74\"><a href=\"https://nf-co.re/usage/local_installation\">Local installation</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"74\" data-line-end=\"75\"><a href=\"https://nf-co.re/usage/adding_own_config\">Adding your own system config</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"75\" data-line-end=\"76\"><a href=\"docs/usage.md#reference-genomes\">Reference genomes</a></li>\r\n</ul>\r\n</li>\r\n<li class=\"has-line-data\" data-line-start=\"76\" data-line-end=\"77\"><a href=\"docs/usage.md\">Running the pipeline</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"77\" data-line-end=\"78\"><a href=\"docs/output.md\">Output and how to interpret the results</a></li>\r\n<li class=\"has-line-data\" data-line-start=\"78\" data-line-end=\"80\"><a href=\"https://nf-co.re/usage/troubleshooting\">Troubleshooting</a></li>\r\n</ol>\r\n<h2 class=\"code-line\" data-line-start=80 data-line-end=81><a id=\"Credits_80\"></a>Credits</h2>\r\n<p class=\"has-line-data\" data-line-start=\"82\" data-line-end=\"83\">These scripts were originally written by <a href=\"https://github.com/svarona\">Sarai Varona</a>, <a href=\"https://github.com/MiguelJulia\">Miguel Juli\u00e1</a> and <a href=\"https://github.com/saramonzon\">Sara Monzon</a> from <a href=\"https://github.com/BU-ISCIII\">BU-ISCIII</a> and co-ordinated by Isabel Cuesta for the <a href=\"https://eng.isciii.es/eng.isciii.es/Paginas/Inicio.html\">Institute of Health Carlos III</a>, Spain. Through collaboration with the nf-core community the pipeline has now been updated substantially to include additional processing steps, to standardise inputs/outputs and to improve pipeline reporting; implemented primarily by <a href=\"https://github.com/drpatelh\">Harshil Patel</a> from <a href=\"https://www.crick.ac.uk/research/science-technology-platforms/bioinformatics-and-biostatistics/\">The Bioinformatics &amp; Biostatistics Group</a> at <a href=\"https://www.crick.ac.uk/\">The Francis Crick Institute</a>, London.</p>\r\n<p class=\"has-line-data\" data-line-start=\"84\" data-line-end=\"85\">Many thanks to others who have helped out and contributed along the way too, including (but not limited to):</p>\r\n<table class=\"table table-striped table-bordered\">\r\n<thead>\r\n<tr>\r\n<th>Name</th>\r\n<th>Affiliation</th>\r\n</tr>\r\n</thead>\r\n<tbody>\r\n<tr>\r\n<td><a href=\"https://github.com/apeltzer\">Alexander Peltzer</a></td>\r\n<td><a href=\"https://www.boehringer-ingelheim.de/\">Boehringer Ingelheim, Germany</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/ameynert\">Alison Meynert</a></td>\r\n<td><a href=\"https://www.ed.ac.uk/\">University of Edinburgh, Scotland</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/edgano\">Edgar Garriga Nogales</a></td>\r\n<td><a href=\"https://www.crg.eu/\">Centre for Genomic Regulation, Spain</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/ekg\">Erik Garrison</a></td>\r\n<td><a href=\"https://www.ucsc.edu/\">UCSC, USA</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/ggabernet\">Gisela Gabernet</a></td>\r\n<td><a href=\"https://portal.qbic.uni-tuebingen.de/portal/\">QBiC, University of T\u00fcbingen, Germany</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/jcurado-flomics\">Joao Curado</a></td>\r\n<td><a href=\"https://www.flomics.com/\">Flomics Biotech, Spain</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/JoseEspinosa\">Jose Espinosa-Carrasco</a></td>\r\n<td><a href=\"https://www.crg.eu/\">Centre for Genomic Regulation, Spain</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/ktrns\">Katrin Sameith</a></td>\r\n<td><a href=\"https://genomecenter.tu-dresden.de\">DRESDEN-concept Genome Center, Germany</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/lcabus-flomics\">Lluc Cabus</a></td>\r\n<td><a href=\"https://www.flomics.com/\">Flomics Biotech, Spain</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/mpozuelo-flomics\">Marta Pozuelo</a></td>\r\n<td><a href=\"https://www.flomics.com/\">Flomics Biotech, Spain</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/MaxUlysse\">Maxime Garcia</a></td>\r\n<td><a href=\"https://www.scilifelab.se/\">SciLifeLab, Sweden</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/heuermh\">Michael Heuer</a></td>\r\n<td><a href=\"https://https://rise.cs.berkeley.edu\">UC Berkeley, USA</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/ewels\">Phil Ewels</a></td>\r\n<td><a href=\"https://www.scilifelab.se/\">SciLifeLab, Sweden</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/subwaystation\">Simon Heumos</a></td>\r\n<td><a href=\"https://portal.qbic.uni-tuebingen.de/portal/\">QBiC, University of T\u00fcbingen, Germany</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/stevekm\">Stephen Kelly</a></td>\r\n<td><a href=\"https://www.mskcc.org/\">Memorial Sloan Kettering Cancer Center, USA</a></td>\r\n</tr>\r\n<tr>\r\n<td><a href=\"https://github.com/thanhleviet\">Thanh Le Viet</a></td>\r\n<td><a href=\"https://quadram.ac.uk/\">Quadram Institute, UK</a></td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n<blockquote>\r\n<p class=\"has-line-data\" data-line-start=\"105\" data-line-end=\"106\">Listed in alphabetical order</p>\r\n</blockquote>\r\n<h2 class=\"code-line\" data-line-start=107 data-line-end=108><a id=\"Contributions_and_Support_107\"></a>Contributions and Support</h2>\r\n<p class=\"has-line-data\" data-line-start=\"109\" data-line-end=\"110\">If you would like to contribute to this pipeline, please see the <a href=\"https://github.com/nf-core/viralrecon/blob/master/.github/CONTRIBUTING.md\">contributing guidelines</a>.</p>\r\n<p class=\"has-line-data\" data-line-start=\"111\" data-line-end=\"112\">For further information or help, don\u2019t hesitate to get in touch on <a href=\"https://nfcore.slack.com/channels/viralrecon\">Slack</a> (you can join with <a href=\"https://nf-co.re/join/slack\">this invite</a>).</p>\r\n<h2 class=\"code-line\" data-line-start=113 data-line-end=114><a id=\"Citation_113\"></a>Citation</h2>\r\n<p class=\"has-line-data\" data-line-start=\"115\" data-line-end=\"116\">If you use nf-core/viralrecon for your analysis, please cite it using the following doi: <a href=\"https://doi.org/10.5281/zenodo.3872730\">10.5281/zenodo.3872730</a></p>\r\n<p class=\"has-line-data\" data-line-start=\"117\" data-line-end=\"118\">An extensive list of references for the tools used by the pipeline can be found in the <a href=\"https://github.com/nf-core/viralrecon/blob/master/CITATIONS.md\"><code>CITATIONS.md</code></a> file.</p>\r\n<p class=\"has-line-data\" data-line-start=\"119\" data-line-end=\"120\">You can cite the <code>nf-core</code> publication as follows:</p>\r\n<blockquote>\r\n<p class=\"has-line-data\" data-line-start=\"121\" data-line-end=\"122\"><strong>The nf-core framework for community-curated bioinformatics pipelines.</strong></p>\r\n<p class=\"has-line-data\" data-line-start=\"123\" data-line-end=\"124\">Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso &amp; Sven Nahnsen.</p>\r\n<p class=\"has-line-data\" data-line-start=\"125\" data-line-end=\"127\"><em>Nat Biotechnol.</em> 2020 Feb 13. doi: <a href=\"https://dx.doi.org/10.1038/s41587-020-0439-x\">10.1038/s41587-020-0439-x</a>.<br>\r\nReadCube: <a href=\"https://rdcu.be/b1GjZ\">Full Access Link</a></p>\r\n</blockquote>\r\n</body></html>",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "metage.* in description",
        "id": "19",
        "keep": "To Curate",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/19?version=1",
        "name": "nf-core/viralrecon",
        "number_of_steps": 0,
        "projects": [
            "nf-core viralrecon"
        ],
        "source": "WorkflowHub",
        "tags": [
            "covid-19"
        ],
        "tools": [],
        "type": "Nextflow",
        "update_time": "2023-01-16",
        "versions": 1
    },
    {
        "create_time": "2020-05-29",
        "creators": [
            "Melchior du Lac"
        ],
        "description": "This workflow converts the top-ranking predicted pathways from the \"RetroSynthesis\" and \"Pathway Analysis\" workflows to plasmids intended to be expressed in the specified organism",
        "doi": null,
        "edam_operation": [],
        "edam_topic": [],
        "filtered_on": "plasmid.* in description",
        "id": "23",
        "keep": "Reject",
        "latest_version": 1,
        "license": "MIT",
        "link": "https:/workflowhub.eu/workflows/23?version=1",
        "name": "Genetic Design",
        "number_of_steps": 7,
        "projects": [
            "IBISBA Workflows"
        ],
        "source": "WorkflowHub",
        "tags": [
            "retrosynthesis",
            "genetic design",
            "pathway prediction"
        ],
        "tools": [
            "\n PartsGenie",
            "\n LCRGenie",
            "\n rpOptBioDes",
            "\n DNAWeaver",
            "\n extractTaxonomy",
            "\n rpSelenzyme",
            "\n rpSBMLtoSBOL"
        ],
        "type": "Galaxy",
        "update_time": "2023-01-16",
        "versions": 1
    }
]